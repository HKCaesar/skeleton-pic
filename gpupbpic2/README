Skeleton 2-1/2D Electromagnetic MPI-GPU Particle-in-Cell (PIC) codes
by Viktor K. Decyk
copyright 2013, regents of the university of california

This program contains sample codes for illustrating the basic structure
of a 2-1/2D Electromagnetic MPI-GPU Particle-in-Cell (PIC) code.  The
GPU procedures are written in CUDA C, and the main programs themselves
are written in both Fortran and C.  The codes have no diagnosics except
for initial and final energies.  Their primary purpose is to provide
example codes for physical science students learning about MPI-GPU PIC
codes.  They are also intended as benchmark reference codes to aid in
developing new codes and in evaluating new computer architectures.  A
single GPU version of this code with the same structure (gpubpic2) also
exists, and can be compared to this code in order to understand how the
multi-GPU algorithms are implemented.

PIC codes are widely used in plasma physics.  They model plasmas as
particles which interact self-consistently via the electromagnetic
fields they themselves produce.  PIC codes generally have three
important procedures in the main iteration loop.  The first is the
deposit, where some particle quantity, such as a charge, is accumulated
on a grid via interpolation to produce a source density.  The second
important procedure is the field solver, which solves Maxwell’s equation
or a subset to obtain the electric and/or magnetic fields from the
source densities.  Finally, once the fields are obtained, the particle
forces are found by interpolation from the grid, and the particle
co-ordinates are updated, using Newton’s second law and the Lorentz
force.  The particle processing parts dominate over the field solving
parts in a typical PIC application.

More details about PIC codes can be found in the texts by C. K. Birdsall
and A. B. Langdon, Plasma Physics via Computer Simulation, 1985,
R. W. Hockney and J. W. Eastwood, Computer Simulation Using Particles,
1981, and John M. Dawson, "Particle simulation of plasmas", Rev. Mod.
Phys. 55, 403 (1983).  Details about the mathematical equations and
units used in this code is given in the companion article,
"Description of Electromagnetic Spectral Code from the UPIC Framework"
by Viktor K. Decyk, UCLA, in the file EMModels.pdf.

Details abut Cuda C can be found in the book by Nicholas Wilt, The CUDA
Handbook, Addison-Wesley, 2013, as well as a number of other texts.
Details abut MPI can be found in the book by William Gropp, Ewing Lusk,
and Anthony Skjellum, Using MPI: Portable Parallel Programming with the
Message-Passing Interface, The MIT Press, 1994.

No warranty for proper operation of this software is given or implied.
Software or information may be copied, distributed, and used at own
risk; it may not be distributed without this notice included verbatim
with each file.  If use of these codes results in a publication, an
acknowledgement is requested.

The code here uses the complete electromagnetic interaction, obtained by
solving Maxwell's equation.  A spectral method using Fast Fourier
Transforms (FFTs) is used to solve the Maxwell and Poisson equations.  A
real to complex FFT is used, and the data in Fourier space is stored in
an unpacked format, where the input and output sizes are not the same.
The boundary conditions are periodic, only electron species are
included, and linear interpolation is used.

For parallelization, the code uses three levels of parallelism.  The
outermost level uses a simple domain decomposition scheme, where the
field quantities (electric and magnetic fields, charge and current
density) are divided among the computational nodes.  The primary
decomposition divides the y values evenly, that is, each MPI node has
all the x values for some y.  The particles are distributed so that the
y co-ordinates of the particles have a value within the domain.  This
simple decomposition works if the particles are uniformly distributed in
space.  Particles at the edge of the domain may need information from
the next domain in order to interpolate the fields.  To avoid
unnecessary communication, one extra guard cell in y is added at the end
of each domain that replicates the first y value in the next domain.
After particles are updated, some particles may move to a neighboring
domain.  A particle manager (GPPORDER2L) is responsible for moving such
particles to the appropriate domain.  The FFT is performed in 3 steps.
In going from real space to Fourier space, the FFT is first performed in
x for the y values in the primary domain.  The data is then transposed
to a secondary domain decomposition, where each node has all the y
values for some x.  The FFT is then performed in the y direction for the
x values in the secondary domain.  Maxwell and Poisson equations are
solved using this secondary decomposition.  There are five main
communication procedures which use MPI.  The first two add the guard
cells for the charge and current densities, the third copies the guard
cells for the electric and magnetic fields.  The fourth is the particle
manager, and the fifth transposes the data between primary and secondary
decompositions using an all to all communication pattern.  Further
information about the domain decomposition parallel algorithms used can
be found in the companion presentation Dcomp.pdf and in the article:
p. c. liewer and v. k. decyk, j. computational phys. 85, 302 (1989).

On each MPI node, there is a local GPU, and two additional levels of
parallelism are used.  The middle level uses a tiling (or blocking)
technique.  Space is divided into small 2D tiles (with typically 16x16
grid points in a tile), and particles are organized so that their
co-ordinates in x and y lie within the same tile and are stored together
in memory.  A group of threads (called a thread block) is assigned to
each tile.  This introduces a third level of parallelism which uses a
vector or data parallel technique, where each thread in a block executes
the same instruction in lock step.  Using multiple threads within a tile
allows for the possibility of data collisions (where 2 or more threads
try to write to the same memory location at the same time), which may
have to be resolved.  The size of the tiles should be smaller than the
shared memory size and the number of tiles should be greater than the
number of multiprocessors on the GPU.

There are 4 major procedures which make use of the tiled data structure,
the charge and current deposits, the particle push, and the particle
reordering.  In the deposit procedures, each thread in a block first
deposits to a small shared memory density array the size of a tile plus
guard cells.  Atomic operations (atomicAdd) in shared memory are needed
to resolve data collisions.  After all the particles in a tile have been
processed the shared memory density array is added to the global density
arrays.  The particle push is similar, where each thread block first
copies the global field arrays to shared memory field arrays which are
then used for field interpolation.  An atomic operation is used only
when counting how many particles are leaving a tile.  The particle
reordering step is described below.

The five main communication procedures which use MPI must also
communicate with the GPU.  Therefore an additional layer of software,
gpplib2, controls the communications between the GPU and MPI.  First of
all, the guard cell procedures send data from the GPU to the host, then
via MPI to a remote host.  The received MPI data is copied to the GPU.
The particle reordering step (GPPORDER2L) is divided into
two procedures.  The first procedure (cgpuppporder2la) moves particles
which are leaving a tile into an ordered particle buffer.  In addition,
those particles which are leaving the MPI node are copied to an MPI send
buffer on the host.  The MPI particle manager (PPPMOVE2) then sends and
receives the particle buffer, and copies the received data to the GPU.
The second procedure (cgpuppporder2lb) then copies the incoming
particles, either from the ordered particle buffer or from the MPI
receive buffer, and inserts the particles into the appropriate location
in the particle array.  The default real to complex MPI-GPU FFT makes
use of the default single GPU FFT in gpupic2. The transpose is done in
stages: first a local transpose is performed on a GPU and copied to the
host.  The host makes use of MPI to scatter the data to other hosts,
each of which copies it to its local GPU.  An additional transpose is
finally performed on the local GPU.  There is also an FFT based on the
NVIDIA cuFFT library, which is faster, but its internal behavior is
opaque.  The default FFT is intended to illustrate how one can
parallelize such an FFT.

Further information about this tiling parallel algorithm used can be
found in the article:  V. K. Decyk and T. V. Singh, "Particle-in-Cell
Algorithms for Emerging Computer Architectures," Computer Physics
Communications, 185, 708 (2014), available at
http://dx.doi.org/10.1016/j.cpc.2013.10.013.  Further information about
the hybrid domain decompositin/GPU scheme can be found in the companion
presentation MPI-GPU-PIC.pdf 

Important differences between the relativistic push and deposit kernels
(gpupprbppush23l, gpu2ppost2l, and gpupp2rjppost2l in gpupbpush2.cu)
and the single GPU versions (in gpubpush2.cu in the gpubpic2 directory)
are highlighted in the file dpgpubpush2_cu.pdf.

Differences between the main codes (gpupbpic2.f90 and gpupbpic2.c) and
the main codes in the single GPU versions in the gpubpic2 directory
(gpubpic2.f90 and gpubpic2.c) are highlighted in the files
dpgpubpic2_f90.pdf and dpgpubpic2_c.pdf, respectively. 

Particles are initialized with a uniform distribution in space and a
gaussian distribution in velocity space.  This describes a plasma in
thermal equilibrium.  The particle data is then copied to the GPU.  The
inner loop contains a current and charge deposit, add and copy guard
cell procedures, a vector and scalar FFT, a transverse current
procedure, Maxwell and Poisson solvers, vector FFTs, copy guard cell
procedures, a particle push, and a particle reordering procedure.  The
final energy and timings are printed.  A sample output file for the
default input parameters is included in the file output.

In more detail, the inner loop of the code contains the following
procedures in Fortran (C):

Deposit section:

   cgpu2pprjppost2l: relativistic current deposit, and update position.
                     x(t+dt/2)->x(t+dt)
   cgpu2ppjppost2l: deposit current density, and update position.
                     x(t+dt/2)->x(t+dt)
   GPPORDER2L (cgpporder2l): move particles to appropriate tile
   cgpu2ppgppost2l: deposit charge density
   GPPCACGUARD2L (cgppcacguard2l): add and copy current density guard
                                   cells
   GPPCAGUARD2L (cgppcaguard2l): add and copy charge density guard cells

Field solve section:
   WAPPFFT2RCS (cwappfft2rcs): FFT charge density to fourier space
   WAPPFFT2RCSN (cwappfft2rcsn): FFT current density to fourier space
   cgpuppcuperp2t: take transverse part of current
   cgpuippbpoisp23t: calculate initial magnetic field in fourier space
   cgpuppmaxwel2t: update transverse electromagnetic fields in fourier
                   space
   cgpuppois23t: calculate smoothed longitudinal electric field in
                 fourier space.
   cgpuppemfield2t: add smoothed longitudinal and transverse electric
                    fields
   cgpuppemfield2t: copy smoothed magnetic field
   WAPPFFT2RCSN (cwappfft2rcsn): FFT smoothed electric field to real
                                 space
   WAPPFFT2RCSN (cwappfft2rcsn): FFT smoothed magnetic field to real
                                 space

Particle Push section:
   GPPCBGUARD2L (cgppcbguard2l): fill in guard cells for smoothed
                                 electric field
   GPPCBGUARD2L (cgppcbguard2l): fill in guard cells for smoothed
                                 magnetic field
   cgpuppgrbppush23l: update relativistic particle co-ordinates with
                      smoothed electric and magnetic fields.
                      x(t)->x(t+dt/2); v(t-dt/2)->v(t+dt/2)
   cgpuppgbppush23l: update particle co-ordinates with smoothed electric
                     and magnetic fields.
                     x(t)->x(t+dt/2); v(t-dt/2)->v(t+dt/2)
   GPPORDER2L (cgpporder2l): move particles to appropriate tile

The inputs to the code are the grid parameters indx, indy, the particle
number parameters npx, npy, the time parameters tend, dt, and the
velocity paramters vtx, vty, vtz, vx0, vy0, vz0, the inverse speed of
light ci, the flag relativity.  In addition, a tile size mx, my, and
overflow size xtras are defined.

In more detail:
indx = exponent which determines length in x direction, nx=2**indx.
indy = exponent which determines length in y direction, ny=2**indy.
   These ensure the system lengths are a power of 2.
npx = number of electrons distributed in x direction.
npy = number of electrons distributed in y direction.
   The total number of particles in the simulation is npx*npy.
tend = time at end of simulation, in units of plasma frequency.
dt = time interval between successive calculations.
   The number of time steps the code runs is given by tend/dt.
   dt should be less than .45*ci for the electromagnetic code.
vtx/vty/vtz = thermal velocity of electrons in x/y/z direction.
   a typical value is 1.0.
vx0/vy0/vz0 = drift velocity of electrons in x/y/z direction.
ci = reciprical of velocity of light
relativity = (no,yes) = (0,1) = relativity is used
mx/my = number of grids points in x and y in each tile.

The major program files contained here include:
gpupbpic2.f90    Fortran90 main program 
gpupbpic2.f03    Fortran2003 main program
gpupbpic2.c      C main program
gpplib2.f90      Fortran90 mixed GPU/MPI procedure library
gpplib2.f03      Fortran2003 mixed GPU/MPI procedure library
gpplib2.c        C mixed GPU/MPI procedure library
gpplib2.h        C mixed GPU/MPI procedure header library
pplib2.f         Fortran77 MPI communications library
pplib2_h.f90     Fortran90 MPI communications interface (header) library
pplib2.f90       Fortran90 MPI communications library
pplib2.c         C MPI communications library
pplib2.h         C MPI communications header library
gpulib2.cu       Cuda C utility library
gpulib2.h        Cuda C utility header library
gpulib2_h.f90    Fortran90 utility header library
gpulib2_c.f03    Fortran2003 utility header library
gpupbpush2.cu    Cuda C procedure library
gpupbpush2.h     Cuda C procedure header library
gpupbpush2_h.f90 Fortran90 procedure header library
gpupbpush2_c.f03 Fortran2003 procedure header library
gpupfft2.cu      Cuda C FFT library
gpupfft2.h       Cuda C FFT header library
gpupfft2_h.f90   Fortran90 FFT header library
gpupfft2_c.f03   Fortran2003 FFT header library
pbpush2.f        Fortran77 procedure library
pbpush2_h.f90    Fortran90 procedure interface (header) library
pbpush2.c        C procedure library
pbpush2.h        C procedure header library
dtimer.c         C timer function, used by both C and Fortran
dtimer_c.f03     Fortran2003 timer header library

Files with the suffix .f90 adhere to the Fortran 90 standard, files with
the suffix .f adhere to the Fortran77 standard, files with the suffix
.f03 adhere to the Fortran2003 standard, and files with the suffix .c
and .h adhere to the C99 standard.

The makefile is setup to use nvcc, icc and ifort with Linux.
Definitions for other compilers are in the Makefile, but are commented
out.

Three executables can be created, fgpupbpic2 for Fortran90, f03gpupbpic2
for Fortran2003, and cgpupbpic2 for C.  The only differences between the
Fortran90 and Fortran2003 codes is how the interface to C is handled.
Interoperability with C is part of Fortran2003, but is still unfamiliar
to many programmers.

To compile program, execute:

Make program_name

where program_name is either: fgpupbpic2, f03gpupbpic2 or cgpupbpic2.

To create all of them, execute:

make

The command to execute a program with both MPI and GPU varies from one
system to another.  One possible command is:

mpiexec -np nproc -perhost n ./program_name

where program_name is either fgpupbpic2, f03gpupbpic2 or cgpupbpic2, and
where nproc is the number of processors to be used, and
where n is the number of GPUs (MPI nodes) per host.

There is one restriction on the number of processors which can be used:
this simple skeleton code does not support the case where MPI nodes have
zero grid points.  This special case can happen for certain combinations
of the grid size in y (set by the parameter indy) and the number of
processors chosen.  If this happens the code will exit with an error
message.  This special case will never occur if the grid size in y is an
exact multiple of the number of processors.

By default, CUDA will use a blocksize of 128 and sets the cache size to
16 KB.  If the user wants to control the blocksize or cache size, the
parameters nblock and nscache can be set in the main program.

The file output contains the results produced for the default parameters.
Typical timing results are shown in the file fgpupbpic2_bench.pdf.

The libraries gpplib2.c, pplib2.c, and pbpush2.c contain wrapper
functions to allow the C libraries to be called from Fortran.  The
library pplib2_f.c contains wrapper functions to allow the Fortran
library to be called from C.
