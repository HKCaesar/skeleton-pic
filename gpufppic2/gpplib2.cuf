!-----------------------------------------------------------------------
! Basic parallel PIC library for GPU-MPI communications
! gpplib2.cuf contains basic communications procedures for 1d partitions
! GPPCAGUARD2L accumulates guard cells and copies to scalar field
! GPPCCGUARD2L replicates guard cells and copies to 2 component vector
!              field
! WAPPFFT2RCS performs real to complex asynchronous fft for scalar
!             array
! WAPPFFT2RCSN performs real to complex asynchronous fft for vector
!              array
! GPUPPFFT2RRCU performs real to complex asynchronous fft for scalar
!               array, based on NVIDIA FFT
! GPUPPFFT2RRCUN performs real to complex asynchronous fft for vector
!                array, based on NVIDIA FFT
! GPPORDER2L sorts partiles by tiles
! GPPTPOSE performs a transpose of a complex scalar array, distributed
!          in y, to a complex scalar array, distributed in x.
!          data from GPU is sent asynchronous, overlapping with MPI
! GPPTPOSEN performs a transpose of an n component complex vector array,
!           distributed in y, to an n component complex vector array,
!           distributed in x.
!           data from GPU is sent asynchronous, overlapping with MPI
! written by viktor k. decyk, ucla
! copyright 2013, regents of the university of california
! update: may 9, 2014
      module fgpplib2
      use cudafor
      use fgpulib2
      use fgpuppush2
      use fgpupfft2
      use pplib2       ! use with pplib2.f90
!     use pplib2_h     ! use with pplib2.f
      use dtimer_c
      implicit none
      private
      public :: GPPCAGUARD2L, GPPCCGUARD2L
      public :: WAPPFFT2RCS, WAPPFFT2RCSN, GPUPPFFT2RRCU, GPUPPFFT2RRCUN
      public :: GPPORDER2L
!
      contains
!
!-----------------------------------------------------------------------
      subroutine GPPCAGUARD2L(g_q,g_qe,g_scs,scs,scr,nx,nyp,kstrt,nvp,  &
     &nxe,nypmx,nxvh,kypd)
! this subroutine copies scalar field and accumulates guard cells in y
! from remote GPU into scalar field
      implicit none
      integer, intent(in) :: nx, nyp, kstrt, nvp, nxe, nypmx, nxvh, kypd
      complex, device, dimension(nxvh,kypd) :: g_q
      real, device, dimension(nxe,nypmx) :: g_qe
      complex, device, dimension(nxvh) :: g_scs
      complex, dimension(nxvh) :: scs, scr
      call fgpuppcaguard2xl(g_q,g_scs,g_qe,nyp,nx,nxe,nypmx,nxvh,kypd)
      call fgpu_ccopyout(scs,g_scs,nxvh)
      call PPPCNAGUARD2L(scs,scr,kstrt,nvp,nxvh)
      call fgpu_ccopyin(scr,g_scs,nxvh)
      call fgpuppcaguard2yl(g_q,g_scs,nx,nxvh,kypd)
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine GPPCCGUARD2L(g_fxy,g_fxye,g_scs,scs,scr,nx,nyp,kstrt,  &
     &nvp,ndim,nxe,nypmx,nxvh,kypd)
! this subroutine copies 2 component vector field and adds additional
! guard cells in y from remote GPU into extended vector field
      implicit none
      integer, intent(in) :: nx, nyp, kstrt, nvp, ndim, nxe, nypmx
      integer, intent(in) :: nxvh, kypd
      complex, device, dimension(nxvh,ndim,kypd) :: g_fxy
      real, device, dimension(ndim,nxe,nypmx)  :: g_fxye
      complex, device, dimension(nxvh*ndim) :: g_scs
      complex, dimension(nxvh*ndim) :: scs, scr
      call fgpuppccguard2xl(g_fxy,g_scs,g_fxye,nyp,nx,nxe,nypmx,nxvh,   &
     &kypd)
      call fgpu_ccopyout(scs,g_scs,nxvh*ndim)
      call PPPCNCGUARD2L(scs,scr,kstrt,nvp,ndim*nxvh)
      call fgpu_ccopyin(scr,g_scs,nxvh*ndim)
      call fgpuppccguard2yl(g_fxye,g_scs,nyp,nx,nxe,nxvh,nypmx)
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine WAPPFFT2RCS(g_f,g_g,g_bsm,g_brm,bsm,brm,isign,g_mixup, &
     &g_sct,ttp,indx,indy,kstrt,nvp,kxpd,kyp,nxhd,nyd,kypd,nxhyd,nxyhd)
! wrapper function for gpu-mpi parallel real to complex fft,
! without packed data
! if isign = -1, g_f = input, g_g = output
! if isign = 1, g_g = input, g_f = output
! (g_bsm,g_brm)/(bsm,brm) are temporary scratch arrays on GPU/host
! nxhd must be = nx/2 + 1
      implicit none
      integer, intent(in) :: isign, indx, indy, kstrt, nvp, kxpd, kyp
      integer, intent(in) :: nxhd, nyd, kypd, nxhyd, nxyhd
      complex, device, dimension(nxhd,kypd) :: g_f
      complex, device, dimension(nyd,kxpd) :: g_g
      complex, device, dimension(kxpd*kyp,nvp) :: g_bsm, g_brm
      integer, device, dimension(nxhyd) :: g_mixup
      complex, device, dimension(nxyhd) :: g_sct
      real, dimension(2) :: ttp
      complex, dimension(kxpd*kyp,nvp) :: bsm, brm
! local data
! kasync = (0,1) = (no,yes) use asynchronous communications
      integer, parameter :: kasync = 1
      integer :: nx, nxh1, ny, nxyp
      real :: ani, time
      double precision :: dtime
      type (timeval) :: itime
      nx = 2**indx
      nxh1 = nx/2 + 1
      ny = 2**indy
      nxyp = kxpd*kyp*(nvp-1)
      ani = 1.0/(real(nx)*real(ny))
! inverse fourier transform
      if (isign < 0) then
         call dtimer(dtime,itime,-1)
! first transpose in x
         call fgpuwppfft2rcsx(g_f,g_bsm,isign,g_mixup,g_sct,indx,indy,  &
     &kstrt,nvp,kxpd,kyp,nxhd,kypd,nxhyd,nxyhd)
! transpose on local GPU
         call fgpuppltpose(g_f,g_g,nxhd,ny,kxpd,kyp,kstrt,nxhd,nyd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! transpose between GPUs
         if (nvp > 1) then
            call dtimer(dtime,itime,-1)
! use asynchronous communication
            if (kasync==1) then
               call GPPTPOSE(g_bsm,g_brm,bsm,brm,nx,ny,kxpd,kyp,kstrt,  &
     &nvp)
! use synchronous communication
            else
               call fgpu_ccopyout(bsm,g_bsm,nxyp)
               call PPPTPOSE(bsm,brm,nxh1,ny,kxpd,kyp,kstrt,nvp)
               call fgpu_ccopyin(brm,g_brm,nxyp)
            endif
            call dtimer(dtime,itime,1)
            time = real(dtime)
            ttp(2) = ttp(2) + time
         endif
! then transpose in y
         call dtimer(dtime,itime,-1)
         call fgpuwppfft2rcsy(g_g,g_brm,isign,g_mixup,g_sct,indx,indy,  &
     &kstrt,nvp,kxpd,kyp,nyd,nxhyd,nxyhd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! forward fourier transform
      else if (isign > 0) then
         call dtimer(dtime,itime,-1)
! first transpose in y
         call fgpuwppfft2rcsy(g_g,g_brm,isign,g_mixup,g_sct,indx,indy,  &
     &kstrt,nvp,kxpd,kyp,nyd,nxhyd,nxyhd)
! transpose on local GPU
         call fgpuppltpose(g_g,g_f,ny,nxhd,kyp,kxpd,kstrt,nyd,nxhd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! transpose between GPUs
        if (nvp > 1) then
            call dtimer(dtime,itime,-1)
! use asynchronous communication
            if (kasync==1) then
               call GPPTPOSE(g_brm,g_bsm,brm,bsm,ny,nx,kyp,kxpd,kstrt,  &
     &nvp)
! use synchronous communication
            else
               call fgpu_ccopyout(brm,g_brm,nxyp)
               call PPPTPOSE(brm,bsm,ny,nxh1,kyp,kxpd,kstrt,nvp)
               call fgpu_ccopyin(bsm,g_bsm,nxyp)
            endif
            call dtimer(dtime,itime,1)
            time = real(dtime)
            ttp(2) = ttp(2) + time
         endif
! then transpose in x
         call dtimer(dtime,itime,-1)
         call fgpuwppfft2rcsx(g_f,g_bsm,isign,g_mixup,g_sct,indx,indy,  &
     &kstrt,nvp,kxpd,kyp,nxhd,kypd,nxhyd,nxyhd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine WAPPFFT2RCSN(g_fn,g_gn,g_bsm,g_brm,bsm,brm,isign,      &
     &g_mixup,g_sct,ttp,indx,indy,kstrt,nvp,ndim,kxpd,kyp,nxhd,nyd,kypd,&
     &nxhyd,nxyhd)
! wrapper function for multiple gpu-mpi parallel real to complex ffts,
! without packed data
! if isign = -1, g_fn = input, g_gn = output
! if isign = 1, g_gn = input, g_fn = output
! (g_bsm,g_brm)/(bsm,brm) are temporary scratch arrays on GPU/host
! ndim = vector dimension
! nxhd must be = nx/2 + 1
      implicit none
      integer, intent(in) :: isign, indx, indy, kstrt, nvp, ndim, kxpd
      integer, intent(in) :: kyp, nxhd, nyd, kypd, nxhyd, nxyhd
      complex, device, dimension(nxhd,ndim,kypd) :: g_fn
      complex, device, dimension(nyd,ndim,kxpd) :: g_gn
      complex, device, dimension(kxpd*ndim*kyp,nvp) :: g_bsm, g_brm
      integer, device, dimension(nxhyd) :: g_mixup
      complex, device, dimension(nxyhd) :: g_sct
      real, dimension(2) :: ttp
      complex, dimension(kxpd*ndim*kyp,nvp) :: bsm, brm
! local data
! kasync = (0,1) = (no,yes) use asynchronous communications
      integer, parameter :: kasync = 1
      integer :: nx, nxh1, ny, nxyp
      real :: ani, time
      double precision :: dtime
      type (timeval) :: itime
      nx = 2**indx
      nxh1 = nx/2 + 1
      ny = 2**indy
      nxyp = kxpd*ndim*kyp*(nvp-1)
      ani = 1.0/(real(nx)*real(ny))
! inverse fourier transform
      if (isign < 0) then
! first transpose in x
         call dtimer(dtime,itime,-1)
         call fgpuwppfft2rcsxn(g_fn,g_bsm,isign,g_mixup,g_sct,indx,indy,&
     &ndim,kstrt,nvp,kxpd,kyp,nxhd,kypd,nxhyd,nxyhd)
! transpose on local GPU
         call fgpuppltposen(g_fn,g_gn,nxhd,ny,kxpd,kyp,kstrt,ndim,nxhd, &
     &nyd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! transpose between GPUs
         if (nvp > 1) then
            call dtimer(dtime,itime,-1)
! use asynchronous communication
            if (kasync==1) then
               call GPPTPOSEN(g_bsm,g_brm,bsm,brm,nx,ny,kxpd,kyp,kstrt, &
     &nvp,ndim)
! use synchronous communication
            else
               call fgpu_ccopyout(bsm,g_bsm,nxyp)
               call PPPTPOSEN(bsm,brm,nxh1,ny,kxpd,kyp,kstrt,nvp,ndim)
               call fgpu_ccopyin(brm,g_brm,nxyp)
            endif
            call dtimer(dtime,itime,1)
            time = real(dtime)
            ttp(2) = ttp(2) + time
         endif
! then transpose in y
         call dtimer(dtime,itime,-1)
         call fgpuwppfft2rcsyn(g_gn,g_brm,isign,g_mixup,g_sct,indx,indy,&
     &ndim,kstrt,nvp,kxpd,kyp,nyd,nxhyd,nxyhd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! forward fourier transform
      else if (isign > 0) then
! first transpose in y
         call dtimer(dtime,itime,-1)
         call fgpuwppfft2rcsyn(g_gn,g_brm,isign,g_mixup,g_sct,indx,indy,&
     &ndim,kstrt,nvp,kxpd,kyp,nyd,nxhyd,nxyhd)
! transpose on local GPU
         call fgpuppltposen(g_gn,g_fn,ny,nxhd,kyp,kxpd,kstrt,ndim,nyd,  &
     &nxhd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! transpose between GPUs
         if (nvp > 1) then
            call dtimer(dtime,itime,-1)
! use asynchronous communication
            if (kasync==1) then
               call GPPTPOSEN(g_brm,g_bsm,brm,bsm,ny,nx,kyp,kxpd,kstrt, &
     &nvp,ndim)
! use synchronous communication
            else
               call fgpu_ccopyout(brm,g_brm,nxyp)
               call PPPTPOSEN(brm,bsm,ny,nxh1,kyp,kxpd,kstrt,nvp,ndim)
               call fgpu_ccopyin(bsm,g_bsm,nxyp)
            endif
            call dtimer(dtime,itime,1)
            time = real(dtime)
            ttp(2) = ttp(2) + time
         endif
! then transpose in x
         call dtimer(dtime,itime,-1)
         call fgpuwppfft2rcsxn(g_fn,g_bsm,isign,g_mixup,g_sct,indx,indy,&
     &ndim,kstrt,nvp,kxpd,kyp,nxhd,kypd,nxhyd,nxyhd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine GPUPPFFT2RRCU(g_f,g_g,g_bsm,g_brm,bsm,brm,isign,ttp,   &
     &indx,indy,kstrt,nvp,kxpd,kyp,nxhd,nyd,kypd)
! wrapper function for gpu-mpi parallel real to complex fft,
! based on NVIDIA FFT, without packed data
! if isign = -1, g_f = input, g_g = output
! if isign = 1, g_g = input, g_f = output
! (g_bsm,g_brm)/(bsm,brm) are temporary scratch arrays on GPU/host
! nxhd must be = nx/2 + 1
      implicit none
      integer, intent(in) :: isign, indx, indy, kstrt, nvp, kxpd, kyp
      integer, intent(in) :: nxhd, nyd, kypd
      complex, device, dimension(nxhd,kypd) :: g_f
      complex, device, dimension(nyd,kxpd) :: g_g
      complex, device, dimension(kxpd*kyp,nvp) :: g_bsm, g_brm
      real, dimension(2) :: ttp
      complex, dimension(kxpd*kyp,nvp) :: bsm, brm
! local data
! kasync = (0,1) = (no,yes) use asynchronous communications
      integer, parameter :: kasync = 1
      integer :: nx, nxh1, ny, nxyp
      real :: ani, time
      double precision :: dtime
      type (timeval) :: itime
      nx = 2**indx
      nxh1 = nx/2 + 1
      ny = 2**indy
      nxyp = kxpd*kyp*(nvp-1)
      ani = 1.0/(real(nx)*real(ny))
! inverse fourier transform
      if (isign < 0) then
! first transpose in x
         call dtimer(dtime,itime,-1)
         call fgpupfft2rrcux(g_f,g_bsm,isign,indx,indy,kstrt,nvp,kxpd,  &
     &kyp,nxhd,kypd)
! transpose on local GPU with scaling
         call fgpuppsltpose(g_f,g_g,ani,nxhd,ny,kxpd,kyp,kstrt,nxhd,nyd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! transpose between GPUs
         if (nvp > 1) then
            call dtimer(dtime,itime,-1)
! use asynchronous communication
            if (kasync==1) then
               call GPPTPOSE(g_bsm,g_brm,bsm,brm,nx,ny,kxpd,kyp,kstrt,  &
     &nvp)
! use synchronous communication
            else
               call fgpu_ccopyout(bsm,g_bsm,nxyp)
               call PPPTPOSE(bsm,brm,nxh1,ny,kxpd,kyp,kstrt,nvp)
               call fgpu_ccopyin(brm,g_brm,nxyp)
            endif
            call dtimer(dtime,itime,1)
            time = real(dtime)
            ttp(2) = ttp(2) + time
         endif
! then transpose in y
         call dtimer(dtime,itime,-1)
         call fgpupfft2rrcuy(g_g,g_brm,isign,indx,indy,kstrt,nvp,kxpd,  &
     &kyp,nyd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! forward fourier transform
      else if (isign > 0) then
! first transpose in y
         call dtimer(dtime,itime,-1)
         call fgpupfft2rrcuy(g_g,g_brm,isign,indx,indy,kstrt,nvp,kxpd,  &
     &kyp,nyd)
! transpose on local GPU
         call fgpuppltpose(g_g,g_f,ny,nxhd,kyp,kxpd,kstrt,nyd,nxhd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! transpose between GPUs
        if (nvp > 1) then
            call dtimer(dtime,itime,-1)
! use asynchronous communication
            if (kasync==1) then
               call GPPTPOSE(g_brm,g_bsm,brm,bsm,ny,nx,kyp,kxpd,kstrt,  &
     &nvp)
! use synchronous communication
            else
               call fgpu_ccopyout(brm,g_brm,nxyp)
               call PPPTPOSE(brm,bsm,ny,nxh1,kyp,kxpd,kstrt,nvp)
               call fgpu_ccopyin(bsm,g_bsm,nxyp)
            endif
            call dtimer(dtime,itime,1)
            time = real(dtime)
            ttp(2) = ttp(2) + time
         endif
! then transpose in x
         call dtimer(dtime,itime,-1)
         call fgpupfft2rrcux(g_f,g_bsm,isign,indx,indy,kstrt,nvp,kxpd,  &
     &kyp,nxhd,kypd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine GPUPPFFT2RRCUN(g_fn,g_gn,g_bsm,g_brm,bsm,brm,isign,    &
     &ttp,indx,indy,kstrt,nvp,ndim,kxpd,kyp,nxhd,nyd,kypd)
! wrapper function for multiple gpu-mpi parallel real to complex ffts,
! based on NVIDIA FFT, without packed data
! if isign = -1, g_fn = input, g_gn = output
! if isign = 1, g_gn = input, g_fn = output
! (g_bsm,g_brm)/(bsm,brm) are temporary scratch arrays on GPU/host
! ndim = vector dimension
! nxhd must be = nx/2 + 1
      implicit none
      integer, intent(in) :: isign, indx, indy, kstrt, nvp, ndim, kxpd
      integer, intent(in) :: kyp, nxhd, nyd, kypd
      complex, device, dimension(nxhd,ndim,kypd) :: g_fn
      complex, device, dimension(nyd,ndim,kxpd) :: g_gn
      complex, device, dimension(kxpd*ndim*kyp,nvp) :: g_bsm, g_brm
      real, dimension(2) :: ttp
      complex, dimension(kxpd*ndim*kyp,nvp) :: bsm, brm
! local data
! kasync = (0,1) = (no,yes) use asynchronous communications
      integer, parameter :: kasync = 1
      integer :: nx, nxh1, ny, nxyp
      real :: ani, time
      double precision :: dtime
      type (timeval) :: itime
      nx = 2**indx
      nxh1 = nx/2 + 1
      ny = 2**indy
      nxyp = kxpd*ndim*kyp*(nvp-1)
      ani = 1.0/(real(nx)*real(ny))
! inverse fourier transform
      if (isign < 0) then
! first transpose in x
         call dtimer(dtime,itime,-1)
         call fgpupfft2rrcuxn(g_fn,g_bsm,isign,indx,indy,ndim,kstrt,nvp,&
     &kxpd,kyp,nxhd,kyp)
! transpose on local GPU with scaling
         call fgpuppsltposen(g_fn,g_gn,ani,nxhd,ny,kxpd,kyp,kstrt,ndim, &
     &nxhd,nyd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! transpose between GPUs
         if (nvp > 1) then
            call dtimer(dtime,itime,-1)
! use asynchronous communication
            if (kasync==1) then
               call GPPTPOSEN(g_bsm,g_brm,bsm,brm,nx,ny,kxpd,kyp,kstrt, &
     &nvp,ndim)
! use synchronous communication
            else
               call fgpu_ccopyout(bsm,g_bsm,nxyp)
               call PPPTPOSEN(bsm,brm,nxh1,ny,kxpd,kyp,kstrt,nvp,ndim)
               call fgpu_ccopyin(brm,g_brm,nxyp)
            endif
            call dtimer(dtime,itime,1)
            time = real(dtime)
            ttp(2) = ttp(2) + time
         endif
! then transpose in y
         call dtimer(dtime,itime,-1)
         call fgpupfft2rrcuyn(g_gn,g_brm,isign,indx,indy,ndim,kstrt,nvp,&
     &kxpd,kyp,nyd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! forward fourier transform
      else if (isign > 0) then
! first transpose in y
         call dtimer(dtime,itime,-1)
         call fgpupfft2rrcuyn(g_gn,g_brm,isign,indx,indy,ndim,kstrt,nvp,&
     &kxpd,kyp,nyd)
! transpose on local GPU
         call fgpuppltposen(g_gn,g_fn,ny,nxhd,kyp,kxpd,kstrt,ndim,nyd,  &
     &nxhd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
! transpose between GPUs
         if (nvp > 1) then
            call dtimer(dtime,itime,-1)
! use asynchronous communication
            if (kasync==1) then
               call GPPTPOSEN(g_brm,g_bsm,brm,bsm,ny,nx,kyp,kxpd,kstrt, &
     &nvp,ndim)
! use synchronous communication
            else
               call fgpu_ccopyout(brm,g_brm,nxyp)
               call PPPTPOSEN(brm,bsm,ny,nxh1,kyp,kxpd,kstrt,nvp,ndim)
               call fgpu_ccopyin(bsm,g_bsm,nxyp)
            endif
            call dtimer(dtime,itime,1)
            time = real(dtime)
            ttp(2) = ttp(2) + time
         endif
! then transpose in x
         call dtimer(dtime,itime,-1)
         call fgpupfft2rrcuxn(g_fn,g_bsm,isign,indx,indy,ndim,kstrt,nvp,&
     &kxpd,kyp,nxhd,kypd)
         call dtimer(dtime,itime,1)
         time = real(dtime)
         ttp(1) = ttp(1) + time
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine GPPORDER2L(g_ppart,g_ppbuff,g_sbufl,g_sbufr,g_kpic,    &
     &g_ncl,g_ihole,g_ncll,g_nclr,sbufl,sbufr,rbufl,rbufr,ncll,nclr,mcll&
     &,mclr,ttp,noff,nyp,kstrt,nvp,idimp,nppmx,nx,ny,mx,my,mx1,myp1,    &
     &npbmx,ntmax,nbmax,g_irc)
! this subroutine performs an mpi-gpu particle sort by x,y grid in tiles
! of mx, my
! linear interpolation, with periodic boundary conditions
! for distributed data, with 1d domain decomposition in y.
      implicit none
      integer, intent(in) :: noff, nyp, kstrt, nvp, idimp, nppmx, nx, ny
      integer, intent(in) :: mx, my, mx1, myp1, npbmx, ntmax, nbmax
      real, device, dimension(nppmx,idimp,mx1*myp1) :: g_ppart
      real, device, dimension(npbmx,idimp,mx1*myp1) :: g_ppbuff
      real, device, dimension(idimp*nbmax) :: g_sbufl, g_sbufr
      integer, device, dimension(mx1*myp1) :: g_kpic
      integer, device, dimension(8,mx1*myp1) :: g_ncl
      integer, device, dimension(2,ntmax+1,mx1*myp1) :: g_ihole
      integer, device, dimension(3,mx1) :: g_ncll, g_nclr
      integer, device, dimension(1) :: g_irc
      real, dimension(idimp*nbmax) :: sbufl, sbufr, rbufl, rbufr
      real, dimension(2) :: ttp
      integer, dimension(3,mx1) :: ncll, nclr, mcll, mclr
! local data
      real :: time
      double precision :: dtime
      type (timeval) :: itime
! first part of particle reorder on x and y cell with mx, my tiles
      call dtimer(dtime,itime,-1)
      call fgpupppord2la(g_ppart,g_ppbuff,g_sbufl,g_sbufr,g_kpic,g_ncl, &
     &g_ihole,g_ncll,g_nclr,noff,nyp,idimp,nppmx,nx,ny,mx,my,mx1,myp1,  &
     &npbmx,ntmax,nbmax,g_irc)
      call dtimer(dtime,itime,1)
      time = real(dtime)
      ttp(1) = ttp(1) + time
! move particles on GPU into appropriate spatial regions:
! updates rbufr, rbufl, mcll, mclr
      call dtimer(dtime,itime,-1)
      ncll = g_ncll
      nclr = g_nclr
      call fgpu_fcopyout(sbufl,g_sbufl,idimp*ncll(3,mx1))
      call fgpu_fcopyout(sbufr,g_sbufr,idimp*nclr(3,mx1))
      call PPPMOVE2(sbufr,sbufl,rbufr,rbufl,ncll,nclr,mcll,mclr,kstrt,  &
     &nvp,idimp,nbmax,mx1)
      g_ncll = mcll
      g_nclr = mclr
      call fgpu_fcopyin(rbufl,g_sbufl,idimp*mcll(3,mx1))
      call fgpu_fcopyin(rbufr,g_sbufr,idimp*mclr(3,mx1))
      call dtimer(dtime,itime,1)
      time = real(dtime)
      ttp(2) = ttp(2) + time
! second part of particle reorder on x and y cell with mx, my tiles:
! updates g_ppart, g_kpic, g_irc
      call dtimer(dtime,itime,-1)
      call fgpupppord2lb(g_ppart,g_ppbuff,g_sbufl,g_sbufr,g_kpic,g_ncl, &
     &g_ihole,g_ncll,g_nclr,idimp,nppmx,mx1,myp1,npbmx,ntmax,nbmax,g_irc&
     &)
      call dtimer(dtime,itime,1)
      time = real(dtime)
      ttp(1) = ttp(1) + time
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine GPPTPOSE(g_bsm,g_btm,sm,tm,nx,ny,kxp,kyp,kstrt,nvp)
! this subroutine sends and receives data between GPUS on different MPI
! nodes to perform a transpose of a matrix distributed in y, to another
! matrix distributed in x.
! one message is sent and received at a time.
! data from GPU is sent asynchronous, overlapping with MPI
! g_bsm/g_btm are complex buffers on GPU to be sent/received
! sm/tm = complex buffers on host to be sent/received
! nx/ny = number of points in x/y
! kxp/kyp = number of data values per block in x/y
! kstrt = starting data block number
! nvp = number of real or virtual processors
      implicit none
      integer, intent(in) :: nx, ny, kxp, kyp, kstrt, nvp
      complex, device, dimension(kxp*kyp,nvp) :: g_bsm, g_btm
      complex, dimension(kxp*kyp,nvp) :: sm, tm
! local data
      integer :: j, n, nn, ks, kyps, kxyp, id, joff, ld, ns, st, stp
      ks = kstrt - 1
      kyps = min(kyp,max(0,ny-kyp*ks))
      kxyp = kxp*kyp
! special case for one processor
! better to use a kernel function
      if (nvp==1) then
         call fgpu_ccopyout(sm,g_bsm,kxyp)
         do j = 1, kxyp
            tm(j,1) = sm(j,1)
         enddo
         call fgpu_ccopyin(tm,g_btm,kxyp)
         return
      endif
      nn = 0
      ns = kxyp
      stp = 1
! send first group to host from GPU
      call fgpu_cascopyout(sm,g_bsm,0,ns,stp)
! this segment is used for mpi computers
      do n = 1, nvp
         id = n - ks - 1
         if (id < 0) id = id + nvp
         if (id /= ks) then
! adjust counter to omit data sent to oneself
            nn = nn + 1
! send next group to host from GPU
            if ((nn+1) < nvp) then
               st = stp + 1
               if (st > 2) st = st - 2 
               call fgpu_cascopyout(sm(1,nn+1),g_bsm,ns*nn,ns,st)
            endif
! wait for previous MPI sends and receives to complete
            if (nn > 1) then
! call MPI_WAIT(msid,istatus,ierr); call MPI_WAIT(mrid,istatus,ierr)
               call ACSNDREC(sm,0,0,0,3)
! copy received group from host to GPU
               call fgpu_cascopyin(tm(1,nn-1),g_btm,ns*(nn-2),ns,3)
            endif
! calculate length of data to send
            joff = kxp*id
            ld = kyps*min(kxp,max(0,nx-joff))
! post receive
! call MPI_IRECV(tm(1,nn),kxyp,mcplx,id,n,lgrp,mrid,ierr)
            call ACSNDREC(tm(1,nn),id,kxyp,n,1)
! wait for previous group to arrive from GPU
            call fgpu_waitstream(stp)
            stp = st
! send data
! call MPI_ISEND(sm(1,nn),ld,mcplx,id,n,lgrp,msid,ierr)
            call ACSNDREC(sm(1,nn),id,ld,n,2)
         endif
      enddo
! wait for last sends and receives to complete
! call MPI_WAIT(msid,istatus,ierr); call MPI_WAIT(mrid,istatus,ierr)
      call ACSNDREC(sm,0,0,0,3)
      call fgpu_cascopyin(tm(1,nn),g_btm,ns*(nn-1),ns,3)
! wait for last group item to arrive
      call fgpu_waitstream(3)
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine GPPTPOSEN(g_bsm,g_btm,sm,tm,nx,ny,kxp,kyp,kstrt,nvp,   &
     &ndim)
! this subroutine sends and receives data between GPUS on different MPI
! nodes to perform a transpose of an n component  matrix distributed in
! y, to another an n component matrix distributed in x.
! one message is sent and received at a time.
! data from GPU is sent asynchronous, overlapping with MPI.
! g_bsm/g_btm are complex buffers on GPU to be sent/received
! sm/tm = complex buffers on host to be sent/received
! nx/ny = number of points in x/y
! kxp/kyp = number of data values per block in x/y
! kstrt = starting data block number
! nvp = number of real or virtual processors
      implicit none
      integer, intent(in) :: nx, ny, kxp, kyp, kstrt, nvp, ndim
      complex, device, dimension(kxp*ndim*kyp,nvp) :: g_bsm, g_btm
      complex, dimension(kxp*ndim*kyp,nvp) :: sm, tm
! local data
      integer :: j, n, nn, ks, kyps, kxyp, id, joff, ld, ns, st, stp
      ks = kstrt - 1
      kyps = ndim*min(kyp,max(0,ny-kyp*ks))
      kxyp = kxp*ndim*kyp
! special case for one processor
! better to use a kernel function
      if (nvp==1) then
         call fgpu_ccopyout(sm,g_bsm,kxyp)
         do j = 1, kxyp
            tm(j,1) = sm(j,1)
          enddo
         call fgpu_ccopyin(tm,g_btm,kxyp)
         return
      endif
      nn = 0
      ns = kxyp
      stp = 1
! send first group to host from GPU
      call fgpu_cascopyout(sm,g_bsm,0,ns,stp)
! this segment is used for mpi computers
      do n = 1, nvp
         id = n - ks - 1
         if (id < 0) id = id + nvp
         if (id /= ks) then
! adjust counter to omit data sent to oneself
            nn = nn + 1
! send next group to host from GPU
            if ((nn+1) < nvp) then
               st = stp + 1
               if (st > 2) st = st - 2
               call fgpu_cascopyout(sm(1,nn+1),g_bsm,ns*nn,ns,st)
            endif
! wait for previous MPI sends and receives to complete
            if (nn > 1) then
! call MPI_WAIT(msid,istatus,ierr); call MPI_WAIT(mrid,istatus,ierr)
               call ACSNDREC(sm,0,0,0,3)
! copy received group from host to GPU
               call fgpu_cascopyin(tm(1,nn-1),g_btm,ns*(nn-2),ns,3)
            endif
! calculate length of data to send
            joff = kxp*id
            ld = kyps*min(kxp,max(0,nx-joff))
! post receive
! call MPI_IRECV(tm(1,nn),kxyp,mcplx,id,n,lgrp,mrid,ierr)
            call ACSNDREC(tm(1,nn),id,kxyp,n,1)
! wait for previous group to arrive from GPU
            call fgpu_waitstream(stp)
            stp = st
! send data
! call MPI_ISEND(sm(1,nn),ld,mcplx,id,n,lgrp,msid,ierr)
            call ACSNDREC(sm(1,nn),id,ld,n,2)
         endif
      enddo
! wait for last sends and receives to complete
! call MPI_WAIT(msid,istatus,ierr); call MPI_WAIT(mrid,istatus,ierr)
      call ACSNDREC(sm,0,0,0,3)
      call fgpu_cascopyin(tm(1,nn),g_btm,ns*(nn-1),ns,3)
! wait for last group item to arrive
      call fgpu_waitstream(3)
      end subroutine
!
      end module
