!-----------------------------------------------------------------------
! CUDA Fortran Library for Skeleton 2-1/2D Electromagnetic GPU PIC Code
! written by Viktor K. Decyk, UCLA
      module fgpubpush2
      use cudafor
      use fgpulib2, only : nblock_size, maxgsx
      implicit none
!
      integer :: crc = 0
      real, device, dimension(:), allocatable :: scr
      save
!
      private
      public :: gpuctpose4, gpuctpose4n
      public :: fgpubppush23l, fgpubppushf23l
      public :: fgpurbppush23l, fgpurbppushf23l
      public :: fgpu2ppost2l, fgpu2jppost2l, fgpu2jppostf2l
      public :: fgpu2rjppost2l, fgpu2rjppostf2l
      public :: fgpucaguard2l, fgpucacguard2l, fgpucbguard2l
      public :: fgpuppord2l, fgpuppordf2l, fgpupois23t
      public :: fgpucuperp2t, fgpuibpois23t, fgpumaxwel2t, fgpuemfield2t
      public :: fgpuwfft2rcs, fgpuwfft2rcsn, fgpusum2
!
      contains
!
!-----------------------------------------------------------------------
      attributes(device) subroutine liscan2(isdata,nths)
! performs local prefix reduction of integer data shared by threads
! using binary tree method.
      implicit none
      integer, value :: nths
      integer, dimension(*) :: isdata
! local data
      integer :: l, mb, kxs, lb, kb
      l = threadIdx%x - 1
      mb = l
      kxs = 1
      do while (kxs < nths)
         lb = kxs*mb
         kb = 2*lb + kxs - 1
         lb = lb + l + kxs
         if (lb < nths) isdata(lb+1) = isdata(lb+1) + isdata(kb+1)
         call syncthreads()
         mb = mb/2
         kxs = kxs + kxs
      enddo
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(device) subroutine lsum2(sdata,n)
! finds local sum of n data items shared by threads
! using binary tree method. input is modified
      implicit none
      real, dimension(*) :: sdata
      integer, value :: n
! local data
      integer :: l, k
      real :: s
      l = threadIdx%x
      k = blockDim%x/2
      s = 0.0
!
      if (l <= n) s = sdata(l)
      do while (k > 0)
         if (l <= k) then
            if ((l+k) <= n) then
               s = s + sdata(l+k)
               sdata(l) = s
            endif
         endif
         call syncthreads()
         k = k/2
      enddo
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpubppush23l(ppart,fxy,bxy,kpic,qbm,&
     &dt,dtc,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
! for 2-1/2d code, this subroutine updates particle co-ordinates and
! velocities using leap-frog scheme in time and first-order linear
! interpolation in space, with magnetic field. Using the Boris Mover.
! threaded version using guard cells
! data read in tiles
! particles stored segmented array
! 119 flops/particle, 1 divide, 29 loads, 5 stores
! input: all, output: ppart, ek
! velocity equations used are:
! vx(t+dt/2) = rot(1)*(vx(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(2)*(vy(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(3)*(vz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fx(x(t),y(t))*dt)
! vy(t+dt/2) = rot(4)*(vx(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(5)*(vy(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(6)*(vz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fy(x(t),y(t))*dt)
! vz(t+dt/2) = rot(7)*(vx(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(8)*(vy(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(9)*(vz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fz(x(t),y(t))*dt)
! where q/m is charge/mass, and the rotation matrix is given by:
!    rot(1) = (1 - (om*dt/2)**2 + 2*(omx*dt/2)**2)/(1 + (om*dt/2)**2)
!    rot(2) = 2*(omz*dt/2 + (omx*dt/2)*(omy*dt/2))/(1 + (om*dt/2)**2)
!    rot(3) = 2*(-omy*dt/2 + (omx*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(4) = 2*(-omz*dt/2 + (omx*dt/2)*(omy*dt/2))/(1 + (om*dt/2)**2)
!    rot(5) = (1 - (om*dt/2)**2 + 2*(omy*dt/2)**2)/(1 + (om*dt/2)**2)
!    rot(6) = 2*(omx*dt/2 + (omy*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(7) = 2*(omy*dt/2 + (omx*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(8) = 2*(-omx*dt/2 + (omy*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(9) = (1 - (om*dt/2)**2 + 2*(omz*dt/2)**2)/(1 + (om*dt/2)**2)
! and om**2 = omx**2 + omy**2 + omz**2
! the rotation matrix is determined by:
! omx = (q/m)*bx(x(t),y(t)), omy = (q/m)*by(x(t),y(t)), and
! omz = (q/m)*bz(x(t),y(t)).
! position equations used are:
! x(t+dt)=x(t) + vx(t+dt/2)*dt
! y(t+dt)=y(t) + vy(t+dt/2)*dt
! fx(x(t),y(t)), fy(x(t),y(t)), and fz(x(t),y(t))
! bx(x(t),y(t)), by(x(t),y(t)), and bz(x(t),y(t))
! are approximated by interpolation from the nearest grid points:
! fx(x,y) = (1-dy)*((1-dx)*fx(n,m)+dx*fx(n+1,m)) + dy*((1-dx)*fx(n,m+1)
!    + dx*fx(n+1,m+1))
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! similarly for fy(x,y), fz(x,y), bx(x,y), by(x,y), bz(x,y)
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! ppart(n,3,m) = velocity vx of particle n in tile m
! ppart(n,4,m) = velocity vy of particle n in tile m
! ppart(n,5,m) = velocity vz of particle n in tile m
! fxy(1,j,k) = x component of force/charge at grid (j,k)
! fxy(2,j,k) = y component of force/charge at grid (j,k)
! fxy(3,j,k) = z component of force/charge at grid (j,k)
! that is, convolution of electric field over particle shape
! bxy(1,j,k) = x component of magnetic field at grid (j,k)
! bxy(2,j,k) = y component of magnetic field at grid (j,k)
! bxy(3,j,k) = z component of magnetic field at grid (j,k)
! that is, the convolution of magnetic field over particle shape
! kpic = number of particles per tile
! qbm = particle charge/mass ratio
! dt = time interval between successive calculations
! dtc = time interval between successive co-ordinate calculations
! kinetic energy/mass at time t is also calculated, using
! ek = .5*sum((vx(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt)**2 +
!      (vy(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt)**2 + 
!      (vz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt)**2)
! idimp = size of phase space = 5
! nppmx = maximum number of particles in tile
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of field arrays, must be >= nx+1
! nyv = second dimension of field arrays, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
! ipbc = particle boundary condition = (0,1,2,3) =
! (none,2d periodic,2d reflecting,mixed reflecting/periodic)
      implicit none
      integer, value :: idimp, nppmx, nx, ny, mx, my, nxv, nyv
      integer, value :: mx1, mxy1, ipbc
      real, value :: qbm, dt, dtc
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(3,nxv,nyv) :: fxy, bxy
      integer, dimension(mxy1) :: kpic
      real, dimension(mxy1) :: ek
! local data
      integer :: noff, moff, npp, mxv
      integer :: i, j, k, ii, nn, mm, nm, b
      real :: qtmh, edgelx, edgely, edgerx, edgery, dxp, dyp, amx, amy
      real :: dx, dy, dz, ox, oy, oz, acx, acy, acz, omxt, omyt, omzt
      real :: omt, anorm
      real :: rot1, rot2, rot3, rot4, rot5, rot6, rot7, rot8, rot9
      real :: x, y
! The sizes of the shared memory arrays are as follows:
! real sfxy(3*(mx+1)*(my+1)), sbxy(3*(mx+1)*(my+1))
! real sek(blockDim%x)
! to conserve memory, sek overlaps with sfxy and sbxy
! and the name sfxy is used instead of sek
      real, shared, dimension(*) :: sfxy
      double precision :: sum1
      b = 3*(mx+1)*(my+1)
      qtmh = 0.5*qbm*dt
      sum1 = 0.0d0
! set boundary values
      edgelx = 0.0
      edgely = 0.0
      edgerx = real(nx)
      edgery = real(ny)
      if (ipbc==2) then
         edgelx = 1.0
         edgely = 1.0
         edgerx = real(nx-1)
         edgery = real(ny-1)
      else if (ipbc==3) then
         edgelx = 1.0
         edgerx = real(nx-1)
      endif
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
! load local fields from global array
         nn = min(mx,nx-noff) + 1
         mm = min(my,ny-moff) + 1
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
               sfxy(3*ii-2) = fxy(1,i+noff,j+moff)
               sfxy(3*ii-1) = fxy(2,i+noff,j+moff)
               sfxy(3*ii) = fxy(3,i+noff,j+moff)
            endif
            ii = ii + blockDim%x
         enddo
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
               sfxy(b+3*ii-2) = bxy(1,i+noff,j+moff)
               sfxy(b+3*ii-1) = bxy(2,i+noff,j+moff)
               sfxy(b+3*ii) = bxy(3,i+noff,j+moff)
            endif
            ii = ii + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! loop over particles in tile
         j = threadIdx%x
         do while (j <= npp)
! find interpolation weights
            x = ppart(j,1,k)
            y = ppart(j,2,k)
            nn = x
            mm = y
            dxp = x - real(nn)
            dyp = y - real(mm)
            nm = 3*(nn - noff) + 3*mxv*(mm - moff) + 1
            amx = 1.0 - dxp
            amy = 1.0 - dyp
! find electric field
            nn = nm
            dx = amx*sfxy(nn)
            dy = amx*sfxy(nn+1)
            dz = amx*sfxy(nn+2)
            mm = nn + 3
            dx = amy*(dxp*sfxy(mm) + dx)
            dy = amy*(dxp*sfxy(mm+1) + dy)
            dz = amy*(dxp*sfxy(mm+2) + dz)
            nn = nn + 3*mxv
            acx = amx*sfxy(nn)
            acy = amx*sfxy(nn+1)
            acz = amx*sfxy(nn+2)
            mm = nn + 3
            dx = dx + dyp*(dxp*sfxy(mm) + acx)
            dy = dy + dyp*(dxp*sfxy(mm+1) + acy)
            dz = dz + dyp*(dxp*sfxy(mm+2) + acz)
! find magnetic field
            nn = nm + b
            ox = amx*sfxy(nn)
            oy = amx*sfxy(nn+1)
            oz = amx*sfxy(nn+2)
            mm = nn + 3
            ox = amy*(dxp*sfxy(mm) + ox)
            oy = amy*(dxp*sfxy(mm+1) + oy)
            oz = amy*(dxp*sfxy(mm+2) + oz)
            nn = nn + 3*mxv
            acx = amx*sfxy(nn)
            acy = amx*sfxy(nn+1)
            acz = amx*sfxy(nn+2)
            mm = nn + 3
            ox = ox + dyp*(dxp*sfxy(mm) + acx)
            oy = oy + dyp*(dxp*sfxy(mm+1) + acy)
            oz = oz + dyp*(dxp*sfxy(mm+2) + acz)
! calculate half impulse
            dx = qtmh*dx
            dy = qtmh*dy
            dz = qtmh*dz
! half acceleration
            acx = ppart(j,3,k) + dx
            acy = ppart(j,4,k) + dy
            acz = ppart(j,5,k) + dz
! time-centered kinetic energy
            sum1 = sum1 + (acx*acx + acy*acy + acz*acz)
! calculate cyclotron frequency
            omxt = qtmh*ox
            omyt = qtmh*oy
            omzt = qtmh*oz
! calculate rotation matrix
            omt = omxt*omxt + omyt*omyt + omzt*omzt
            anorm = 2.0/(1.0 + omt)
            omt = 0.5*(1.0 - omt)
            rot4 = omxt*omyt
            rot7 = omxt*omzt
            rot8 = omyt*omzt
            rot1 = omt + omxt*omxt
            rot5 = omt + omyt*omyt
            rot9 = omt + omzt*omzt
            rot2 = omzt + rot4
            rot4 = -omzt + rot4
            rot3 = -omyt + rot7
            rot7 = omyt + rot7
            rot6 = omxt + rot8
            rot8 = -omxt + rot8
! new velocity
            dx = (rot1*acx + rot2*acy + rot3*acz)*anorm + dx
            dy = (rot4*acx + rot5*acy + rot6*acz)*anorm + dy
            dz = (rot7*acx + rot8*acy + rot9*acz)*anorm + dz
            ppart(j,3,k) = dx
            ppart(j,4,k) = dy
            ppart(j,5,k) = dz
! new position
            dx = x + dx*dtc
            dy = y + dy*dtc
! reflecting boundary conditions
            if (ipbc==2) then
               if ((dx < edgelx).or.(dx >= edgerx)) then
                  dx = ppart(j,1,k)
                  ppart(j,3,k) = -ppart(j,3,k)
               endif
               if ((dy < edgely).or.(dy >= edgery)) then
                  dy = ppart(j,2,k)
                  ppart(j,4,k) = -ppart(j,4,k)
               endif
! mixed reflecting/periodic boundary conditions
            else if (ipbc==3) then
               if ((dx < edgelx).or.(dx >= edgerx)) then
                  dx = ppart(j,1,k)
                  ppart(j,3,k) = -ppart(j,3,k)
               endif
            endif
! set new position
            ppart(j,1,k) = dx
            ppart(j,2,k) = dy
            j = j + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! add kinetic energies in tile
         sfxy(threadIdx%x) = real(sum1)
! synchronize threads
         call syncthreads()
         call lsum2(sfxy,blockDim%x)
! normalize kinetic energy of tile
         if (threadIdx%x==1) ek(k) = 0.5*sfxy(1)
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpubppushf23l(ppart,fxy,bxy,kpic,ncl&
     &,ihole,qbm,dt,dtc,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,    &
     &ntmax,irc)
! for 2-1/2d code, this subroutine updates particle co-ordinates and
! velocities using leap-frog scheme in time and first-order linear
! interpolation in space, with magnetic field. Using the Boris Mover.
! with periodic boundary conditions.
! also determines list of particles which are leaving this tile
! threaded version using guard cells
! data read in tiles
! particles stored segmented array
! 119 flops/particle, 1 divide, 29 loads, 5 stores
! input: all except ncl, ihole, irc, output: ppart, ncl, ihole, irc, ek
! velocity equations used are:
! vx(t+dt/2) = rot(1)*(vx(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(2)*(vy(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(3)*(vz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fx(x(t),y(t))*dt)
! vy(t+dt/2) = rot(4)*(vx(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(5)*(vy(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(6)*(vz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fy(x(t),y(t))*dt)
! vz(t+dt/2) = rot(7)*(vx(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(8)*(vy(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(9)*(vz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fz(x(t),y(t))*dt)
! where q/m is charge/mass, and the rotation matrix is given by:
!    rot(1) = (1 - (om*dt/2)**2 + 2*(omx*dt/2)**2)/(1 + (om*dt/2)**2)
!    rot(2) = 2*(omz*dt/2 + (omx*dt/2)*(omy*dt/2))/(1 + (om*dt/2)**2)
!    rot(3) = 2*(-omy*dt/2 + (omx*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(4) = 2*(-omz*dt/2 + (omx*dt/2)*(omy*dt/2))/(1 + (om*dt/2)**2)
!    rot(5) = (1 - (om*dt/2)**2 + 2*(omy*dt/2)**2)/(1 + (om*dt/2)**2)
!    rot(6) = 2*(omx*dt/2 + (omy*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(7) = 2*(omy*dt/2 + (omx*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(8) = 2*(-omx*dt/2 + (omy*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(9) = (1 - (om*dt/2)**2 + 2*(omz*dt/2)**2)/(1 + (om*dt/2)**2)
! and om**2 = omx**2 + omy**2 + omz**2
! the rotation matrix is determined by:
! omx = (q/m)*bx(x(t),y(t)), omy = (q/m)*by(x(t),y(t)), and
! omz = (q/m)*bz(x(t),y(t)).
! position equations used are:
! x(t+dt)=x(t) + vx(t+dt/2)*dt
! y(t+dt)=y(t) + vy(t+dt/2)*dt
! fx(x(t),y(t)), fy(x(t),y(t)), and fz(x(t),y(t))
! bx(x(t),y(t)), by(x(t),y(t)), and bz(x(t),y(t))
! are approximated by interpolation from the nearest grid points:
! fx(x,y) = (1-dy)*((1-dx)*fx(n,m)+dx*fx(n+1,m)) + dy*((1-dx)*fx(n,m+1)
!    + dx*fx(n+1,m+1))
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! similarly for fy(x,y), fz(x,y), bx(x,y), by(x,y), bz(x,y)
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! ppart(n,3,m) = velocity vx of particle n in tile m
! ppart(n,4,m) = velocity vy of particle n in tile m
! ppart(n,5,m) = velocity vz of particle n in tile m
! fxy(1,j,k) = x component of force/charge at grid (j,k)
! fxy(2,j,k) = y component of force/charge at grid (j,k)
! fxy(3,j,k) = z component of force/charge at grid (j,k)
! that is, convolution of electric field over particle shape
! bxy(1,j,k) = x component of magnetic field at grid (j,k)
! bxy(2,j,k) = y component of magnetic field at grid (j,k)
! bxy(3,j,k) = z component of magnetic field at grid (j,k)
! that is, the convolution of magnetic field over particle shape
! kpic(k) = number of particles in tile k
! ncl(i,k) = number of particles going to destination i, tile k
! ihole(1,:,k) = location of hole in array left by departing particle
! ihole(2,:,k) = destination of particle leaving hole
! ihole(1,1,k) = ih, number of holes left (error, if negative)
! qbm = particle charge/mass ratio
! dt = time interval between successive calculations
! dtc = time interval between successive co-ordinate calculations
! kinetic energy/mass at time t is also calculated, using
! ek = .5*sum((vx(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt)**2 +
!      (vy(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt)**2 + 
!      (vz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt)**2)
! idimp = size of phase space = 5
! nppmx = maximum number of particles in tile
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of field arrays, must be >= nx+1
! nyv = second dimension of field arrays, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
! ntmax = size of hole array for particles leaving tiles
! irc = maximum overflow, returned only if error occurs, when irc > 0
! optimized version
      implicit none
      integer, value :: idimp, nppmx, nx, ny, mx, my, nxv, nyv
      integer, value :: mx1, mxy1, ntmax
      real, value :: qbm, dt, dtc
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(3,nxv,nyv) :: fxy, bxy
      integer, dimension(mxy1) :: kpic
      integer, dimension(8,mxy1) :: ncl
      integer, dimension(2,ntmax+1,mxy1) :: ihole
      real, dimension(mxy1) :: ek
      integer, dimension(1) :: irc
! local data
      integer :: noff, moff, nhoff, mhoff, npp, mxv
      integer :: i, j, k, ii, ih, nn, mm, nm, b, old
      real :: qtmh, dxp, dyp, amx, amy, dx, dy, dz, ox, oy, oz
      real :: acx, acy, acz, omxt, omyt, omzt, omt, anorm
      real :: rot1, rot2, rot3, rot4, rot5, rot6, rot7, rot8, rot9
      real :: anx, any, edgelx, edgely, edgerx, edgery
      real :: x, y
! The sizes of the shared memory arrays are as follows:
! real sfxy(3*(mx+1)*(my+1)), sbxy(3*(mx+1)*(my+1))
! real sek(blockDim%x)
! integer sih(blockDim%x), sncl(8), nh(1)
! to conserve memory, sek overlaps with sfxy, sbxy, and sih
! and the name sfxy is used instead of sek
! In CUDA Fortran, multiple assumed-size shared memory arrays all start
! at the same memory location
      real, shared, dimension(*) :: sfxy
      integer, shared, dimension(*):: shm
      integer :: sih, sncl, nh
      double precision :: sum1
      sih = (6*(mx+1)*(my+1)*sizeof(x))/sizeof(i)
      sncl = sih + blockDim%x
      nh = blockDim%x*(sizeof(x)/sizeof(i))
      sncl = max(sncl,nh)
      nh = sncl + 8
      b = 3*(mx+1)*(my+1)
      qtmh = 0.5*qbm*dt
      anx = real(nx)
      any = real(ny)
      sum1 = 0.0d0
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
         nn = min(mx,nx-noff)
         mm = min(my,ny-moff)
         edgelx = noff
         edgerx = noff + nn
         edgely = moff
         edgery = moff + mm
! load local fields from global array
         nn = nn + 1
         mm = mm + 1
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
               sfxy(3*ii-2) = fxy(1,i+noff,j+moff)
               sfxy(3*ii-1) = fxy(2,i+noff,j+moff)
               sfxy(3*ii) = fxy(3,i+noff,j+moff)
            endif
            ii = ii + blockDim%x
         enddo
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
               sfxy(b+3*ii-2) = bxy(1,i+noff,j+moff)
               sfxy(b+3*ii-1) = bxy(2,i+noff,j+moff)
               sfxy(b+3*ii) = bxy(3,i+noff,j+moff)
            endif
            ii = ii + blockDim%x
         enddo
! clear counters
         j = threadIdx%x
         do while (j <= 8)
            shm(sncl+j) = 0
            j = j + blockDim%x
         enddo
         if (threadIdx%x==1) then
            shm(nh+1) = 0
         endif
! synchronize threads
         call syncthreads()
! loop over particles in tile
         ii = (npp - 1)/int(blockDim%x) + 1
         nhoff = 0
         do i = 1, ii
            j = threadIdx%x + blockDim%x*(i - 1)
            shm(sih+threadIdx%x) = 0
            if (j <= npp) then
! find interpolation weights
               x = ppart(j,1,k)
               y = ppart(j,2,k)
               nn = x
               mm = y
               dxp = x - real(nn)
               dyp = y - real(mm)
               nm = 3*(nn - noff) + 3*mxv*(mm - moff) + 1
               amx = 1.0 - dxp
               amy = 1.0 - dyp
! find electric field
               nn = nm
               dx = amx*sfxy(nn)
               dy = amx*sfxy(nn+1)
               dz = amx*sfxy(nn+2)
               mm = nn + 3
               dx = amy*(dxp*sfxy(mm) + dx)
               dy = amy*(dxp*sfxy(mm+1) + dy)
               dz = amy*(dxp*sfxy(mm+2) + dz)
               nn = nn + 3*mxv
               acx = amx*sfxy(nn)
               acy = amx*sfxy(nn+1)
               acz = amx*sfxy(nn+2)
               mm = nn + 3
               dx = dx + dyp*(dxp*sfxy(mm) + acx)
               dy = dy + dyp*(dxp*sfxy(mm+1) + acy)
               dz = dz + dyp*(dxp*sfxy(mm+2) + acz)
! find magnetic field
               nn = nm + b
               ox = amx*sfxy(nn)
               oy = amx*sfxy(nn+1)
               oz = amx*sfxy(nn+2)
               mm = nn + 3
               ox = amy*(dxp*sfxy(mm) + ox)
               oy = amy*(dxp*sfxy(mm+1) + oy)
               oz = amy*(dxp*sfxy(mm+2) + oz)
               nn = nn + 3*mxv
               acx = amx*sfxy(nn)
               acy = amx*sfxy(nn+1)
               acz = amx*sfxy(nn+2)
               mm = nn + 3
               ox = ox + dyp*(dxp*sfxy(mm) + acx)
               oy = oy + dyp*(dxp*sfxy(mm+1) + acy)
               oz = oz + dyp*(dxp*sfxy(mm+2) + acz)
! calculate half impulse
               dx = qtmh*dx
               dy = qtmh*dy
               dz = qtmh*dz
! half acceleration
               acx = ppart(j,3,k) + dx
               acy = ppart(j,4,k) + dy
               acz = ppart(j,5,k) + dz
! time-centered kinetic energy
               sum1 = sum1 + (acx*acx + acy*acy + acz*acz)
! calculate cyclotron frequency
               omxt = qtmh*ox
               omyt = qtmh*oy
               omzt = qtmh*oz
! calculate rotation matrix
               omt = omxt*omxt + omyt*omyt + omzt*omzt
               anorm = 2.0/(1.0 + omt)
               omt = 0.5*(1.0 - omt)
               rot4 = omxt*omyt
               rot7 = omxt*omzt
               rot8 = omyt*omzt
               rot1 = omt + omxt*omxt
               rot5 = omt + omyt*omyt
               rot9 = omt + omzt*omzt
               rot2 = omzt + rot4
               rot4 = -omzt + rot4
               rot3 = -omyt + rot7
               rot7 = omyt + rot7
               rot6 = omxt + rot8
               rot8 = -omxt + rot8
! new velocity
               dx = (rot1*acx + rot2*acy + rot3*acz)*anorm + dx
               dy = (rot4*acx + rot5*acy + rot6*acz)*anorm + dy
               dz = (rot7*acx + rot8*acy + rot9*acz)*anorm + dz
               ppart(j,3,k) = dx
               ppart(j,4,k) = dy
               ppart(j,5,k) = dz
! new position
               dx = x + dx*dtc
               dy = y + dy*dtc
! find particles going out of bounds
               mm = 0
! count how many particles are going in each direction in ncl
! save their address and destination in ihole
! use periodic boundary conditions and check for roundoff error
! mm = direction particle is going
               if (dx >= edgerx) then
                  if (dx >= anx) dx = dx - anx
                  mm = 2
               else if (dx < edgelx) then
                  if (dx < 0.0) then
                     dx = dx + anx
                     if (dx < anx) then
                        mm = 1
                     else
                        dx = 0.0
                     endif
                  else
                     mm = 1
                  endif
               endif
               if (dy >= edgery) then
                  if (dy >= any) dy = dy - any
                  mm = mm + 6
               else if (dy < edgely) then
                  if (dy < 0.0) then
                     dy = dy + any
                     if (dy < any) then
                        mm = mm + 3
                     else
                        dy = 0.0
                     endif
                  else
                     mm = mm + 3
                  endif
               endif
! using prefix scan for ih to keep holes ordered
               if (mm > 0) then
                  old = atomicAdd(shm(sncl+mm),1)
                  shm(sih+threadIdx%x) = 1
               endif
            endif
! synchronize threads
            call syncthreads()
            nn = npp - blockDim%x*(i - 1)
            if (nn > blockDim%x) nn = blockDim%x
! perform local prefix reduction
            call liscan2(shm(sih+1),nn)
            if (j <= npp) then
! set new position
               ppart(j,1,k) = dx
               ppart(j,2,k) = dy
! write out location and direction of departing particles
               ih = shm(sih+threadIdx%x)
               mhoff = 0
               if (threadIdx%x > 1) mhoff = shm(sih+threadIdx%x-1)
! this thread has a hole present
               if (ih > mhoff) then
                  ih = ih + nhoff
                  if (ih <= ntmax) then
                     ihole(1,ih+1,k) = j
                     ihole(2,ih+1,k) = mm
                  else
                     shm(nh+1) = 1
                  endif
               endif
            endif
! update number of holes in this iteration
            if (nn > 0) nhoff = nhoff + shm(sih+nn)
! synchronize threads
            call syncthreads()
         enddo
! add kinetic energies in tile
         sfxy(threadIdx%x) = real(sum1)
! synchronize threads
         call syncthreads()
         call lsum2(sfxy,blockDim%x)
! write out counters
         j = threadIdx%x
         do while (j <= 8)
            ncl(j,k) = shm(sncl+j)
            j = j + blockDim%x
         enddo
! set error and end of file flag
         if (threadIdx%x==1) then
! ihole overflow
            ih  = nhoff
            if (shm(nh+1) > 0) then
               irc(1) = ih
               ih = -ih
            endif
            ihole(1,1,k) = ih
! normalize kinetic energy of tile
            ek(k) = 0.5*sfxy(1)
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpurbppush23l(ppart,fxy,bxy,kpic,qbm&
     &,dt,dtc,ci,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
! for 2-1/2d code, this subroutine updates particle co-ordinates and
! velocities using leap-frog scheme in time and first-order linear
! interpolation in space, for relativistic particles with magnetic field
! Using the Boris Mover.
! threaded version using guard cells
! data read in tiles
! particles stored segmented array
! 131 flops/particle, 4 divides, 2 sqrts, 25 loads, 5 stores
! input: all, output: ppart, ek
! momentum equations used are:
! px(t+dt/2) = rot(1)*(px(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(2)*(py(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(3)*(pz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fx(x(t),y(t))*dt)
! py(t+dt/2) = rot(4)*(px(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(5)*(py(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(6)*(pz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fy(x(t),y(t))*dt)
! pz(t+dt/2) = rot(7)*(px(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(8)*(py(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(9)*(pz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fz(x(t),y(t))*dt)
! where q/m is charge/mass, and the rotation matrix is given by:
!    rot(1) = (1 - (om*dt/2)**2 + 2*(omx*dt/2)**2)/(1 + (om*dt/2)**2)
!    rot(2) = 2*(omz*dt/2 + (omx*dt/2)*(omy*dt/2))/(1 + (om*dt/2)**2)
!    rot(3) = 2*(-omy*dt/2 + (omx*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(4) = 2*(-omz*dt/2 + (omx*dt/2)*(omy*dt/2))/(1 + (om*dt/2)**2)
!    rot(5) = (1 - (om*dt/2)**2 + 2*(omy*dt/2)**2)/(1 + (om*dt/2)**2)
!    rot(6) = 2*(omx*dt/2 + (omy*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(7) = 2*(omy*dt/2 + (omx*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(8) = 2*(-omx*dt/2 + (omy*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(9) = (1 - (om*dt/2)**2 + 2*(omz*dt/2)**2)/(1 + (om*dt/2)**2)
! and om**2 = omx**2 + omy**2 + omz**2
! the rotation matrix is determined by:
! omx = (q/m)*bx(x(t),y(t))*gami, omy = (q/m)*by(x(t),y(t))*gami, and
! omz = (q/m)*bz(x(t),y(t))*gami,
! where gami = 1./sqrt(1.+(px(t)*px(t)+py(t)*py(t)+pz(t)*pz(t))*ci*ci)
! position equations used are:
! x(t+dt) = x(t) + px(t+dt/2)*dtg
! y(t+dt) = y(t) + py(t+dt/2)*dtg
! where dtg = dtc/sqrt(1.+(px(t+dt/2)*px(t+dt/2)+py(t+dt/2)*py(t+dt/2)+
! pz(t+dt/2)*pz(t+dt/2))*ci*ci)
! fx(x(t),y(t)), fy(x(t),y(t)), and fz(x(t),y(t))
! bx(x(t),y(t)), by(x(t),y(t)), and bz(x(t),y(t))
! are approximated by interpolation from the nearest grid points:
! fx(x,y) = (1-dy)*((1-dx)*fx(n,m)+dx*fx(n+1,m)) + dy*((1-dx)*fx(n,m+1)
!    + dx*fx(n+1,m+1))
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! similarly for fy(x,y), fz(x,y), bx(x,y), by(x,y), bz(x,y)
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! ppart(n,3,m) = x momentum of particle n in tile m
! ppart(n,4,m) = y momentum of particle n in tile m
! ppart(n,5,m) = z momentum of particle n in tile m
! fxy(1,j,k) = x component of force/charge at grid (j,k)
! fxy(2,j,k) = y component of force/charge at grid (j,k)
! fxy(3,j,k) = z component of force/charge at grid (j,k)
! that is, convolution of electric field over particle shape
! bxy(1,j,k) = x component of magnetic field at grid (j,k)
! bxy(2,j,k) = y component of magnetic field at grid (j,k)
! bxy(3,j,k) = z component of magnetic field at grid (j,k)
! that is, the convolution of magnetic field over particle shape
! kpic = number of particles per tile
! qbm = particle charge/mass ratio
! dt = time interval between successive calculations
! dtc = time interval between successive co-ordinate calculations
! ci = reciprical of velocity of light
! kinetic energy/mass at time t is also calculated, using
! ek = gami*sum((px(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt)**2 +
!      (py(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt)**2 +
!      (pz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt)**2)/(1. + gami)
! idimp = size of phase space = 5
! nppmx = maximum number of particles in tile
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of field arrays, must be >= nx+1
! nyv = second dimension of field arrays, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
! ipbc = particle boundary condition = (0,1,2,3) =
! (none,2d periodic,2d reflecting,mixed reflecting/periodic)
      implicit none
      integer, value :: idimp, nppmx, nx, ny, mx, my, nxv, nyv
      integer, value :: mx1, mxy1, ipbc
      real, value :: qbm, dt, dtc, ci
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(3,nxv,nyv) :: fxy, bxy
      integer, dimension(mxy1) :: kpic
      real, dimension(mxy1) :: ek
! local data
      integer :: noff, moff, npp, mxv
      integer :: i, j, k, ii, nn, mm, nm, b
      real :: qtmh, ci2, edgelx, edgely, edgerx, edgery, dxp, dyp
      real :: amx, amy, dx, dy, dz, ox, oy, oz, acx, acy, acz
      real :: p2, gami, qtmg, dtg, omxt, omyt, omzt, omt, anorm
      real :: rot1, rot2, rot3, rot4, rot5, rot6, rot7, rot8, rot9
      real :: x, y
! The sizes of the shared memory arrays are as follows:
! real sfxy(3*(mx+1)*(my+1)), sbxy(3*(mx+1)*(my+1))
! real sek(blockDim%x)
! to conserve memory, sek overlaps with sfxy and sbxy
! and the name sfxy is used instead of sek
      real, shared, dimension(*) :: sfxy
      double precision :: sum1
      b = 3*(mx+1)*(my+1)
      qtmh = 0.5*qbm*dt
      ci2 = ci*ci
      sum1 = 0.0d0
! set boundary values
      edgelx = 0.0
      edgely = 0.0
      edgerx = real(nx)
      edgery = real(ny)
      if (ipbc==2) then
         edgelx = 1.0
         edgely = 1.0
         edgerx = real(nx-1)
         edgery = real(ny-1)
      else if (ipbc==3) then
         edgelx = 1.0
         edgerx = real(nx-1)
      endif
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
! load local fields from global array
         nn = min(mx,nx-noff) + 1
         mm = min(my,ny-moff) + 1
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
               sfxy(3*ii-2) = fxy(1,i+noff,j+moff)
               sfxy(3*ii-1) = fxy(2,i+noff,j+moff)
               sfxy(3*ii) = fxy(3,i+noff,j+moff)
            endif
            ii = ii + blockDim%x
         enddo
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
               sfxy(b+3*ii-2) = bxy(1,i+noff,j+moff)
               sfxy(b+3*ii-1) = bxy(2,i+noff,j+moff)
               sfxy(b+3*ii) = bxy(3,i+noff,j+moff)
            endif
            ii = ii + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! loop over particles in tile
         j = threadIdx%x
         do while (j <= npp)
! find interpolation weights
            x = ppart(j,1,k)
            y = ppart(j,2,k)
            nn = x
            mm = y
            dxp = x - real(nn)
            dyp = y - real(mm)
            nm = 3*(nn - noff) + 3*mxv*(mm - moff) + 1
            amx = 1.0 - dxp
            amy = 1.0 - dyp
! find electric field
            nn = nm
            dx = amx*sfxy(nn)
            dy = amx*sfxy(nn+1)
            dz = amx*sfxy(nn+2)
            mm = nn + 3
            dx = amy*(dxp*sfxy(mm) + dx)
            dy = amy*(dxp*sfxy(mm+1) + dy)
            dz = amy*(dxp*sfxy(mm+2) + dz)
            nn = nn + 3*mxv
            acx = amx*sfxy(nn)
            acy = amx*sfxy(nn+1)
            acz = amx*sfxy(nn+2)
            mm = nn + 3
            dx = dx + dyp*(dxp*sfxy(mm) + acx)
            dy = dy + dyp*(dxp*sfxy(mm+1) + acy)
            dz = dz + dyp*(dxp*sfxy(mm+2) + acz)
! find magnetic field
            nn = nm + b
            ox = amx*sfxy(nn)
            oy = amx*sfxy(nn+1)
            oz = amx*sfxy(nn+2)
            mm = nn + 3
            ox = amy*(dxp*sfxy(mm) + ox)
            oy = amy*(dxp*sfxy(mm+1) + oy)
            oz = amy*(dxp*sfxy(mm+2) + oz)
            nn = nn + 3*mxv
            acx = amx*sfxy(nn)
            acy = amx*sfxy(nn+1)
            acz = amx*sfxy(nn+2)
            mm = nn + 3
            ox = ox + dyp*(dxp*sfxy(mm) + acx)
            oy = oy + dyp*(dxp*sfxy(mm+1) + acy)
            oz = oz + dyp*(dxp*sfxy(mm+2) + acz)
! calculate half impulse
            dx = qtmh*dx
            dy = qtmh*dy
            dz = qtmh*dz
! half acceleration
            acx = ppart(j,3,k) + dx
            acy = ppart(j,4,k) + dy
            acz = ppart(j,5,k) + dz
! find inverse gamma
            p2 = acx*acx + acy*acy + acz*acz
            gami = 1.0/sqrt(1.0 + p2*ci2)
! renormalize magnetic field
            qtmg = qtmh*gami
! time-centered kinetic energy
            sum1 = sum1 + gami*p2/(1.0 + gami)
! calculate cyclotron frequency
            omxt = qtmg*ox
            omyt = qtmg*oy
            omzt = qtmg*oz
! calculate rotation matrix
            omt = omxt*omxt + omyt*omyt + omzt*omzt
            anorm = 2.0/(1.0 + omt)
            omt = 0.5*(1.0 - omt)
            rot4 = omxt*omyt
            rot7 = omxt*omzt
            rot8 = omyt*omzt
            rot1 = omt + omxt*omxt
            rot5 = omt + omyt*omyt
            rot9 = omt + omzt*omzt
            rot2 = omzt + rot4
            rot4 = -omzt + rot4
            rot3 = -omyt + rot7
            rot7 = omyt + rot7
            rot6 = omxt + rot8
            rot8 = -omxt + rot8
! new momentum
            dx = (rot1*acx + rot2*acy + rot3*acz)*anorm + dx
            dy = (rot4*acx + rot5*acy + rot6*acz)*anorm + dy
            dz = (rot7*acx + rot8*acy + rot9*acz)*anorm + dz
            ppart(j,3,k) = dx
            ppart(j,4,k) = dy
            ppart(j,5,k) = dz
! update inverse gamma
            p2 = dx*dx + dy*dy + dz*dz
            dtg = dtc/sqrt(1.0 + p2*ci2)
! new position
            dx = x + dx*dtg
            dy = y + dy*dtg
! reflecting boundary conditions
            if (ipbc==2) then
               if ((dx < edgelx).or.(dx >= edgerx)) then
                  dx = ppart(j,1,k)
                  ppart(j,3,k) = -ppart(j,3,k)
               endif
               if ((dy < edgely).or.(dy >= edgery)) then
                  dy = ppart(j,2,k)
                  ppart(j,4,k) = -ppart(j,4,k)
               endif
! mixed reflecting/periodic boundary conditions
            else if (ipbc==3) then
               if ((dx < edgelx).or.(dx >= edgerx)) then
                  dx = ppart(j,1,k)
                  ppart(j,3,k) = -ppart(j,3,k)
               endif
            endif
! set new position
            ppart(j,1,k) = dx
            ppart(j,2,k) = dy
            j = j + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! add kinetic energies in tile
         sfxy(threadIdx%x) = real(sum1)
! synchronize threads
         call syncthreads()
         call lsum2(sfxy,blockDim%x)
! normalize kinetic energy of tile
         if (threadIdx%x==1) ek(k) = sfxy(1)
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpurbppushf23l(ppart,fxy,bxy,kpic,  &
     &ncl,ihole,qbm,dt,dtc,ci,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,   &
     &mxy1,ntmax,irc)
! for 2-1/2d code, this subroutine updates particle co-ordinates and
! velocities using leap-frog scheme in time and first-order linear
! interpolation in space, for relativistic particles with magnetic field
! with periodic boundary conditions.
! Using the Boris Mover.
! also determines list of particles which are leaving this tile
! threaded version using guard cells
! data read in tiles
! particles stored segmented array
! 131 flops/particle, 4 divides, 2 sqrts, 25 loads, 5 stores
! input: all except ncl, ihole, irc, output: ppart, ncl, ihole, irc, ek
! momentum equations used are:
! px(t+dt/2) = rot(1)*(px(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(2)*(py(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(3)*(pz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fx(x(t),y(t))*dt)
! py(t+dt/2) = rot(4)*(px(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(5)*(py(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(6)*(pz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fy(x(t),y(t))*dt)
! pz(t+dt/2) = rot(7)*(px(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt) +
!    rot(8)*(py(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt) +
!    rot(9)*(pz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt) +
!    .5*(q/m)*fz(x(t),y(t))*dt)
! where q/m is charge/mass, and the rotation matrix is given by:
!    rot(1) = (1 - (om*dt/2)**2 + 2*(omx*dt/2)**2)/(1 + (om*dt/2)**2)
!    rot(2) = 2*(omz*dt/2 + (omx*dt/2)*(omy*dt/2))/(1 + (om*dt/2)**2)
!    rot(3) = 2*(-omy*dt/2 + (omx*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(4) = 2*(-omz*dt/2 + (omx*dt/2)*(omy*dt/2))/(1 + (om*dt/2)**2)
!    rot(5) = (1 - (om*dt/2)**2 + 2*(omy*dt/2)**2)/(1 + (om*dt/2)**2)
!    rot(6) = 2*(omx*dt/2 + (omy*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(7) = 2*(omy*dt/2 + (omx*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(8) = 2*(-omx*dt/2 + (omy*dt/2)*(omz*dt/2))/(1 + (om*dt/2)**2)
!    rot(9) = (1 - (om*dt/2)**2 + 2*(omz*dt/2)**2)/(1 + (om*dt/2)**2)
! and om**2 = omx**2 + omy**2 + omz**2
! the rotation matrix is determined by:
! omx = (q/m)*bx(x(t),y(t))*gami, omy = (q/m)*by(x(t),y(t))*gami, and
! omz = (q/m)*bz(x(t),y(t))*gami,
! where gami = 1./sqrt(1.+(px(t)*px(t)+py(t)*py(t)+pz(t)*pz(t))*ci*ci)
! position equations used are:
! x(t+dt) = x(t) + px(t+dt/2)*dtg
! y(t+dt) = y(t) + py(t+dt/2)*dtg
! where dtg = dtc/sqrt(1.+(px(t+dt/2)*px(t+dt/2)+py(t+dt/2)*py(t+dt/2)+
! pz(t+dt/2)*pz(t+dt/2))*ci*ci)
! fx(x(t),y(t)), fy(x(t),y(t)), and fz(x(t),y(t))
! bx(x(t),y(t)), by(x(t),y(t)), and bz(x(t),y(t))
! are approximated by interpolation from the nearest grid points:
! fx(x,y) = (1-dy)*((1-dx)*fx(n,m)+dx*fx(n+1,m)) + dy*((1-dx)*fx(n,m+1)
!    + dx*fx(n+1,m+1))
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! similarly for fy(x,y), fz(x,y), bx(x,y), by(x,y), bz(x,y)
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! ppart(n,3,m) = x momentum of particle n in tile m
! ppart(n,4,m) = y momentum of particle n in tile m
! ppart(n,5,m) = z momentum of particle n in tile m
! fxy(1,j,k) = x component of force/charge at grid (j,k)
! fxy(2,j,k) = y component of force/charge at grid (j,k)
! fxy(3,j,k) = z component of force/charge at grid (j,k)
! that is, convolution of electric field over particle shape
! bxy(1,j,k) = x component of magnetic field at grid (j,k)
! bxy(2,j,k) = y component of magnetic field at grid (j,k)
! bxy(3,j,k) = z component of magnetic field at grid (j,k)
! that is, the convolution of magnetic field over particle shape
! kpic(k) = number of particles in tile k
! ncl(i,k) = number of particles going to destination i, tile k
! ihole(1,:,k) = location of hole in array left by departing particle
! ihole(2,:,k) = destination of particle leaving hole
! ihole(1,1,k) = ih, number of holes left (error, if negative)
! qbm = particle charge/mass ratio
! dt = time interval between successive calculations
! dtc = time interval between successive co-ordinate calculations
! ci = reciprical of velocity of light
! kinetic energy/mass at time t is also calculated, using
! ek = gami*sum((px(t-dt/2) + .5*(q/m)*fx(x(t),y(t))*dt)**2 +
!      (py(t-dt/2) + .5*(q/m)*fy(x(t),y(t))*dt)**2 +
!      (pz(t-dt/2) + .5*(q/m)*fz(x(t),y(t))*dt)**2)/(1. + gami)
! idimp = size of phase space = 5
! nppmx = maximum number of particles in tile
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of field arrays, must be >= nx+1
! nyv = second dimension of field arrays, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
! ntmax = size of hole array for particles leaving tiles
! irc = maximum overflow, returned only if error occurs, when irc > 0
! optimized version
      implicit none
      integer, value :: idimp, nppmx, nx, ny, mx, my, nxv, nyv
      integer, value :: mx1, mxy1, ntmax
      real, value :: qbm, dt, dtc, ci
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(3,nxv,nyv) :: fxy, bxy
      integer, dimension(mxy1) :: kpic
      integer, dimension(8,mxy1) :: ncl
      integer, dimension(2,ntmax+1,mxy1) :: ihole
      real, dimension(mxy1) :: ek
      integer, dimension(1) :: irc
! local data
      integer :: noff, moff, nhoff, mhoff, npp, mxv
      integer :: i, j, k, ii, ih, nn, mm, nm, b, old
      real :: qtmh, ci2, dxp, dyp, amx, amy, dx, dy, dz, ox, oy, oz
      real :: acx, acy, acz, p2, gami, qtmg, dtg, omxt, omyt, omzt, omt
      real :: rot1, rot2, rot3, rot4, rot5, rot6, rot7, rot8, rot9
      real :: anorm, anx, any, edgelx, edgely, edgerx, edgery
      real :: x, y
! The sizes of the shared memory arrays are as follows:
! real sfxy(3*(mx+1)*(my+1)), sbxy(3*(mx+1)*(my+1))
! real sek(blockDim%x)
! integer sih(blockDim%x), sncl(8), nh(1)
! to conserve memory, sek overlaps with sfxy, sbxy, and sih
! and the name sfxy is used instead of sek
! In CUDA Fortran, multiple assumed-size shared memory arrays all start
! at the same memory location
      real, shared, dimension(*) :: sfxy
      integer, shared, dimension(*):: shm
      integer :: sih, sncl, nh
      double precision :: sum1
      sih = (6*(mx+1)*(my+1)*sizeof(x))/sizeof(i)
      sncl = sih + blockDim%x
      nh = blockDim%x*(sizeof(x)/sizeof(i))
      sncl = max(sncl,nh)
      nh = sncl + 8
      b = 3*(mx+1)*(my+1)
      qtmh = 0.5*qbm*dt
      ci2 = ci*ci
      anx = real(nx)
      any = real(ny)
      sum1 = 0.0d0
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
         nn = min(mx,nx-noff)
         mm = min(my,ny-moff)
         edgelx = noff
         edgerx = noff + nn
         edgely = moff
         edgery = moff + mm
! load local fields from global array
         nn = nn + 1
         mm = mm + 1
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
               sfxy(3*ii-2) = fxy(1,i+noff,j+moff)
               sfxy(3*ii-1) = fxy(2,i+noff,j+moff)
               sfxy(3*ii) = fxy(3,i+noff,j+moff)
            endif
            ii = ii + blockDim%x
         enddo
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
               sfxy(b+3*ii-2) = bxy(1,i+noff,j+moff)
               sfxy(b+3*ii-1) = bxy(2,i+noff,j+moff)
               sfxy(b+3*ii) = bxy(3,i+noff,j+moff)
            endif
            ii = ii + blockDim%x
         enddo
! clear counters
         j = threadIdx%x
         do while (j <= 8)
            shm(sncl+j) = 0
            j = j + blockDim%x
         enddo
         if (threadIdx%x==1) then
            shm(nh+1) = 0
         endif
! synchronize threads
         call syncthreads()
! loop over particles in tile
         ii = (npp - 1)/int(blockDim%x) + 1
         nhoff = 0
         do i = 1, ii
            j = threadIdx%x + blockDim%x*(i - 1)
            shm(sih+threadIdx%x) = 0
            if (j <= npp) then
! find interpolation weights
               x = ppart(j,1,k)
               y = ppart(j,2,k)
               nn = x
               mm = y
               dxp = x - real(nn)
               dyp = y - real(mm)
               nm = 3*(nn - noff) + 3*mxv*(mm - moff) + 1
               amx = 1.0 - dxp
               amy = 1.0 - dyp
! find electric field
               nn = nm
               dx = amx*sfxy(nn)
               dy = amx*sfxy(nn+1)
               dz = amx*sfxy(nn+2)
               mm = nn + 3
               dx = amy*(dxp*sfxy(mm) + dx)
               dy = amy*(dxp*sfxy(mm+1) + dy)
               dz = amy*(dxp*sfxy(mm+2) + dz)
               nn = nn + 3*mxv
               acx = amx*sfxy(nn)
               acy = amx*sfxy(nn+1)
               acz = amx*sfxy(nn+2)
               mm = nn + 3
               dx = dx + dyp*(dxp*sfxy(mm) + acx)
               dy = dy + dyp*(dxp*sfxy(mm+1) + acy)
               dz = dz + dyp*(dxp*sfxy(mm+2) + acz)
! find magnetic field
               nn = nm + b
               ox = amx*sfxy(nn)
               oy = amx*sfxy(nn+1)
               oz = amx*sfxy(nn+2)
               mm = nn + 3
               ox = amy*(dxp*sfxy(mm) + ox)
               oy = amy*(dxp*sfxy(mm+1) + oy)
               oz = amy*(dxp*sfxy(mm+2) + oz)
               nn = nn + 3*mxv
               acx = amx*sfxy(nn)
               acy = amx*sfxy(nn+1)
               acz = amx*sfxy(nn+2)
               mm = nn + 3
               ox = ox + dyp*(dxp*sfxy(mm) + acx)
               oy = oy + dyp*(dxp*sfxy(mm+1) + acy)
               oz = oz + dyp*(dxp*sfxy(mm+2) + acz)
! calculate half impulse
               dx = qtmh*dx
               dy = qtmh*dy
               dz = qtmh*dz
! half acceleration
               acx = ppart(j,3,k) + dx
               acy = ppart(j,4,k) + dy
               acz = ppart(j,5,k) + dz
! find inverse gamma 
               p2 = acx*acx + acy*acy + acz*acz
               gami = 1.0/sqrt(1.0 + p2*ci2)
! renormalize magnetic field
               qtmg = qtmh*gami
! time-centered kinetic energy
               sum1 = sum1 + gami*p2/(1.0 + gami)
! calculate cyclotron frequency
               omxt = qtmg*ox
               omyt = qtmg*oy
               omzt = qtmg*oz
! calculate rotation matrix
               omt = omxt*omxt + omyt*omyt + omzt*omzt
               anorm = 2.0/(1.0 + omt)
               omt = 0.5*(1.0 - omt)
               rot4 = omxt*omyt
               rot7 = omxt*omzt
               rot8 = omyt*omzt
               rot1 = omt + omxt*omxt
               rot5 = omt + omyt*omyt
               rot9 = omt + omzt*omzt
               rot2 = omzt + rot4
               rot4 = -omzt + rot4
               rot3 = -omyt + rot7
               rot7 = omyt + rot7
               rot6 = omxt + rot8
               rot8 = -omxt + rot8
! new momentum
               dx = (rot1*acx + rot2*acy + rot3*acz)*anorm + dx
               dy = (rot4*acx + rot5*acy + rot6*acz)*anorm + dy
               dz = (rot7*acx + rot8*acy + rot9*acz)*anorm + dz
               ppart(j,3,k) = dx
               ppart(j,4,k) = dy
               ppart(j,5,k) = dz
! update inverse gamma
               p2 = dx*dx + dy*dy + dz*dz
               dtg = dtc/sqrt(1.0 + p2*ci2)
! new position
               dx = x + dx*dtg
               dy = y + dy*dtg
! find particles going out of bounds
               mm = 0
! count how many particles are going in each direction in ncl
! save their address and destination in ihole
! use periodic boundary conditions and check for roundoff error
! mm = direction particle is going
               if (dx >= edgerx) then
                  if (dx >= anx) dx = dx - anx
                  mm = 2
               else if (dx < edgelx) then
                  if (dx < 0.0) then
                     dx = dx + anx
                     if (dx < anx) then
                        mm = 1
                     else
                        dx = 0.0
                     endif
                  else
                     mm = 1
                  endif
               endif
               if (dy >= edgery) then
                  if (dy >= any) dy = dy - any
                  mm = mm + 6
               else if (dy < edgely) then
                  if (dy < 0.0) then
                     dy = dy + any
                     if (dy < any) then
                        mm = mm + 3
                     else
                        dy = 0.0
                     endif
                  else
                     mm = mm + 3
                  endif
               endif
! using prefix scan for ih to keep holes ordered
               if (mm > 0) then
                  old = atomicAdd(shm(sncl+mm),1)
                  shm(sih+threadIdx%x) = 1
               endif
            endif
! synchronize threads
            call syncthreads()
            nn = npp - blockDim%x*(i - 1)
            if (nn > blockDim%x) nn = blockDim%x
! perform local prefix reduction
            call liscan2(shm(sih+1),nn)
            if (j <= npp) then
! set new position
               ppart(j,1,k) = dx
               ppart(j,2,k) = dy
! write out location and direction of departing particles
               ih = shm(sih+threadIdx%x)
               mhoff = 0
               if (threadIdx%x > 1) mhoff = shm(sih+threadIdx%x-1)
! this thread has a hole present
               if (ih > mhoff) then
                  ih = ih + nhoff
                  if (ih <= ntmax) then
                     ihole(1,ih+1,k) = j
                     ihole(2,ih+1,k) = mm
                  else
                     shm(nh+1) = 1
                  endif
               endif
            endif
! update number of holes in this iteration
            if (nn > 0) nhoff = nhoff + shm(sih+nn)
! synchronize threads
            call syncthreads()
         enddo
! add kinetic energies in tile
         sfxy(threadIdx%x) = real(sum1)
! synchronize threads
         call syncthreads()
         call lsum2(sfxy,blockDim%x)
! write out counters
         j = threadIdx%x
         do while (j <= 8)
            ncl(j,k) = shm(sncl+j)
            j = j + blockDim%x
         enddo
! set error and end of file flag
         if (threadIdx%x==1) then
! ihole overflow
            ih  = nhoff
            if (shm(nh+1) > 0) then
               irc(1) = ih
               ih = -ih
            endif
            ihole(1,1,k) = ih
! normalize kinetic energy of tile
            ek(k) = sfxy(1)
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpu2ppost2l(ppart,q,kpic,qm,nppmx,  &
     &idimp,mx,my,nxv,nyv,mx1,mxy1)
! for 2d code, this subroutine calculates particle charge density
! using first-order linear interpolation, periodic boundaries
! threaded version using guard cells
! data deposited in tiles
! particles stored segmented array
! 17 flops/particle, 6 loads, 4 stores
! input: all, output: q
! charge density is approximated by values at the nearest grid points
! q(n,m)=qm*(1.-dx)*(1.-dy)
! q(n+1,m)=qm*dx*(1.-dy)
! q(n,m+1)=qm*(1.-dx)*dy
! q(n+1,m+1)=qm*dx*dy
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! q(j,k) = charge density at grid point j,k
! kpic = number of particles per tile
! qm = charge on particle, in units of e
! nppmx = maximum number of particles in tile
! idimp = size of phase space = 4
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of charge array, must be >= nx+1
! nyv = second dimension of charge array, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
      implicit none
      integer, value :: nppmx, idimp, mx, my, nxv, nyv, mx1, mxy1
      real, value :: qm
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(nxv,nyv) :: q
      integer, dimension(mxy1) :: kpic
! local data
      integer :: noff, moff, npp, mxv
      integer :: i, j, k, ii, nn, mm, np, mp
      real :: dxp, dyp, amx, amy, old
! The size of the shared memory array is as follows:
! real sq((mx+1)*(my+1)) 
      real, shared, dimension((mx+1)*(my+1)) :: sq
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
! zero out local accumulator
         i = threadIdx%x
         do while (i <= mxv*(my+1))
            sq(i) = 0.0
            i = i + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! loop over particles in tile
         j = threadIdx%x
         do while (j <= npp)
! find interpolation weights
            dxp = ppart(j,1,k)
            nn = dxp
            dyp = ppart(j,2,k)
            mm = dyp
            dxp = qm*(dxp - real(nn))
            dyp = dyp - real(mm)
            nn = nn - noff + 1
            mm = mxv*(mm - moff)
            amx = qm - dxp
            mp = mm + mxv
            amy = 1.0 - dyp
            np = nn + 1
! deposit charge within tile to local accumulator
! original deposit charge, has data hazard on GPU
!           sq(np+mp) = sq(np+mp) + dxp*dyp
!           sq(nn+mp) = sq(nn+mp) + amx*dyp
!           sq(np+mm) = sq(np+mm) + dxp*amy
!           sq(nn+mm) = sq(nn+mm) + amx*amy
! for devices with compute capability 2.x
            old = atomicAdd(sq(np+mp),dxp*dyp)
            old = atomicAdd(sq(nn+mp),amx*dyp)
            old = atomicAdd(sq(np+mm),dxp*amy)
            old = atomicAdd(sq(nn+mm),amx*amy)
            j = j + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! deposit charge to global array
         nn = min(mxv,nxv-noff)
         mm = min(my+1,nyv-moff)
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
! original deposit charge, has data hazard on GPU
!              q(i+noff,j+moff) = q(i+noff,j+moff) + sq(ii)
! for devices with compute capability 2.x 
               old = atomicAdd(q(i+noff,j+moff),sq(ii))
            endif
            ii = ii + blockDim%x
         enddo
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpu2jppost2l(ppart,cu,kpic,qm,dt,   &
     &nppmx,idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
! for 2-1/2d code, this subroutine calculates particle current density
! using first-order linear interpolation
! in addition, particle positions are advanced a half time-step
! threaded version using guard cells
! data deposited in tiles
! particles stored segmented array
! 41 flops/particle, 17 loads, 14 stores
! input: all, output: ppart, cu
! current density is approximated by values at the nearest grid points
! cu(i,n,m)=qci*(1.-dx)*(1.-dy)
! cu(i,n+1,m)=qci*dx*(1.-dy)
! cu(i,n,m+1)=qci*(1.-dx)*dy
! cu(i,n+1,m+1)=qci*dx*dy
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! and qci = qm*vi, where i = x,y,z
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! ppart(n,3,m) = velocity vx of particle n in tile m
! ppart(n,4,m) = velocity vy of particle n in tile m
! ppart(n,5,m) = velocity vz of particle n in tile m
! cu(i,j,k) = ith component of current density at grid point j,k
! kpic = number of particles per tile
! qm = charge on particle, in units of e
! dt = time interval between successive calculations
! nppmx = maximum number of particles in tile
! idimp = size of phase space = 5
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of current array, must be >= nx+1
! nyv = second dimension of current array, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
! ipbc = particle boundary condition = (0,1,2,3) =
! (none,2d periodic,2d reflecting,mixed reflecting/periodic)
      implicit none
      integer, value :: nppmx, idimp, nx, ny, mx, my, nxv, nyv
      integer, value :: mx1, mxy1, ipbc
      real, value :: qm, dt
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(3,nxv,nyv) :: cu
      integer, dimension(mxy1) :: kpic
! local data
      integer :: noff, moff, npp, mxv
      integer :: i, j, k, ii, nn, mm, old
      real :: edgelx, edgely, edgerx, edgery, dxp, dyp, amx, amy
      real :: x, y, dx, dy, vx, vy, vz
! The size of the shared memory array is as follows:
! real scu(3*(mx+1)*(my+1))
      real, shared, dimension(3*(mx+1)*(my+1)) :: scu
! set boundary values
      edgelx = 0.0
      edgely = 0.0
      edgerx = real(nx)
      edgery = real(ny)
      if (ipbc==2) then
         edgelx = 1.0
         edgely = 1.0
         edgerx = real(nx-1)
         edgery = real(ny-1)
      else if (ipbc==3) then
         edgelx = 1.0
         edgerx = real(nx-1)
      endif
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
! zero out local accumulator
         i = threadIdx%x
         do while (i <= 3*mxv*(my+1))
            scu(i) = 0.0
            i = i + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! loop over particles in tile
         j = threadIdx%x
         do while (j <= npp)
! find interpolation weights
            x = ppart(j,1,k)
            y = ppart(j,2,k)
            nn = x
            mm = y
            dxp = qm*(x - real(nn))
            dyp = y - real(mm)
            nn = 3*(nn - noff) + 3*mxv*(mm - moff) + 1
            amx = qm - dxp
            amy = 1.0 - dyp
! deposit current
            dx = amx*amy
            dy = dxp*amy
            vx = ppart(j,3,k)
            vy = ppart(j,4,k)
            vz = ppart(j,5,k)
! original current deposit, has data hazard on GPU
!           scu(nn) = scu(nn) + vx*dx
!           scu(nn+1) = scu(nn+1) + vy*dx
!           scu(nn+2) = scu(nn+2) + vz*dx
! for devices with compute capability 2.x
            old = atomicAdd(scu(nn),vx*dx)
            old = atomicAdd(scu(nn+1),vy*dx)
            old = atomicAdd(scu(nn+2),vz*dx)
            dx = amx*dyp
            mm = nn + 3
! original current deposit, has data hazard on GPU
!           scu(mm) = scu(mm) + vx*dy
!           scu(mm+1) = scu(mm+1) + vy*dy
!           scu(mm+2) = scu(mm+2) + vz*dy
! for devices with compute capability 2.x
            old = atomicAdd(scu(mm),vx*dy)
            old = atomicAdd(scu(mm+1),vy*dy)
            old = atomicAdd(scu(mm+2),vz*dy)
            dy = dxp*dyp
            nn = nn + 3*mxv
! original current deposit, has data hazard on GPU
!           scu(nn) = scu(nn) + vx*dx
!           scu(nn+1) = scu(nn+1) + vy*dx
!           scu(nn+2) = scu(nn+2) + vz*dx
! for devices with compute capability 2.x
            old = atomicAdd(scu(nn),vx*dx)
            old = atomicAdd(scu(nn+1),vy*dx)
            old = atomicAdd(scu(nn+2),vz*dx)
            mm = nn + 3
! original current deposit, has data hazard on GPU
!           scu(mm) = scu(mm) + vx*dy
!           scu(mm+1) = scu(mm+1) + vy*dy
!           scu(mm+2) = scu(mm+2) + vz*dy
! for devices with compute capability 2.x
            old = atomicAdd(scu(mm),vx*dy)
            old = atomicAdd(scu(mm+1),vy*dy)
            old = atomicAdd(scu(mm+2),vz*dy)
! advance position half a time-step
            dx = x + vx*dt
            dy = y + vy*dt
! reflecting boundary conditions
            if (ipbc==2) then
               if ((dx < edgelx).or.(dx >= edgerx)) then
                  dx = ppart(j,1,k)
                  ppart(j,3,k) = -ppart(j,3,k)
               endif
               if ((dy < edgely).or.(dy >= edgery)) then
                  dy = ppart(j,2,k)
                  ppart(j,4,k) = -ppart(j,4,k)
               endif
! mixed reflecting/periodic boundary conditions
            else if (ipbc==3) then
               if ((dx < edgelx).or.(dx >= edgerx)) then
                  dx = ppart(j,1,k)
                  ppart(j,3,k) = -ppart(j,3,k)
               endif
            endif
! set new position
            ppart(j,1,k) = dx
            ppart(j,2,k) = dy
            j = j + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! deposit current to global array
         nn = min(mxv,nxv-noff)
         mm = min(my+1,nyv-moff)
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
! original current deposit, has data hazard on GPU
!              cu(1,i+noff,j+moff) = cu(1,i+noff,j+moff) + scu(3*ii-2)
!              cu(2,i+noff,j+moff) = cu(2,i+noff,j+moff) + scu(3*ii-1)
!              cu(3,i+noff,j+moff) = cu(3,i+noff,j+moff) + scu(3*ii)
! for devices with compute capability 2.x
               old = atomicAdd(cu(1,i+noff,j+moff),scu(3*ii-2))
               old = atomicAdd(cu(2,i+noff,j+moff),scu(3*ii-1))
               old = atomicAdd(cu(3,i+noff,j+moff),scu(3*ii))
            endif
            ii = ii + blockDim%x
         enddo
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpu2jppostf2l(ppart,cu,kpic,ncl,    &
     &ihole,qm,dt,nppmx,idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ntmax,irc)
! for 2-1/2d code, this subroutine calculates particle current density
! using first-order linear interpolation
! in addition, particle positions are advanced a half time-step
! with periodic boundary conditions.
! also determines list of particles which are leaving this tile
! threaded version using guard cells
! data deposited in tiles
! particles stored segmented array
! 41 flops/particle, 17 loads, 14 stores
! input: all except ncl, ihole, irc,
! output: ppart, cu, ncl, ihole, irc
! current density is approximated by values at the nearest grid points
! cu(i,n,m)=qci*(1.-dx)*(1.-dy)
! cu(i,n+1,m)=qci*dx*(1.-dy)
! cu(i,n,m+1)=qci*(1.-dx)*dy
! cu(i,n+1,m+1)=qci*dx*dy
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! and qci = qm*vi, where i = x,y,z
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! ppart(n,3,m) = velocity vx of particle n in tile m
! ppart(n,4,m) = velocity vy of particle n in tile m
! ppart(n,5,m) = velocity vz of particle n in tile m
! cu(i,j,k) = ith component of current density at grid point j,k
! kpic(k) = number of particles in tile k
! ncl(i,k) = number of particles going to destination i, tile k
! ihole(1,:,k) = location of hole in array left by departing particle
! ihole(2,:,k) = destination of particle leaving hole
! ihole(1,1,k) = ih, number of holes left (error, if negative)
! qm = charge on particle, in units of e
! dt = time interval between successive calculations
! nppmx = maximum number of particles in tile
! idimp = size of phase space = 5
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of current array, must be >= nx+1
! nyv = second dimension of current array, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
! ntmax = size of hole array for particles leaving tiles
! irc = maximum overflow, returned only if error occurs, when irc > 0
! optimized version
      implicit none
      integer, value :: nppmx, idimp, nx, ny, mx, my, nxv, nyv
      integer, value :: mx1, mxy1, ntmax
      real, value :: qm, dt
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(3,nxv,nyv) :: cu
      integer, dimension(mxy1) :: kpic
      integer, dimension(8,mxy1) :: ncl
      integer, dimension(2,ntmax+1,mxy1) :: ihole
      integer, dimension(1) :: irc
! local data
      integer :: noff, moff, nhoff, mhoff, npp, mxv
      integer ::  i, j, k, ii, ih, nn, mm, old
      real :: dxp, dyp, amx, amy
      real :: x, y, dx, dy, vx, vy, vz
      real :: anx, any, edgelx, edgely, edgerx, edgery
! The sizes of the shared memory arrays are as follows:
! real scu(3*(mx+1)*(my+1))
! integer sncl(8), sih(blockDim%x), nh(1)
      real, shared, dimension(3*(mx+1)*(my+1)) :: scu
      integer, shared, dimension(8):: sncl
      integer, shared, dimension(blockDim%x):: sih
      integer, shared, dimension(1):: nh
      anx = real(nx)
      any = real(ny)
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
         nn = min(mx,nx-noff)
         mm = min(my,ny-moff)
         edgelx = noff
         edgerx = noff + nn
         edgely = moff
         edgery = moff + mm
! zero out local accumulator
         i = threadIdx%x
         do while (i <= 3*mxv*(my+1))
            scu(i) = 0.0
            i = i + blockDim%x
         enddo
! clear counters
         j = threadIdx%x
         do while (j <= 8)
            sncl(j) = 0
            j = j + blockDim%x
         enddo
         if (threadIdx%x==1) then
            nh(1) = 0
         endif
! synchronize threads
         call syncthreads()
! loop over particles in tile
         ii = (npp - 1)/int(blockDim%x) + 1
         nhoff = 0
         do i = 1, ii
            j = threadIdx%x + blockDim%x*(i - 1)
            sih(threadIdx%x) = 0
            if (j <= npp) then
! find interpolation weights
               x = ppart(j,1,k)
               y = ppart(j,2,k)
               nn = x
               mm = y
               dxp = qm*(x - real(nn))
               dyp = y - real(mm)
               nn = 3*(nn - noff) + 3*mxv*(mm - moff) + 1
               amx = qm - dxp
               amy = 1.0 - dyp
! deposit current
               dx = amx*amy
               dy = dxp*amy
               vx = ppart(j,3,k)
               vy = ppart(j,4,k)
               vz = ppart(j,5,k)
! original current deposit, has data hazard on GPU
!              scu(nn) = scu(nn) + vx*dx
!              scu(nn+1) = scu(nn+1) + vy*dx
!              scu(nn+2) = scu(nn+2) + vz*dx
! for devices with compute capability 2.x
               old = atomicAdd(scu(nn),vx*dx)
               old = atomicAdd(scu(nn+1),vy*dx)
               old = atomicAdd(scu(nn+2),vz*dx)
               dx = amx*dyp
               mm = nn + 3
! original current deposit, has data hazard on GPU
!              scu(mm) = scu(mm) + vx*dy
!              scu(mm+1) = scu(mm+1) + vy*dy
!              scu(mm+2) = scu(mm+2) + vz*dy
! for devices with compute capability 2.x
               old = atomicAdd(scu(mm),vx*dy)
               old = atomicAdd(scu(mm+1),vy*dy)
               old = atomicAdd(scu(mm+2),vz*dy)
               dy = dxp*dyp
               nn = nn + 3*mxv
! original current deposit, has data hazard on GPU
!              scu(nn) = scu(nn) + vx*dx
!              scu(nn+1) = scu(nn+1) + vy*dx
!              scu(nn+2) = scu(nn+2) + vz*dx
! for devices with compute capability 2.x
               old = atomicAdd(scu(nn),vx*dx)
               old = atomicAdd(scu(nn+1),vy*dx)
               old = atomicAdd(scu(nn+2),vz*dx)
               mm = nn + 3
! original current deposit, has data hazard on GPU
!              scu(mm) = scu(mm) + vx*dy
!              scu(mm+1) = scu(mm+1) + vy*dy
!              scu(mm+2) = scu(mm+2) + vz*dy
! for devices with compute capability 2.x
               old = atomicAdd(scu(mm),vx*dy)
               old = atomicAdd(scu(mm+1),vy*dy)
               old = atomicAdd(scu(mm+2),vz*dy)
! advance position half a time-step
               dx = x + vx*dt
               dy = y + vy*dt
! find particles going out of bounds
               mm = 0
! count how many particles are going in each direction in ncl
! save their address and destination in ihole
! use periodic boundary conditions and check for roundoff error
! mm = direction particle is going
               if (dx >= edgerx) then
                  if (dx >= anx) dx = dx - anx
                  mm = 2
               else if (dx < edgelx) then
                  if (dx < 0.0) then
                     dx = dx + anx
                     if (dx < anx) then
                        mm = 1
                     else
                        dx = 0.0
                     endif
                  else
                     mm = 1
                  endif
               endif
               if (dy >= edgery) then
                  if (dy >= any) dy = dy - any
                  mm = mm + 6
               else if (dy < edgely) then
                  if (dy < 0.0) then
                     dy = dy + any
                     if (dy < any) then
                        mm = mm + 3
                     else
                        dy = 0.0
                     endif
                  else
                     mm = mm + 3
                  endif
               endif
! using prefix scan for ih to keep holes ordered
               if (mm > 0) then
                  old = atomicAdd(sncl(mm),1)
                  sih(threadIdx%x) = 1
               endif
            endif
! synchronize threads
            call syncthreads()
            nn = npp - blockDim%x*(i - 1)
            if (nn > blockDim%x) nn = blockDim%x
! perform local prefix reduction
            call liscan2(sih(1),nn)
            if (j <= npp) then
! set new position
               ppart(j,1,k) = dx
               ppart(j,2,k) = dy
! write out location and direction of departing particles
               ih = sih(threadIdx%x)
               mhoff = 0
               if (threadIdx%x > 1) mhoff = sih(threadIdx%x-1)
! this thread has a hole present
               if (ih > mhoff) then
                  ih = ih + nhoff
                  if (ih <= ntmax) then
                     ihole(1,ih+1,k) = j
                     ihole(2,ih+1,k) = mm
                  else
                     nh(1) = 1
                  endif
               endif
            endif
! update number of holes in this iteration
            if (nn > 0) nhoff = nhoff + sih(nn)
! synchronize threads
            call syncthreads()
         enddo
! deposit current to global array
         nn = min(mxv,nxv-noff)
         mm = min(my+1,nyv-moff)
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
! original current deposit, has data hazard on GPU
!              cu(1,i+noff,j+moff) = cu(1,i+noff,j+moff) + scu(3*ii-2)
!              cu(2,i+noff,j+moff) = cu(2,i+noff,j+moff) + scu(3*ii-1)
!              cu(3,i+noff,j+moff) = cu(3,i+noff,j+moff) + scu(3*ii)
! for devices with compute capability 2.x
               old = atomicAdd(cu(1,i+noff,j+moff),scu(3*ii-2))
               old = atomicAdd(cu(2,i+noff,j+moff),scu(3*ii-1))
               old = atomicAdd(cu(3,i+noff,j+moff),scu(3*ii))
            endif
            ii = ii + blockDim%x
         enddo
! write out counters
         j = threadIdx%x
         do while (j <= 8)
            ncl(j,k) = sncl(j)
            j = j + blockDim%x
         enddo
! set error and end of file flag
         if (threadIdx%x==1) then
! ihole overflow
            ih  = nhoff
            if (nh(1) > 0) then
               irc(1) = ih
               ih = -ih
            endif
            ihole(1,1,k) = ih
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpu2rjppost2l(ppart,cu,kpic,qm,dt,ci&
     &,nppmx,idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
! for 2-1/2d code, this subroutine calculates particle current density
! using first-order linear interpolation for relativistic particles
! in addition, particle positions are advanced a half time-step
! threaded version using guard cells
! data deposited in tiles
! particles stored segmented array
! 47 flops/particle, 1 divide, 1 sqrt, 17 loads, 14 stores
! input: all, output: ppart, cu
! current density is approximated by values at the nearest grid points
! cu(i,n,m)=qci*(1.-dx)*(1.-dy)
! cu(i,n+1,m)=qci*dx*(1.-dy)
! cu(i,n,m+1)=qci*(1.-dx)*dy
! cu(i,n+1,m+1)=qci*dx*dy
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! and qci = qm*pi*gami, where i = x,y,z
! where gami = 1./sqrt(1.+sum(pi**2)*ci*ci)
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! ppart(n,3,m) = x momentum of particle n in tile m
! ppart(n,4,m) = y momentum of particle n in tile m
! ppart(n,5,m) = z momentum of particle n in tile m
! cu(i,j,k) = ith component of current density at grid point j,k
! kpic = number of particles per tile
! qm = charge on particle, in units of e
! dt = time interval between successive calculations
! ci = reciprical of velocity of light
! nppmx = maximum number of particles in tile
! idimp = size of phase space = 5
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of current array, must be >= nx+1
! nyv = second dimension of current array, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
! ipbc = particle boundary condition = (0,1,2,3) =
! (none,2d periodic,2d reflecting,mixed reflecting/periodic)
      implicit none
      integer, value :: nppmx, idimp, nx, ny, mx, my, nxv, nyv
      integer, value :: mx1, mxy1, ipbc
      real, value :: qm, dt, ci
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(3,nxv,nyv) :: cu
      integer, dimension(mxy1) :: kpic
! local data
      integer :: noff, moff, npp, mxv
      integer :: i, j, k, ii, nn, mm, old
      real :: ci2, edgelx, edgely, edgerx, edgery, dxp, dyp, amx, amy
      real :: x, y, dx, dy, vx, vy, vz, p2, gami
! The size of the shared memory array is as follows:
! real scu(3*(mx+1)*(my+1))
      real, shared, dimension(3*(mx+1)*(my+1)) :: scu
      ci2 = ci*ci
! set boundary values
      edgelx = 0.0
      edgely = 0.0
      edgerx = real(nx)
      edgery = real(ny)
      if (ipbc==2) then
         edgelx = 1.0
         edgely = 1.0
         edgerx = real(nx-1)
         edgery = real(ny-1)
      else if (ipbc==3) then
         edgelx = 1.0
         edgerx = real(nx-1)
      endif
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
! zero out local accumulator
         i = threadIdx%x
         do while (i <= 3*mxv*(my+1))
            scu(i) = 0.0
            i = i + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! loop over particles in tile
         j = threadIdx%x
         do while (j <= npp)
! find interpolation weights
            x = ppart(j,1,k)
            y = ppart(j,2,k)
            nn = x
            mm = y
            dxp = qm*(x - real(nn))
            dyp = y - real(mm)
! find inverse gamma
            vx = ppart(j,3,k)
            vy = ppart(j,4,k)
            vz = ppart(j,5,k)
            p2 = vx*vx + vy*vy + vz*vz
            gami = 1.0/sqrt(1.0 + p2*ci2)
! calculate weights
            nn = 3*(nn - noff) + 3*mxv*(mm - moff) + 1
            amx = qm - dxp
            amy = 1.0 - dyp
! deposit current
            dx = amx*amy
            dy = dxp*amy
            vx = vx*gami
            vy = vy*gami
            vz = vz*gami
! original current deposit, has data hazard on GPU
!           scu(nn) = scu(nn) + vx*dx
!           scu(nn+1) = scu(nn+1) + vy*dx
!           scu(nn+2) = scu(nn+2) + vz*dx
! for devices with compute capability 2.x
            old = atomicAdd(scu(nn),vx*dx)
            old = atomicAdd(scu(nn+1),vy*dx)
            old = atomicAdd(scu(nn+2),vz*dx)
            dx = amx*dyp
            mm = nn + 3
! original current deposit, has data hazard on GPU
!           scu(mm) = scu(mm) + vx*dy
!           scu(mm+1) = scu(mm+1) + vy*dy
!           scu(mm+2) = scu(mm+2) + vz*dy
! for devices with compute capability 2.x
            old = atomicAdd(scu(mm),vx*dy)
            old = atomicAdd(scu(mm+1),vy*dy)
            old = atomicAdd(scu(mm+2),vz*dy)
            dy = dxp*dyp
            nn = nn + 3*mxv
! original current deposit, has data hazard on GPU
!           scu(nn) = scu(nn) + vx*dx
!           scu(nn+1) = scu(nn+1) + vy*dx
!           scu(nn+2) = scu(nn+2) + vz*dx
! for devices with compute capability 2.x
            old = atomicAdd(scu(nn),vx*dx)
            old = atomicAdd(scu(nn+1),vy*dx)
            old = atomicAdd(scu(nn+2),vz*dx)
            mm = nn + 3
! original current deposit, has data hazard on GPU
!           scu(mm) = scu(mm) + vx*dy
!           scu(mm+1) = scu(mm+1) + vy*dy
!           scu(mm+2) = scu(mm+2) + vz*dy
! for devices with compute capability 2.x
            old = atomicAdd(scu(mm),vx*dy)
            old = atomicAdd(scu(mm+1),vy*dy)
            old = atomicAdd(scu(mm+2),vz*dy)
! advance position half a time-step
            dx = x + vx*dt
            dy = y + vy*dt
! reflecting boundary conditions
            if (ipbc==2) then
               if ((dx < edgelx).or.(dx >= edgerx)) then
                  dx = ppart(j,1,k)
                  ppart(j,3,k) = -ppart(j,3,k)
               endif
               if ((dy < edgely).or.(dy >= edgery)) then
                  dy = ppart(j,2,k)
                  ppart(j,4,k) = -ppart(j,4,k)
               endif
! mixed reflecting/periodic boundary conditions
            else if (ipbc==3) then
               if ((dx < edgelx).or.(dx >= edgerx)) then
                  dx = ppart(j,1,k)
                  ppart(j,3,k) = -ppart(j,3,k)
               endif
            endif
! set new position
            ppart(j,1,k) = dx
            ppart(j,2,k) = dy
            j = j + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! deposit current to global array
         nn = min(mxv,nxv-noff)
         mm = min(my+1,nyv-moff)
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
! original current deposit, has data hazard on GPU
!              cu(1,i+noff,j+moff) = cu(1,i+noff,j+moff) + scu(3*ii-2)
!              cu(2,i+noff,j+moff) = cu(2,i+noff,j+moff) + scu(3*ii-1)
!              cu(3,i+noff,j+moff) = cu(3,i+noff,j+moff) + scu(3*ii)
! for devices with compute capability 2.x
               old = atomicAdd(cu(1,i+noff,j+moff),scu(3*ii-2))
               old = atomicAdd(cu(2,i+noff,j+moff),scu(3*ii-1))
               old = atomicAdd(cu(3,i+noff,j+moff),scu(3*ii))
            endif
            ii = ii + blockDim%x
         enddo
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpu2rjppostf2l(ppart,cu,kpic,ncl,   &
     &ihole,qm,dt,ci,nppmx,idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ntmax,irc)
! for 2-1/2d code, this subroutine calculates particle current density
! using first-order linear interpolation for relativistic particles
! in addition, particle positions are advanced a half time-step
! with periodic boundary conditions.
! also determines list of particles which are leaving this tile
! threaded version using guard cells
! data deposited in tiles
! particles stored segmented array
! 47 flops/particle, 1 divide, 1 sqrt, 17 loads, 14 stores
! input: all except ncl, ihole, irc,
! output: ppart, cu, ncl, ihole, irc
! current density is approximated by values at the nearest grid points
! cu(i,n,m)=qci*(1.-dx)*(1.-dy)
! cu(i,n+1,m)=qci*dx*(1.-dy)
! cu(i,n,m+1)=qci*(1.-dx)*dy
! cu(i,n+1,m+1)=qci*dx*dy
! where n,m = leftmost grid points and dx = x-n, dy = y-m
! and qci = qm*pi*gami, where i = x,y,z
! where gami = 1./sqrt(1.+sum(pi**2)*ci*ci)
! ppart(n,1,m) = position x of particle n in tile m
! ppart(n,2,m) = position y of particle n in tile m
! ppart(n,3,m) = x momentum of particle n in tile m
! ppart(n,4,m) = y momentum of particle n in tile m
! ppart(n,5,m) = z momentum of particle n in tile m
! cu(i,j,k) = ith component of current density at grid point j,k
! kpic(k) = number of particles in tile k
! ncl(i,k) = number of particles going to destination i, tile k
! ihole(1,:,k) = location of hole in array left by departing particle
! ihole(2,:,k) = destination of particle leaving hole
! ihole(1,1,k) = ih, number of holes left (error, if negative)
! qm = charge on particle, in units of e
! dt = time interval between successive calculations
! ci = reciprical of velocity of light
! nppmx = maximum number of particles in tile
! idimp = size of phase space = 5
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! nxv = first dimension of current array, must be >= nx+1
! nyv = second dimension of current array, must be >= ny+1
! mx1 = (system length in x direction - 1)/mx + 1
! mxy1 = mx1*my1, where my1 = (system length in y direction - 1)/my + 1
! ntmax = size of hole array for particles leaving tiles
! irc = maximum overflow, returned only if error occurs, when irc > 0
! optimized version
      implicit none
      integer, value :: nppmx, idimp, nx, ny, mx, my, nxv, nyv
      integer, value :: mx1, mxy1, ntmax
      real, value :: qm, dt, ci
      real, dimension(nppmx,idimp,mxy1) :: ppart
      real, dimension(3,nxv,nyv) :: cu
      integer, dimension(mxy1) :: kpic
      integer, dimension(8,mxy1) :: ncl
      integer, dimension(2,ntmax+1,mxy1) :: ihole
      integer, dimension(1) :: irc
! local data
      integer :: noff, moff, npoff, nhoff, mhoff, npp, mxv
      integer :: i, j, k, ii, ih, nn, mm, old
      real :: ci2, dxp, dyp, amx, amy
      real :: x, y, dx, dy, vx, vy, vz, p2, gami
      real :: anx, any, edgelx, edgely, edgerx, edgery
! The sizes of the shared memory arrays are as follows:
! real scu(3*(mx+1)*(my+1))
! integer sncl(8), sih(blockDim%x), nh(1)
      real, shared, dimension(3*(mx+1)*(my+1)) :: scu
      integer, shared, dimension(8):: sncl
      integer, shared, dimension(blockDim%x):: sih
      integer, shared, dimension(1):: nh
      anx = real(nx)
      any = real(ny)
      ci2 = ci*ci
      mxv = mx + 1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
         nn = min(mx,nx-noff)
         mm = min(my,ny-moff)
         edgelx = noff
         edgerx = noff + nn
         edgely = moff
         edgery = moff + mm
! zero out local accumulator
         i = threadIdx%x
         do while (i <= 3*mxv*(my+1))
            scu(i) = 0.0
            i = i + blockDim%x
         enddo
! clear counters
         j = threadIdx%x
         do while (j <= 8)
            sncl(j) = 0
            j = j + blockDim%x
         enddo
         if (threadIdx%x==1) then
            nh(1) = 0
         endif
! synchronize threads
         call syncthreads()
! loop over particles in tile
         ii = (npp - 1)/int(blockDim%x) + 1
         nhoff = 0
         do i = 1, ii
            j = threadIdx%x + blockDim%x*(i - 1)
            sih(threadIdx%x) = 0
            if (j <= npp) then
! find interpolation weights
               x = ppart(j,1,k)
               y = ppart(j,2,k)
               nn = x
               mm = y
               dxp = qm*(x - real(nn))
               dyp = y - real(mm)
! find inverse gamma
               vx = ppart(j,3,k)
               vy = ppart(j,4,k)
               vz = ppart(j,5,k)
               p2 = vx*vx + vy*vy + vz*vz
               gami = 1.0/sqrt(1.0 + p2*ci2)
! calculate weights
               nn = 3*(nn - noff) + 3*mxv*(mm - moff) + 1
               amx = qm - dxp
               amy = 1.0 - dyp
! deposit current
               dx = amx*amy
               dy = dxp*amy
               vx = vx*gami
               vy = vy*gami
               vz = vz*gami
! original current deposit, has data hazard on GPU
!              scu(nn) = scu(nn) + vx*dx
!              scu(nn+1) = scu(nn+1) + vy*dx
!              scu(nn+2) = scu(nn+2) + vz*dx
! for devices with compute capability 2.x
               old = atomicAdd(scu(nn),vx*dx)
               old = atomicAdd(scu(nn+1),vy*dx)
               old = atomicAdd(scu(nn+2),vz*dx)
               dx = amx*dyp
               mm = nn + 3
! original current deposit, has data hazard on GPU
!              scu(mm) = scu(mm) + vx*dy
!              scu(mm+1) = scu(mm+1) + vy*dy
!              scu(mm+2) = scu(mm+2) + vz*dy
! for devices with compute capability 2.x
               old = atomicAdd(scu(mm),vx*dy)
               old = atomicAdd(scu(mm+1),vy*dy)
               old = atomicAdd(scu(mm+2),vz*dy)
               dy = dxp*dyp
               nn = nn + 3*mxv
! original current deposit, has data hazard on GPU
!              scu(nn) = scu(nn) + vx*dx
!              scu(nn+1) = scu(nn+1) + vy*dx
!              scu(nn+2) = scu(nn+2) + vz*dx
! for devices with compute capability 2.x
               old = atomicAdd(scu(nn),vx*dx)
               old = atomicAdd(scu(nn+1),vy*dx)
               old = atomicAdd(scu(nn+2),vz*dx)
               mm = nn + 3
! original current deposit, has data hazard on GPU
!              scu(mm) = scu(mm) + vx*dy
!              scu(mm+1) = scu(mm+1) + vy*dy
!              scu(mm+2) = scu(mm+2) + vz*dy
! for devices with compute capability 2.x
               old = atomicAdd(scu(mm),vx*dy)
               old = atomicAdd(scu(mm+1),vy*dy)
               old = atomicAdd(scu(mm+2),vz*dy)
! advance position half a time-step
               dx = x + vx*dt
               dy = y + vy*dt
! find particles going out of bounds
               mm = 0
! count how many particles are going in each direction in ncl
! save their address and destination in ihole
! use periodic boundary conditions and check for roundoff error
! mm = direction particle is going
               if (dx >= edgerx) then
                  if (dx >= anx) dx = dx - anx
                  mm = 2
               else if (dx < edgelx) then
                  if (dx < 0.0) then
                     dx = dx + anx
                     if (dx < anx) then
                        mm = 1
                     else
                        dx = 0.0
                     endif
                  else
                     mm = 1
                  endif
               endif
               if (dy >= edgery) then
                  if (dy >= any) dy = dy - any
                  mm = mm + 6
               else if (dy < edgely) then
                  if (dy < 0.0) then
                     dy = dy + any
                     if (dy < any) then
                        mm = mm + 3
                     else
                        dy = 0.0
                     endif
                  else
                     mm = mm + 3
                  endif
               endif
! using prefix scan for ih to keep holes ordered
               if (mm > 0) then
                  old = atomicAdd(sncl(mm),1)
                  sih(threadIdx%x) = 1
               endif
            endif
! synchronize threads
            call syncthreads()
            nn = npp - blockDim%x*(i - 1)
            if (nn > blockDim%x) nn = blockDim%x
! perform local prefix reduction
            call liscan2(sih(1),nn)
            if (j <= npp) then
! set new position
               ppart(j,1,k) = dx
               ppart(j,2,k) = dy
! write out location and direction of departing particles
               ih = sih(threadIdx%x)
               mhoff = 0
               if (threadIdx%x > 1) mhoff = sih(threadIdx%x-1)
! this thread has a hole present
               if (ih > mhoff) then
                  ih = ih + nhoff
                  if (ih <= ntmax) then
                     ihole(1,ih+1,k) = j
                     ihole(2,ih+1,k) = mm
                  else
                     nh(1) = 1
                  endif
               endif
            endif
! update number of holes in this iteration
            if (nn > 0) nhoff = nhoff + sih(nn)
! synchronize threads
            call syncthreads()
         enddo
! deposit current to global array
         nn = min(mxv,nxv-noff)
         mm = min(my+1,nyv-moff)
         ii = threadIdx%x
         do while (ii <= mxv*(my+1))
            j = (ii - 1)/mxv
            i = ii - mxv*j
            j = j + 1
            if ((i <= nn) .and. (j <= mm)) then
! original current deposit, has data hazard on GPU
!              cu(1,i+noff,j+moff) = cu(1,i+noff,j+moff) + scu(3*ii-2)
!              cu(2,i+noff,j+moff) = cu(2,i+noff,j+moff) + scu(3*ii-1)
!              cu(3,i+noff,j+moff) = cu(3,i+noff,j+moff) + scu(3*ii)
! for devices with compute capability 2.x
               old = atomicAdd(cu(1,i+noff,j+moff),scu(3*ii-2))
               old = atomicAdd(cu(2,i+noff,j+moff),scu(3*ii-1))
               old = atomicAdd(cu(3,i+noff,j+moff),scu(3*ii))
            endif
            ii = ii + blockDim%x
         enddo
! write out counters
         j = threadIdx%x
         do while (j <= 8)
            ncl(j,k) = sncl(j)
            j = j + blockDim%x
         enddo
! set error and end of file flag
         if (threadIdx%x==1) then
! ihole overflow
            ih  = nhoff
            if (nh(1) > 0) then
               irc(1) = ih
               ih = -ih
            endif
            ihole(1,1,k) = ih
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpucaguard2l(qc,q,nx,ny,nxe,nye,nxvh&
     &,nyv)
! copy and accumulate extended periodic scalar field q 
! into complex output field qc
! linear interpolation
! nx/ny = system length in x/y direction
! nxe = first dimension of input field array q, must be >= nx+1
! nye = second dimension of input field array q, must be >= ny+1
! nxvh = first dimension of output field array qc, must be >= nx/2+1
! nyv = second dimension of output field array qc, must be >= ny
      implicit none
      integer, value :: nx, ny, nxe, nye, nxvh, nyv
      complex, dimension(nxvh,nyv) :: qc
      real, dimension(nxe,nye) :: q
! local data
      integer :: j, k, nxh
      real :: at1, at2
      nxh = nx/2
      k = blockIdx%x
! copy interior points
      if (k <= ny) then
         j = threadIdx%x
         at2 = 0.0
         do while (j <= nxh)
            if (k==1) then
               at1 = q(2*j-1,ny+1)
               at2 = q(2*j,ny+1)
               if (j==1) at1 = at1 + q(nx+1,1) + q(nx+1,ny+1)
            endif
            if (k > 1) then
               at1 = 0.0
               if (j==1) at1 = q(nx+1,k)
            endif
            qc(j,k) = cmplx(q(2*j-1,k)+at1,q(2*j,k)+at2)
            j = j + blockDim%x
         enddo
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpucacguard2l(cuc,cu,nx,ny,nxe,nye, &
     &nxvh,nyv)
! copy and accumulate extended periodic vector field cu 
! into complex output field cuc
! linear interpolation
! nx/ny = system length in x/y direction
! nxe = second dimension of input field array cu, must be >= nx+1
! nye = third dimension of input field array cu, must be >= ny+1
! nxvh = first dimension of output field array cuc, must be >= nx/2+1
! nyv = third dimension of output field array cuc, must be >= ny
      implicit none
      integer, value :: nx, ny, nxe, nye, nxvh, nyv
      complex, dimension(nxvh,3,nyv) :: cuc
      real, dimension(3,nxe,nye) :: cu
! local data
      integer :: j, k, nxh
      real :: at1, at2, at3, at4, at5, at6
      nxh = nx/2
      k = blockIdx%x
! copy interior points
      if (k <= ny) then
         at2 = 0.0
         at4 = 0.0
         at6 = 0.0
         j = threadIdx%x
         do while (j <= nxh)
            if (k==1) then
               at1 = cu(1,2*j-1,ny+1)
               at2 = cu(1,2*j,ny+1)
               at3 = cu(2,2*j-1,ny+1)
               at4 = cu(2,2*j,ny+1)
               at5 = cu(3,2*j-1,ny+1)
               at6 = cu(3,2*j,ny+1)
               if (j==1) then
                  at1 = at1 + cu(1,nx+1,1) + cu(1,nx+1,ny+1)
                  at3 = at3 + cu(2,nx+1,1) + cu(2,nx+1,ny+1)
                  at5 = at5 + cu(3,nx+1,1) + cu(3,nx+1,ny+1)
               endif
            endif
            if (k > 1) then
               at1 = 0.0
               at3 = 0.0
               at5 = 0.0
               if (j==1) then
                  at1 = cu(1,nx+1,k)
                  at3 = cu(2,nx+1,k)
                  at5 = cu(3,nx+1,k)
               endif
            endif
            cuc(j,1,k) = cmplx(cu(1,2*j-1,k)+at1,cu(1,2*j,k)+at2)
            cuc(j,2,k) = cmplx(cu(2,2*j-1,k)+at3,cu(2,2*j,k)+at4)
            cuc(j,3,k) = cmplx(cu(3,2*j-1,k)+at5,cu(3,2*j,k)+at6)
            j = j + blockDim%x
         enddo
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpucbguard2l(bxyc,bxy,nx,ny,nxe,nye,&
     &nxvh,nyv)
! copy and replicate complex input 2d vector field bxyc
! into extended periodic field bxy
! linear interpolation
! nx/ny = system length in x/y direction
! nxe = second dimension of input field array bxy, must be >= nx+1
! nye = third dimension of input field array bxy, must be >= ny+1
! nxvh = first dimension of input field array bxyc, must be >= nx/2+1
! nyv = third dimension of input field array bxyc, must be >= ny 
      implicit none
      integer, value :: nx, ny, nxe, nye, nxvh, nyv
      complex, dimension(nxvh,3,nyv) :: bxyc
      real, dimension(3,nxe,nye) :: bxy
! local data
      integer :: j, k, nxh
      complex :: a, b, c
      nxh = nx/2
      k = blockIdx%x
! copy interior points
      if (k <= ny) then
         j = threadIdx%x
         do while (j <= nxh)
            a = bxyc(j,1,k)
            b = bxyc(j,2,k)
            c = bxyc(j,3,k)
            bxy(1,2*j-1,k) = real(a)
            bxy(2,2*j-1,k) = real(b)
            bxy(3,2*j-1,k) = real(c)
            bxy(1,2*j,k) = aimag(a)
            bxy(2,2*j,k) = aimag(b)
            bxy(3,2*j,k) = aimag(c)
            j = j + blockDim%x
         enddo
      endif
! accumulate edges of extended field
      if (blockIdx%x==1) then
         k = threadIdx%x
         do while (k <= ny)
            a = bxyc(1,1,k)
            b = bxyc(1,2,k)
            c = bxyc(1,3,k)
            bxy(1,nx+1,k) = real(a)
            bxy(2,nx+1,k) = real(b)
            bxy(3,nx+1,k) = real(c)
            k = k + blockDim%x
         enddo
         j = threadIdx%x
         do while (j <= nxh)
            a = bxyc(j,1,1)
            b = bxyc(j,2,1)
            c = bxyc(j,3,1)
            bxy(1,2*j-1,ny+1) = real(a)
            bxy(2,2*j-1,ny+1) = real(b)
            bxy(3,2*j-1,ny+1) = real(c)
            bxy(1,2*j,ny+1) = aimag(a)
            bxy(2,2*j,ny+1) = aimag(b)
            bxy(3,2*j,ny+1) = aimag(c)
            j = j + blockDim%x
         enddo
         if (threadIdx%x==1) then
            a = bxyc(1,1,1)
            b = bxyc(1,2,1)
            c = bxyc(1,3,1)
            bxy(1,nx+1,ny+1) = real(a)
            bxy(2,nx+1,ny+1) = real(b)
            bxy(3,nx+1,ny+1) = real(c)
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpuppfnd2l(ppart,kpic,ncl,ihole,    &
     &idimp,nppmx,nx,ny,mx,my,mx1,my1,ntmax,irc)
! this subroutine performs first step of a particle sort by x,y grid
! in tiles of mx, my, where one finds the particles leaving tile and
! stores their number, location, and destination in ncl and ihole.
! linear interpolation, with periodic boundary conditions
! tiles are assumed to be arranged in 2D linear memory
! input: all except ncl, ihole, irc
! output: ppart, ncl, ihole, irc
! ppart(n,1,k) = position x of particle n in tile k
! ppart(n,2,k) = position y of particle n in tile k 
! kpic(k) = number of particles in tile k
! ncl(i,k) = number of particles going to destination i, tile k
! ihole(1,:,k) = location of hole in array left by departing particle
! ihole(2,:,k) = destination of particle leaving hole
! ihole(1,1,k) = ih, number of holes left (error, if negative)
! idimp = size of phase space = 4
! nppmx = maximum number of particles in tile
! nx/ny = system length in x/y direction
! mx/my = number of grids in sorting cell in x/y
! mx1 = (system length in x direction - 1)/mx + 1
! my1 = (system length in y direction - 1)/my + 1
! ntmax = size of hole array for particles leaving tiles
! irc = maximum overflow, returned only if error occurs, when irc > 0
      implicit none
      integer, value :: idimp, nppmx, nx, ny, mx, my, mx1, my1, ntmax
      real, dimension(nppmx,idimp,mx1*my1) :: ppart
      integer, dimension(mx1*my1) :: kpic
      integer, dimension(8,mx1*my1) :: ncl
      integer, dimension(2,ntmax+1,mx1*my1) :: ihole
      integer, dimension(1) :: irc
! local data
      integer :: mxy1, noff, moff, npp, j, k, ih, ist, nn, mm, nths
      integer :: old
      real :: anx, any, edgelx, edgely, edgerx, edgery, dx, dy
! The sizes of the shared memory arrays are as follows:
! integer sncl(8), sih(blockDim%x), nh(1)
      integer, shared, dimension(8) :: sncl
      integer, shared, dimension(blockDim%x) :: sih
      integer, shared, dimension(1) :: nh
      mxy1 = mx1*my1
      anx = real(nx)
      any = real(ny)
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! find and count particles leaving tiles and determine destination
! update ppart, ihole, ncl
! loop over tiles
      if (k <= mxy1) then
         noff = (k - 1)/mx1
         moff = my*noff
         noff = mx*(k - mx1*noff - 1)
         npp = kpic(k)
         nn = min(mx,nx-noff)
         mm = min(my,ny-moff)
         edgelx = noff
         edgerx = noff + nn
         edgely = moff
         edgery = moff + mm
! clear counters
         j = threadIdx%x
         do while (j <= 8)
            sncl(j) = 0
            j = j + blockDim%x
         enddo
         if (threadIdx%x==1) then
            nh(1) = 0
         endif
! synchronize threads
         call syncthreads()
! loop over particles in tile
         mm = (npp - 1)/int(blockDim%x) + 1
         noff = 0
         do nn = 1, mm
            j = threadIdx%x + blockDim%x*(nn - 1)
            sih(threadIdx%x) = 0
            if (j <= npp) then
               dx = ppart(j,1,k)
               dy = ppart(j,2,k)
! find particles going out of bounds
               ist = 0
! count how many particles are going in each direction in ncl
! save their address and destination in ihole
! use periodic boundary conditions and check for roundoff error
! ist = direction particle is going
               if (dx >= edgerx) then
                  if (dx >= anx) ppart(j,1,k) = dx - anx
                  ist = 2
               else if (dx < edgelx) then
                  if (dx < 0.0) then
                     dx = dx + anx
                     if (dx < anx) then
                        ist = 1
                     else
                        dx = 0.0
                     endif
                     ppart(j,1,k) = dx
                  else
                     ist = 1
                  endif
               endif
               if (dy >= edgery) then
                  if (dy >= any) ppart(j,2,k) = dy - any
                  ist = ist + 6
               else if (dy < edgely) then
                  if (dy < 0.0) then
                     dy = dy + any
                     if (dy < any) then
                        ist = ist + 3
                     else
                        dy = 0.0
                     endif
                     ppart(j,2,k) = dy
                  else
                     ist = ist + 3
                  endif
               endif
! using prefix scan for ih to keep holes ordered
               if (ist > 0) then
                  old = atomicAdd(sncl(ist),1)
                  sih(threadIdx%x) = 1
               endif
            endif
! synchronize threads
            call syncthreads()
            nths = npp - blockDim%x*(nn - 1)
            if (nths > blockDim%x) nths = blockDim%x
! perform local prefix reduction
            call liscan2(sih,nths)
            if (j <= npp) then
               ih = sih(threadIdx%x)
               moff = 0
               if (threadIdx%x > 1) moff = sih(threadIdx%x-1)
! this thread has a hole present
               if (ih > moff) then
                  ih = ih + noff
                  if (ih <= ntmax) then
                     ihole(1,ih+1,k) = j
                     ihole(2,ih+1,k) = ist
                  else
                     nh(1) = 1
                  endif
               endif
            endif
! update number of holes in this iteration
            if (nths > 0) noff = noff + sih(nths)
! synchronize threads
            call syncthreads()
         enddo
! write out counters
         j = threadIdx%x
         do while (j <= 8)
            ncl(j,k) = sncl(j)
            j = j + blockDim%x
         enddo
! set error and end of file flag
         if (threadIdx%x==1) then
! ihole overflow
            ih  = noff
            if (nh(1) > 0) then
               irc(1) = ih
               ih = -ih
            endif
            ihole(1,1,k) = ih
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpuppmov2l(ppart,ppbuff,ncl,ihole,  &
     &idimp,nppmx,mx1,my1,npbmx,ntmax,irc)
! this subroutine performs second step of a particle sort by x,y grid
! in tiles of mx, my, where prefix scan of ncl is performed and
! departing particles are buffered in ppbuff in direction order.
! linear interpolation, with periodic boundary conditions
! tiles are assumed to be arranged in 2D linear memory
! input: all except ppbuff, irc
! output: ppbuff, ncl, irc
! ppart(n,i,k) = i co-ordinate of particle n in tile k 
! ppbuff(n,i,k) = i co-ordinate of particle n in tile k
! ncl(i,k) = number of particles going to destination i, tile k
! ihole(1,:,k) = location of hole in array left by departing particle
! ihole(2,:,k) = direction destination of particle leaving hole
! all for tile k
! ihole(1,1,k) = ih, number of holes left (error, if negative)
! idimp = size of phase space = 4
! nppmx = maximum number of particles in tile
! mx1 = (system length in x direction - 1)/mx + 1
! my1 = (system length in y direction - 1)/my + 1
! npbmx = size of buffer array ppbuff
! ntmax = size of hole array for particles leaving tiles
! irc = maximum overflow, returned only if error occurs, when irc > 0
      implicit none
      integer, value :: idimp, nppmx, mx1, my1, npbmx, ntmax
      real, dimension(nppmx,idimp,mx1*my1) :: ppart
      real, dimension(npbmx,idimp,mx1*my1) :: ppbuff
      integer, dimension(8,mx1*my1) :: ncl
      integer, dimension(2,ntmax+1,mx1*my1) :: ihole
      integer, dimension(1) :: irc
! local data
      integer ::  mxy1, i, j, k, ii, nh, ist, j1, ierr
! The sizes of the shared memory arrays are as follows:
! integer sncl(8), ip(1)
! blockDim%x should be >= 8
      integer, shared, dimension(8) :: sncl
      integer, shared, dimension(1) :: ip
      mxy1 = mx1*my1
      ierr = 0
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
      j = threadIdx%x
! buffer particles that are leaving tile: update ppbuff, ncl
! loop over tiles
      if (k <= mxy1) then
! find address offset for ordered ppbuff array
         if (j <= 8) then
            ist = ncl(j,k)
            sncl(j) = ist
         endif
         if (threadIdx%x==1) ip(1) = 0
! synchronize threads
         call syncthreads()
! perform local prefix reduction
         call liscan2(sncl,8)
         if (j <= 8) sncl(j) = sncl(j) - ist
! synchronize threads
         call syncthreads()
         nh = ihole(1,1,k)
! loop over particles leaving tile
         do while (j <= nh)
! buffer particles that are leaving tile, in direction order
            j1 = ihole(1,j+1,k)
            ist = ihole(2,j+1,k)
            ii = atomicAdd(sncl(ist),1) + 1
            if (ii <= npbmx) then
               do i = 1, idimp
               ppbuff(ii,i,k) = ppart(j1,i,k)
               enddo
            else
               ip(1) = 1
            endif
            j = j + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
! write out counters
         j = threadIdx%x
         if (j <= 8) then
            ncl(j,k) = sncl(j)
         endif
! set error
         if (threadIdx%x==1) then
            if (ip(1) > 0) ierr = max(ierr,sncl(8))
         endif
      endif
! ppbuff overflow
      if (ierr > 0) irc(1) = ierr
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpuppord2l(ppart,ppbuff,kpic,ncl,   &
     &ihole,idimp,nppmx,mx1,my1,npbmx,ntmax,irc)
! this subroutine performs third step of a particle sort by x,y grid
! in tiles of mx, my, where incoming particles from other tiles are
! copied into ppart.
! linear interpolation, with periodic boundary conditions
! tiles are assumed to be arranged in 2D linear memory
! input: all except irc
! output: ppart, kpic, irc
! ppart(n,i,k) = i co-ordinate of particle n in tile k 
! ppbuff(n,i,k) = i co-ordinate of particle n in tile k
! kpic(k) = number of particles in tile k
! ncl(i,k) = number of particles going to destination i, tile k
! ihole(1,:,k) = location of hole in array left by departing particle
! ihole(2,:,k) = direction destination of particle leaving hole
! all for tile k
! ihole(1,1,k) = ih, number of holes left (error, if negative)
! idimp = size of phase space = 4
! nppmx = maximum number of particles in tile
! mx1 = (system length in x direction - 1)/mx + 1
! my1 = (system length in y direction - 1)/my + 1
! npbmx = size of buffer array ppbuff
! ntmax = size of hole array for particles leaving tiles
! irc = maximum overflow, returned only if error occurs, when irc > 0
      implicit none
      integer, value :: idimp, nppmx, mx1, my1, npbmx, ntmax
      real, dimension(nppmx,idimp,mx1*my1) :: ppart
      real, dimension(npbmx,idimp,mx1*my1) :: ppbuff
      integer, dimension(mx1*my1) :: kpic
      integer, dimension(8,mx1*my1) :: ncl
      integer, dimension(2,ntmax+1,mx1*my1) :: ihole
      integer, dimension(1) :: irc
! local data 
      integer :: mxy1, npp, ncoff, i, j, k, ii, jj, kx, ky, ni, nh
      integer :: nn, mm, ll, ip, j1, j2, j3, kxl, kxr, kk, kl, kr
      integer :: nths
! The sizes of the shared memory arrays are as follows:
! integer ks(8), sip(8), sj(blockDim%x), sj1(1), ist(1)
      integer, shared, dimension(8) :: ks, sip
      integer, shared, dimension(blockDim%x) :: sj
      integer, shared, dimension(1) :: sj1, ist
      mxy1 = mx1*my1
! k = tile number
      k = blockIdx%x + gridDim%x*(blockIdx%y - 1)
! copy incoming particles from buffer into ppart: update ppart, kpic
! loop over tiles
      if (k <= mxy1) then
         npp = kpic(k)
         ky = (k - 1)/mx1 + 1
! loop over tiles in y, assume periodic boundary conditions
         kk = (ky - 1)*mx1
! find tile above
         kl = ky - 1
         if (kl < 1) kl = kl + my1
         kl = (kl - 1)*mx1
! find tile below
         kr = ky + 1
         if (kr > my1) kr = kr - my1
         kr = (kr - 1)*mx1
! loop over tiles in x, assume periodic boundary conditions
         kx = k - (ky - 1)*mx1
         kxl = kx - 1 
         if (kxl < 1) kxl = kxl + mx1
         kxr = kx + 1
         if (kxr > mx1) kxr = kxr - mx1
! find tile number for different directions
         if (threadIdx%x==1) then
            ks(1) = kxr + kk
            ks(2) = kxl + kk
            ks(3) = kx + kr
            ks(4) = kxr + kr
            ks(5) = kxl + kr
            ks(6) = kx + kl
            ks(7) = kxr + kl
            ks(8) = kxl + kl
            sj1(1) = 0
            ist(1) = 0
         endif
! synchronize threads
         call syncthreads()
! find number of incoming particles
         kk = 0
         ncoff = 0
         ip = 0
         ii = threadIdx%x
         if (ii <= 8) then
            kk = ks(ii)
            if (ii > 1) ncoff = ncl(ii-1,kk)
            ip = ncl(ii,kk) - ncoff
            kk = ncoff + idimp*npbmx*(kk - 1)
            sip(ii) = ip
         endif
! synchronize threads
         call syncthreads()
! perform local prefix reduction
         call liscan2(sip,8)
         ni = sip(8)
! loop over directions
         nh = ihole(1,1,k)
         j1 = 0
         mm = (ni - 1)/int(blockDim%x) + 1
         do nn = 1, mm
            j = threadIdx%x + blockDim%x*(nn - 1)
            sj(threadIdx%x) = 0
            if (threadIdx%x==1) sj(1) = sj1(1)
! synchronize threads
            call syncthreads()
! calculate offset for reading from particle buffer
            if (ii <= 8) then
! mark next location where direction ii changes
               jj = sip(ii) - blockDim%x*(nn - 1)
               if ((jj >= 0) .and. (jj < blockDim%x)) then
                  if (ip > 0) sj(jj+1) = sj(jj+1) - (kk + ip)
               endif
            endif
! synchronize threads
            call syncthreads()
! calculate offset for reading from particle buffer
            if (ii <= 8) then
! mark location where direction ii starts
               jj = jj - ip
               if ((jj >= 0) .and. (jj < blockDim%x)) then
                  if (ip > 0) sj(jj+1) = sj(jj+1) + kk
               endif
            endif
            nths = ni - blockDim%x*(nn - 1)
            if (nths > blockDim%x) nths = blockDim%x
! synchronize threads
            call syncthreads()
! perform local prefix reduction
            call liscan2(sj,nths)
! save last value for next time
            if (threadIdx%x==1) then
               jj = 0
               if (nths > 0) jj = sj(nths)
               sj1(1)= jj
            endif
            if (j <= ni) then
! insert incoming particles into holes
               if (j <= nh) then
                  j1 = ihole(1,j+1,k)
! place overflow at end of array
               else
                  j1 = npp + (j - nh)
               endif
               if (j1 <= nppmx) then
                  jj = sj(threadIdx%x)
                  j2 = idimp*npbmx
                  j3 = (j + jj - 1)/j2
                  j2 = j + jj - j2*j3
                  do i = 1, idimp
                  ppart(j1,i,k) = ppbuff(j2,i,j3+1)
                  enddo
               else
                  ist(1) = 1
               endif
            endif
! synchronize threads
            call syncthreads()
         enddo
! update particle number if all holes have been filled
         jj = ni - nh
         if (jj > 0) npp = npp + jj
! fill up remaining holes in particle array with particles from end
         ip = nh - ni
         if (ip > 0) then
            mm = (ip - 1)/int(blockDim%x) + 1
            kk = 0
            ll = 0
! loop over holes
            do nn = 1, mm
               j = threadIdx%x + blockDim%x*(nn - 1)
! j1 = locations of particles to fill holes, in decreasing order
               j1 = 0
               if (j <= ip) j1 = npp - j + 1
! j2 = locations of holes at the end, in decreasing order
               j2 = 0
               jj = nh - ll - threadIdx%x + 2
               if (jj > 1) j2 = ihole(1,jj,k)
! holes with locations greater than npp-ip do not need to be filled
! identify such holes
               sj(threadIdx%x) = 1
! synchronize threads
               call syncthreads()
! omit particles at end that are holes
               ii = npp - (j2 + blockDim%x*(nn - 1)) + 1
               if ((ii > 0) .and. (ii <= blockDim%x)) sj(ii) = 0
               nths = ip - blockDim%x*(nn - 1)
               if (nths > blockDim%x) nths = blockDim%x
! synchronize threads
               call syncthreads()
! perform local prefix reduction
               call liscan2(sj,nths)
! ii = number particles at end to be moved
               ii = 0
               if (nths > 0) ii = sj(nths)
! identify which particles at end to be moved
               if (ii < nths) then
                  ncoff = 0
                  if (j <= ip) then
                     if (threadIdx%x > 1) ncoff = sj(threadIdx%x-1)
                     jj = sj(threadIdx%x)
                  endif
! synchronize threads
                  call syncthreads()
                  if (j <= ip) then
                     if (jj > ncoff) then
                        sj(jj) = j1
                     endif
                  endif
! synchronize threads
                  call syncthreads()
               endif
! j2 = locations of holes to be filled in increasing order
               j2 = 0
               if (j <= ip) then
                  j1 = npp - j + 1
                  jj = threadIdx%x + ni + kk
                  if (jj <= nh) j2 = ihole(1,jj+1,k)
               endif
! move particles from end into remaining holes
               if (j <= (ii+blockDim%x*(nn-1))) then
                  if (ii < nths) j1 = sj(threadIdx%x)
                  do i = 1, idimp
                  ppart(j2,i,k) = ppart(j1,i,k)
                  enddo
               endif
! accumulate number of holes filled
               kk = kk + ii
! accumulate number of holes skipped over
               ii = nths - ii
               ll = ll + ii
            enddo
! update number of particles
            npp = npp - ip
         endif
! set error and update particle
         if (threadIdx%x==1) then
! ppart overflow
            if (ist(1) > 0) irc(1) = npp
            kpic(k) = npp
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpupois23t(qt,fxyt,ffct,we,nx,ny,   &
     &nxvh,nyv,nxhd,nyhd)
! this subroutine solves 2d poisson's equation in fourier space for
! force/charge (or convolution of electric field over particle shape)
! with periodic boundary conditions, without packed data.
! Zeros out z component.
! vector length is second dimension
! input: qt,ffct,nx,ny,nxvh,nyv,nxhd,nyhd, output: fxyt,we
! approximate flop count is: 26*nxc*nyc + 12*(nxc + nyc)
! where nxc = nx/2 - 1, nyc = ny/2 - 1
! equation used is:
! fx(ky,kx) = -sqrt(-1)*kx*g(ky,kx)*s(ky,kx)*q(ky,kx),
! fy(ky,kx) = -sqrt(-1)*ky*g(ky,kx)*s(ky,kx)*q(ky,kx),
! where kx = 2pi*j/nx, ky = 2pi*k/ny, and j,k = fourier mode numbers,
! g(ky,kx) = (affp/(kx**2+ky**2))*s(ky,kx),
! s(ky,kx) = exp(-((kx*ax)**2+(ky*ay)**2)/2), except for
! fx(kx=pi) = fy(kx=pi) = fx(ky=pi) = fy(ky=pi) = 0, and
! fx(kx=0,ky=0) = fy(kx=0,ky=0) = 0.
! qt(k,j) = complex charge density for fourier mode (k,j)
! fxyt(k,1,j) = x component of complex force/charge,
! fxyt(k,2,j) = y component of complex force/charge,
! fxyt(k,3,j) = z component of complex force/charge,
! all for fourier mode (k,j)
! aimag(ffct(k,j)) = finite-size particle shape factor s
! real(ffct(k,j)) = potential green's function g
! for fourier mode (k,j)
! electric field energy is also calculated, using
! we = nx*ny*sum((affp/(kx**2+ky**2))*|q(ky,kx)*s(ky,kx)|**2)
! nx/ny = system length in x/y direction
! nxvh = second dimension of field arrays, must be >= nxh+1
! nyv = first dimension of field arrays, must be >= ny
! nxhd = second dimension of form factor array, must be >= nxh
! nyhd = first dimension of form factor array, must be >= nyh
      implicit none
      integer, value :: nx, ny, nxvh, nyv, nxhd, nyhd
      complex, dimension(nyv,nxvh) :: qt
      complex, dimension(nyv,3,nxvh) :: fxyt
      complex, dimension(nyhd,nxhd) :: ffct
      real, dimension(nxvh) :: we
! local data
      integer :: nxh, nyh, ny2, nxh1, j, k, k1
      real :: dnx, dny, dkx, at1, at2, at3
      complex :: zero, zt1, zt2
! The size of the shared memory array is as follows:
! real ss(blockDim%x)
      real, shared, dimension(blockDim%x) :: ss
      double precision :: wp
      nxh = nx/2
      nyh = max(1,ny/2)
      ny2 = ny + 2
      nxh1 = nxh + 1
      dnx = 6.28318530717959/real(nx)
      dny = 6.28318530717959/real(ny)
      zero = cmplx(0.0,0.0)
! calculate force/charge and sum field energy
      wp = 0.0d0
! mode numbers 0 < kx < nx/2 and 0 < ky < ny/2
!     do j = 2, nxh
      j = blockIdx%x
      if ((j > 1) .and. (j <= nxh)) then
         dkx = dnx*real(j - 1)
!        do k = 2, nyh
         k = threadIdx%x
         do while (k <= nyh)
            if (k > 1) then
               k1 = ny2 - k
               at1 = real(ffct(k,j))*aimag(ffct(k,j))
               at2 = dkx*at1
               at3 = dny*real(k - 1)*at1
               zt1 = cmplx(aimag(qt(k,j)),-real(qt(k,j)))
               zt2 = cmplx(aimag(qt(k1,j)),-real(qt(k1,j)))
               fxyt(k,1,j) = at2*zt1
               fxyt(k1,1,j) = at2*zt2
               fxyt(k,2,j) = at3*zt1
               fxyt(k1,2,j) = -at3*zt2
               fxyt(k,3,j) = zero
               fxyt(k1,3,j) = zero
               wp = wp + dble(at1*(qt(k,j)*conjg(qt(k,j))               &
     &            + qt(k1,j)*conjg(qt(k1,j))))
            endif
            k = k + blockDim%x
         enddo
      endif
! mode numbers ky = 0, ny/2
      if (blockIdx%x==1) then
         k1 = nyh + 1
!        do j = 2, nxh
         j = threadIdx%x
         do while (j <= nxh)
            if (j > 1) then
               at1 = real(ffct(1,j))*aimag(ffct(1,j))
               at3 = dnx*real(j - 1)*at1
               zt1 = cmplx(aimag(qt(1,j)),-real(qt(1,j)))
               fxyt(1,1,j) = at3*zt1
               fxyt(k1,1,j) = zero
               fxyt(1,2,j) = zero
               fxyt(k1,2,j) = zero
               fxyt(1,3,j) = zero
               fxyt(k1,3,j) = zero
               wp = wp + dble(at1*(qt(1,j)*conjg(qt(1,j))))
            endif
            j = j + blockDim%x
         enddo
! mode numbers kx = 0, nx/2
!        do k = 2, nyh
         k = threadIdx%x
         do while (k <= nyh)
            if (k > 1) then
               k1 = ny2 - k
               at1 = real(ffct(k,1))*aimag(ffct(k,1))
               at2 = dny*real(k - 1)*at1
               zt1 = cmplx(aimag(qt(k,1)),-real(qt(k,1)))
               fxyt(k,1,1) = zero
               fxyt(k1,1,1) = zero
               fxyt(k,2,1) = at2*zt1
               fxyt(k1,2,1) = at2*conjg(zt1)
               fxyt(k,3,1) = zero
               fxyt(k1,3,1) = zero
               fxyt(k,1,nxh1) = zero
               fxyt(k,2,nxh1) = zero
               fxyt(k,3,nxh1) = zero
               fxyt(k1,1,nxh1) = zero
               fxyt(k1,2,nxh1) = zero
               fxyt(k1,3,nxh1) = zero
               wp = wp + dble(at1*(qt(k,1)*conjg(qt(k,1))))
            endif
            k = k + blockDim%x
         enddo
         if (threadIdx%x==1) then
            k1 = nyh + 1
            fxyt(1,1,1) = zero
            fxyt(1,2,1) = zero
            fxyt(1,3,1) = zero
            fxyt(k1,1,1) = zero
            fxyt(k1,2,1) = zero
            fxyt(k1,3,1) = zero
            fxyt(1,1,nxh1) = zero
            fxyt(1,2,nxh1) = zero
            fxyt(1,3,nxh1) = zero
            fxyt(k1,1,nxh1) = zero
            fxyt(k1,2,nxh1) = zero
            fxyt(k1,3,nxh1) = zero
         endif
      endif
      j = blockIdx%x
      if (j <= nxh1) then
! sum potential energies for each x co-ordinate
         ss(threadIdx%x) = real(wp)
! synchronize threads/
         call syncthreads()
         call lsum2(ss,blockDim%x)
! normalize potential energy for each x co-ordinate
         if (threadIdx%x==1) we(j) = ss(1)*real(nx*ny)
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpucuperp2t(cut,nx,ny,nxvh,nyv)
! this subroutine calculates the transverse current in fourier space
! without packed data.
! input: all, output: cut
! approximate flop count is: 36*nxc*nyc
! and nxc*nyc divides
! where nxc = nx/2 - 1, nyc = ny/2 - 1
! the transverse current is calculated using the equation:
! cux(ky,kx) = cux(ky,kx)
!             -kx*(kx*cux(ky,kx)+ky*cuy(ky,kx))/(kx*kx+ky*ky)
! cuy(ky,kx) = cuy(ky,kx)
!             -ky*(kx*cux(ky,kx)+ky*cuy(ky,kx))/(kx*kx+ky*ky)
! where kx = 2pi*j/nx, ky = 2pi*k/ny, and j,k = fourier mode numbers,
! except for cux(kx=pi) = cuy(kx=pi) = 0, cux(ky=pi) = cuy(ky=pi) = 0,
! and cux(kx=0,ky=0) = cuy(kx=0,ky=0) = 0.
! cut(j,i,k) = complex current density for fourier mode (k,j)
! nx/ny = system length in x/y direction
! nxvh = third dimension of current array, must be >= nxh
! nyv = first dimension of current array, must be >= ny
      implicit none
      integer, value :: nx, ny, nxvh, nyv
      complex, dimension(nyv,3,nxvh) :: cut
! local data
      integer :: nxh, nyh, ny2, nxh1, j, k, k1
      real :: dnx, dny, dkx, dkx2, dky, at1
      complex :: zero, zt1
      nxh = nx/2
      nyh = max(1,ny/2)
      ny2 = ny + 2
      nxh1 = nxh + 1
      dnx = 6.28318530717959/real(nx)
      dny = 6.28318530717959/real(ny)
      zero = cmplx(0.0,0.0)
! calculate transverse part of current
! mode numbers 0 < kx < nx/2 and 0 < ky < ny/2
!     do j = 2, nxh
      j = blockIdx%x
      if ((j > 1) .and. (j <= nxh)) then
         dkx = dnx*real(j - 1)
         dkx2 = dkx*dkx
!        do k = 2, nyh
         k = threadIdx%x
         do while (k <= nyh)
            if (k > 1) then
               k1 = ny2 - k
               dky = dny*real(k - 1)
               at1 = 1.0/(dkx2 + dky*dky)
               zt1 = at1*(dkx*cut(k,1,j) + dky*cut(k,2,j))
               cut(k,1,j) = cut(k,1,j) - dkx*zt1
               cut(k,2,j) = cut(k,2,j) - dky*zt1
               zt1 = at1*(dkx*cut(k1,1,j) - dky*cut(k1,2,j))
               cut(k1,1,j) = cut(k1,1,j) - dkx*zt1
               cut(k1,2,j) = cut(k1,2,j) + dky*zt1
            endif
            k = k + blockDim%x
         enddo
      endif
! mode numbers ky = 0, ny/2
      if (blockIdx%x==1) then
         k1 = nyh + 1
!        do j = 2, nxh
         j = threadIdx%x
         do while (j <= nxh)
            if (j > 1) then
               cut(1,1,j) = zero
               cut(k1,1,j) = zero
               cut(k1,2,j) = zero
            endif
            j = j + blockDim%x
         enddo
! mode numbers kx = 0, nx/2
!        do k = 2, nyh
         k = threadIdx%x
         do while (k <= nyh)
            if (k > 1) then
               k1 = ny2 - k
               cut(k,2,1) = zero
               cut(k1,1,1) = conjg(cut(k,1,1))
               cut(k1,2,1) = zero
               cut(k,1,nxh1) = zero
               cut(k,2,nxh1) = zero
               cut(k1,1,nxh1) = zero
               cut(k1,2,nxh1) = zero
            endif
            k = k + blockDim%x
         enddo
         if (threadIdx%x==1) then
            k1 = nyh + 1
            cut(1,1,1) = zero
            cut(1,2,1) = zero
            cut(k1,1,1) = zero
            cut(k1,2,1) = zero
            cut(1,1,nxh1) = zero
            cut(1,2,nxh1) = zero
            cut(k1,1,nxh1) = zero
            cut(k1,2,nxh1) = zero
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpuibpois23t(cut,bxyt,ffct,ci,wm,nx,&
     &ny,nxvh,nyv,nxhd,nyhd)
! this subroutine solves 2-1/2d poisson's equation in fourier space for
! magnetic field, with periodic boundary conditions,
! without packed data.
! input: cut,ffct,ci,nx,ny,nxv,nyhd, output: bxyt,wm
! approximate flop count is: 90*nxc*nyc + 40*(nxc + nyc)
! where nxc = nx/2 - 1, nyc = ny/2 - 1
! the magnetic field is calculated using the equations:
! bx(ky,kx) = ci*ci*sqrt(-1)*g(ky,kx)*ky*cuz(ky,kx),
! by(ky,kx) = -ci*ci*sqrt(-1)*g(ky,kx)*kx*cuz(ky,kx),
! bz(ky,kx) = ci*ci*sqrt(-1)*g(ky,kx)]*(kx*cuy(ky,kx)-ky*cux(ky,kx)),
! where kx = 2pi*j/nx, ky = 2pi*k/ny, and j,k = fourier mode numbers,
! g(ky,kx) = (affp/(kx**2+ky**2))*s(ky,kx),
! s(ky,kx) = exp(-((kx*ax)**2+(ky*ay)**2)/2), except for
! bx(kx=pi) = by(kx=pi) = bz(kx=pi) = bx(ky=pi) = by(ky=pi) = bz(ky=pi) 
! = 0, and bx(kx=0,ky=0) = by(kx=0,ky=0) = bz(kx=0,ky=0) = 0.
! cut(k,i,j) = complex current density for fourier mode (k,j)
! bxyt(k,i,j) = i component of complex magnetic field
! all for fourier mode (k,j)
! aimag(ffc(k,j)) = finite-size particle shape factor s
! real(ffc(k,j)) = potential green's function g
! for fourier mode (j,k)
! ci = reciprical of velocity of light
! magnetic field energy is also calculated, using
! wm = nx*ny*sum((affp/(kx**2+ky**2))*ci*ci*
!    |cu(ky,kx)*s(ky,kx)|**2), where
! affp = normalization constant = nx*ny/np, where np=number of particles
! this expression is valid only if the current is divergence-free
! nx/ny = system length in x/y direction
! nxvh = third dimension of field arrays, must be >= nxh
! nyv = first dimension of field arrays, must be >= ny
! nxhd = second dimension of form factor array, must be >= nxh
! nyhd = first dimension of form factor array, must be >= nyh
      implicit none
      integer, value :: nx, ny, nxvh, nyv, nxhd, nyhd
      real, value :: ci
      complex, dimension(nyv,3,nxvh) :: cut, bxyt
      complex, dimension(nyhd,nxhd) :: ffct
      real, dimension(nxvh) :: wm
! local data
      integer :: nxh, nyh, ny2, nxh1,j, k, k1
      real :: dnx, dny, dkx, ci2, at1, at2, at3
      complex :: zero, zt1, zt2, zt3
! The size of the shared memory array is as follows:
! real ss(blockDim%x)
      real, shared, dimension(blockDim%x) :: ss
      double precision :: wp
      nxh = nx/2
      nyh = max(1,ny/2)
      ny2 = ny + 2
      nxh1 = nxh + 1
      dnx = 6.28318530717959/real(nx)
      dny = 6.28318530717959/real(ny)
      zero = cmplx(0.0,0.0)
      ci2 = ci*ci
! calculate magnetic field and sum field energy
      wp = 0.0d0
! mode numbers 0 < kx < nx/2 and 0 < ky < ny/2
!     do j = 2, nxh
      j = blockIdx%x
      if ((j > 1) .and. (j <= nxh)) then
         dkx = dnx*real(j - 1)
!        do k = 2, nyh
         k = threadIdx%x
         do while (k <= nyh)
            if (k > 1) then
               k1 = ny2 - k
               at1 = ci2*real(ffct(k,j))
               at2 = dkx*at1
               at3 = dny*real(k - 1)*at1
               at1 = at1*aimag(ffct(k,j))
               zt1 = cmplx(-aimag(cut(k,3,j)),real(cut(k,3,j)))
               zt2 = cmplx(-aimag(cut(k,2,j)),real(cut(k,2,j)))
               zt3 = cmplx(-aimag(cut(k,1,j)),real(cut(k,1,j)))
               bxyt(k,1,j) = at3*zt1
               bxyt(k,2,j) = -at2*zt1
               bxyt(k,3,j) = at2*zt2 - at3*zt3
               wp = wp + dble(at1*(cut(k,1,j)*conjg(cut(k,1,j))         &
     &            + cut(k,2,j)*conjg(cut(k,2,j))                        &
     &            + cut(k,3,j)*conjg(cut(k,3,j))))
               zt1 = cmplx(-aimag(cut(k1,3,j)),real(cut(k1,3,j)))
               zt2 = cmplx(-aimag(cut(k1,2,j)),real(cut(k1,2,j)))
               zt3 = cmplx(-aimag(cut(k1,1,j)),real(cut(k1,1,j)))
               bxyt(k1,1,j) = -at3*zt1
               bxyt(k1,2,j) = -at2*zt1
               bxyt(k1,3,j) = at2*zt2 + at3*zt3
               wp = wp + dble(at1*(cut(k1,1,j)*conjg(cut(k1,1,j))       &
     &            + cut(k1,2,j)*conjg(cut(k1,2,j))                      &
     &            + cut(k1,3,j)*conjg(cut(k1,3,j))))
            endif
            k = k + blockDim%x
         enddo
      endif
! mode numbers ky = 0, ny/2
      if (blockIdx%x==1) then
         k1 = nyh + 1
!        do j = 2, nxh
         j = threadIdx%x
         do while (j <= nxh)
            if (j > 1) then
               at1 = ci2*real(ffct(1,j))
               at2 = dnx*real(j - 1)*at1
               at1 = at1*aimag(ffct(1,j))
               zt1 = cmplx(-aimag(cut(1,3,j)),real(cut(1,3,j)))
               zt2 = cmplx(-aimag(cut(1,2,j)),real(cut(1,2,j)))
               bxyt(1,1,j) = zero
               bxyt(1,2,j) = -at2*zt1
               bxyt(1,3,j) = at2*zt2
               bxyt(k1,1,j) = zero
               bxyt(k1,2,j) = zero
               bxyt(k1,3,j) = zero
               wp = wp + dble(at1*(cut(1,1,j)*conjg(cut(1,1,j))         &
     &            + cut(1,2,j)*conjg(cut(1,2,j))                        &
     &            + cut(1,3,j)*conjg(cut(1,3,j))))
            endif
            j = j + blockDim%x
         enddo
! mode numbers kx = 0, nx/2
!        do k = 2, nyh
         k = threadIdx%x
         do while (k <= nyh)
            if (k > 1) then
               k1 = ny2 - k
               at1 = ci2*real(ffct(k,1))
               at3 = dny*real(k - 1)*at1
               at1 = at1*aimag(ffct(k,1))
               zt1 = cmplx(-aimag(cut(k,3,1)),real(cut(k,3,1)))
               zt3 = cmplx(-aimag(cut(k,1,1)),real(cut(k,1,1)))
               bxyt(k,1,1) = at3*zt1
               bxyt(k,2,1) = zero
               bxyt(k,3,1) = -at3*zt3
               bxyt(k1,1,1) = at3*conjg(zt1)
               bxyt(k1,2,1) = zero
               bxyt(k1,3,1) = -at3*conjg(zt3)
               bxyt(k,1,nxh1) = zero
               bxyt(k,2,nxh1) = zero
               bxyt(k,3,nxh1) = zero
               bxyt(k1,1,nxh1) = zero
               bxyt(k1,2,nxh1) = zero
               bxyt(k1,3,nxh1) = zero
               wp = wp + dble(at1*(cut(k,1,1)*conjg(cut(k,1,1))         &
     &            + cut(k,2,1)*conjg(cut(k,2,1))                        &
     &            + cut(k,3,1)*conjg(cut(k,3,1))))
            endif
            k = k + blockDim%x
         enddo
         if (threadIdx%x==1) then
            k1 = nyh + 1
            bxyt(1,1,1) = zero
            bxyt(1,2,1) = zero
            bxyt(1,3,1) = zero
            bxyt(k1,1,1) = zero
            bxyt(k1,2,1) = zero
            bxyt(k1,3,1) = zero
            bxyt(1,1,nxh1) = zero
            bxyt(1,2,nxh1) = zero
            bxyt(1,3,nxh1) = zero
            bxyt(k1,1,nxh1) = zero
            bxyt(k1,2,nxh1) = zero
            bxyt(k1,3,nxh1) = zero
         endif
      endif
      j = blockIdx%x
      if (j <= nxh1) then
! sum magnetic energies for each x co-ordinate
         ss(threadIdx%x) = real(wp)
! synchronize threads/
         call syncthreads()
         call lsum2(ss,blockDim%x)
! normalize magnetic energy for each x co-ordinate
         if (threadIdx%x==1) wm(j) = ss(1)*real(nx*ny)
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpumaxwel2t(exyt,bxyt,cut,ffct,ci,dt&
     &,wf,wm,nx,ny,nxvh,nyv,nxhd,nyhd)
! this subroutine solves 2-1/2d maxwell's equation in fourier space for
! transverse electric and magnetic fields with periodic boundary
! conditions, without packed data.
! input: all, output: wf, wm, exy, bxy
! approximate flop count is: 286*nxc*nyc + 84*(nxc + nyc)
! where nxc = nx/2 - 1, nyc = ny/2 - 1
! the magnetic field is first updated half a step using the equations:
! bx(ky,kx) = bx(ky,kx) - .5*dt*sqrt(-1)*ky*ez(ky,kx)
! by(ky,kx) = by(ky,kx) + .5*dt*sqrt(-1)*kx*ez(ky,kx)
! bz(ky,kx) = bz(ky,kx) - .5*dt*sqrt(-1)*(kx*ey(ky,kx)-ky*ex(ky,kx))
! the electric field is then updated a whole step using the equations:
! ex(ky,kx) = ex(ky,kx) + c2*dt*sqrt(-1)*ky*bz(ky,kx)
!                         - affp*dt*cux(ky,kx)*s(ky,kx)
! ey(ky,kx) = ey(ky,kx) - c2*dt*sqrt(-1)*kx*bz(ky,kx)
!                         - affp*dt*cuy(ky,kx)*s(ky,kx)
! ez(ky,kx) = ez(ky,kx) + c2*dt*sqrt(-1)*(kx*by(ky,kx)-ky*bx(ky,kx))
!                         - affp*dt*cuz(ky,kx)*s(ky,kx)
! the magnetic field is finally updated the remaining half step with
! the new electric field and the previous magnetic field equations.
! where kx = 2pi*j/nx, ky = 2pi*k/ny, c2 = 1./(ci*ci)
! and s(ky,kx) = exp(-((kx*ax)**2+(ky*ay)**2)
! j,k = fourier mode numbers, except for
! ex(kx=pi) = ey(kx=pi) = ez(kx=pi) = 0,
! ex(ky=pi) = ey(ky=pi) = ex(ky=pi) = 0,
! ex(kx=0,ky=0) = ey(kx=0,ky=0) = ez(kx=0,ky=0) = 0.
! and similarly for bx, by, bz.
! cut(k,i,j) = complex current density
! exyt(k,i,j) = complex transverse electric field
! bxyt(k,i,j) = complex magnetic field
! for component i, all for fourier mode (k,j)
! real(ffct(1,1)) = affp = normalization constant = nx*ny/np,
! where np=number of particles
! aimag(ffct(k,j)) = finite-size particle shape factor s,
! s(ky,kx) = exp(-((kx*ax)**2+(ky*ay)**2)/2)
! for fourier mode (k,j)
! ci = reciprical of velocity of light
! dt = time interval between successive calculations
! transverse electric field energy is also calculated, using
! wf = nx*ny**sum((1/affp)*|exy(ky,kx)|**2)
! magnetic field energy is also calculated, using
! wm = nx*ny**sum((c2/affp)*|bxy(ky,kx)|**2)
! nx/ny = system length in x/y direction
! nxvh = third dimension of field arrays, must be >= nxh
! nyv = first dimension of field arrays, must be >= ny
! nxhd = second dimension of form factor array, must be >= nxh
! nyhd = first dimension of form factor array, must be >= nyh
      implicit none
      integer, value :: nx, ny, nxvh, nyv, nxhd, nyhd
      real, value :: ci, dt
      complex, dimension(nyv,3,nxvh) :: exyt, bxyt, cut
      complex, dimension(nyhd,nxhd) :: ffct
      real, dimension(nxvh) :: wf, wm
! local data
      integer :: nxh, nyh, ny2, nxh1, j, k, k1
      real :: dnx, dny, dth, c2, cdt, affp, anorm, dkx, dky, afdt, adt
      complex :: zero, zt1, zt2, zt3, zt4, zt5, zt6, zt7, zt8, zt9
! The size of the shared memory array is as follows:
! real ss(blockDim%x)
      real, shared, dimension(blockDim%x) :: ss
      double precision :: wp, ws
      nxh = nx/2
      nyh = max(1,ny/2)
      ny2 = ny + 2
      nxh1 = nxh + 1
      dnx = 6.28318530717959/real(nx)
      dny = 6.28318530717959/real(ny)
      dth = .5*dt
      c2 = 1./(ci*ci)
      cdt = c2*dt
      affp = real(ffct(1,1))
      adt = affp*dt
      zero = cmplx(0.0,0.0)
      anorm = 1.0/affp
! update electromagnetic field and sum field energies
      wp = 0.0d0
      ws = 0.0d0
! calculate the electromagnetic fields
! mode numbers 0 < kx < nx/2 and 0 < ky < ny/2
!     do j = 2, nxh
      j = blockIdx%x
      if ((j > 1) .and. (j <= nxh)) then
         dkx = dnx*real(j - 1)
!        do k = 2, nyh
         k = threadIdx%x
         do while (k <= nyh)
            if (k > 1) then
               k1 = ny2 - k
               dky = dny*real(k - 1)
               afdt = adt*aimag(ffct(k,j))
! update magnetic field half time step, ky > 0
               zt1 = cmplx(-aimag(exyt(k,3,j)),real(exyt(k,3,j)))
               zt2 = cmplx(-aimag(exyt(k,2,j)),real(exyt(k,2,j)))
               zt3 = cmplx(-aimag(exyt(k,1,j)),real(exyt(k,1,j)))
               zt4 = bxyt(k,1,j) - dth*(dky*zt1)
               zt5 = bxyt(k,2,j) + dth*(dkx*zt1)
               zt6 = bxyt(k,3,j) - dth*(dkx*zt2 - dky*zt3)
! update electric field whole time step
               zt1 = cmplx(-aimag(zt6),real(zt6))
               zt2 = cmplx(-aimag(zt5),real(zt5))
               zt3 = cmplx(-aimag(zt4),real(zt4))
               zt7 = exyt(k,1,j) + cdt*(dky*zt1) - afdt*cut(k,1,j)
               zt8 = exyt(k,2,j) - cdt*(dkx*zt1) - afdt*cut(k,2,j)
               zt9 = exyt(k,3,j) + cdt*(dkx*zt2 - dky*zt3) - afdt*cut(k,3,j)
! update magnetic field half time step and store electric field
               zt1 = cmplx(-aimag(zt9),real(zt9))
               zt2 = cmplx(-aimag(zt8),real(zt8))
               zt3 = cmplx(-aimag(zt7),real(zt7))
               exyt(k,1,j) = zt7
               exyt(k,2,j) = zt8
               exyt(k,3,j) = zt9
               ws = ws + dble(anorm*(zt7*conjg(zt7) + zt8*conjg(zt8)    &
     &            + zt9*conjg(zt9)))
               zt4 = zt4 - dth*(dky*zt1)
               zt5 = zt5 + dth*(dkx*zt1)
               zt6 = zt6 - dth*(dkx*zt2 - dky*zt3)
               bxyt(k,1,j) = zt4
               bxyt(k,2,j) = zt5
               bxyt(k,3,j) = zt6
               wp = wp + dble(anorm*(zt4*conjg(zt4) + zt5*conjg(zt5)    &
     &            + zt6*conjg(zt6)))
! update magnetic field half time step, ky < 0
               zt1 = cmplx(-aimag(exyt(k1,3,j)),real(exyt(k1,3,j)))
               zt2 = cmplx(-aimag(exyt(k1,2,j)),real(exyt(k1,2,j)))
               zt3 = cmplx(-aimag(exyt(k1,1,j)),real(exyt(k1,1,j)))
               zt4 = bxyt(k1,1,j) + dth*(dky*zt1)
               zt5 = bxyt(k1,2,j) + dth*(dkx*zt1)
               zt6 = bxyt(k1,3,j) - dth*(dkx*zt2 + dky*zt3)
! update electric field whole time step
               zt1 = cmplx(-aimag(zt6),real(zt6))
               zt2 = cmplx(-aimag(zt5),real(zt5))
               zt3 = cmplx(-aimag(zt4),real(zt4))
               zt7 = exyt(k1,1,j) - cdt*(dky*zt1) - afdt*cut(k1,1,j)
               zt8 = exyt(k1,2,j) - cdt*(dkx*zt1) - afdt*cut(k1,2,j)
               zt9 = exyt(k1,3,j) + cdt*(dkx*zt2 + dky*zt3) - afdt*cut(k1,3,j)
! update magnetic field half time step and store electric field
               zt1 = cmplx(-aimag(zt9),real(zt9))
               zt2 = cmplx(-aimag(zt8),real(zt8))
               zt3 = cmplx(-aimag(zt7),real(zt7))
               exyt(k1,1,j) = zt7
               exyt(k1,2,j) = zt8
               exyt(k1,3,j) = zt9
               ws = ws + dble(anorm*(zt7*conjg(zt7) + zt8*conjg(zt8)    &
     &            + zt9*conjg(zt9)))
               zt4 = zt4 + dth*(dky*zt1)
               zt5 = zt5 + dth*(dkx*zt1)
               zt6 = zt6 - dth*(dkx*zt2 + dky*zt3)
               bxyt(k1,1,j) = zt4
               bxyt(k1,2,j) = zt5
               bxyt(k1,3,j) = zt6
               wp = wp + dble(anorm*(zt4*conjg(zt4) + zt5*conjg(zt5)    &
     &            + zt6*conjg(zt6)))
            endif
            k = k + blockDim%x
         enddo
      endif
! mode numbers ky = 0, ny/2
      if (blockIdx%x==1) then
         k1 = nyh + 1
!        do j = 2, nxh
         j = threadIdx%x
         do while (j <= nxh)
            if (j > 1) then
               dkx = dnx*real(j - 1)
               afdt = adt*aimag(ffct(1,j))
! update magnetic field half time step
               zt1 = cmplx(-aimag(exyt(1,3,j)),real(exyt(1,3,j)))
               zt2 = cmplx(-aimag(exyt(1,2,j)),real(exyt(1,2,j)))
               zt5 = bxyt(1,2,j) + dth*(dkx*zt1)
               zt6 = bxyt(1,3,j) - dth*(dkx*zt2)
! update electric field whole time step
               zt1 = cmplx(-aimag(zt6),real(zt6))
               zt2 = cmplx(-aimag(zt5),real(zt5))
               zt8 = exyt(1,2,j) - cdt*(dkx*zt1) - afdt*cut(1,2,j)
               zt9 = exyt(1,3,j) + cdt*(dkx*zt2) - afdt*cut(1,3,j)
! update magnetic field half time step and store electric field
               zt1 = cmplx(-aimag(zt9),real(zt9))
               zt2 = cmplx(-aimag(zt8),real(zt8))
               exyt(1,1,j) = zero
               exyt(1,2,j) = zt8
               exyt(1,3,j) = zt9
               ws = ws + dble(anorm*(zt8*conjg(zt8) + zt9*conjg(zt9)))
               zt5 = zt5 + dth*(dkx*zt1)
               zt6 = zt6 - dth*(dkx*zt2)
               bxyt(1,1,j) = zero
               bxyt(1,2,j) = zt5
               bxyt(1,3,j) = zt6
               wp = wp + dble(anorm*(zt5*conjg(zt5) + zt6*conjg(zt6)))
               bxyt(k1,1,j) = zero
               bxyt(k1,2,j) = zero
               bxyt(k1,3,j) = zero
               exyt(k1,1,j) = zero
               exyt(k1,2,j) = zero
               exyt(k1,3,j) = zero
            endif
            j = j + blockDim%x
         enddo
! mode numbers kx = 0, nx/2
!        do k = 2, nyh
         k = threadIdx%x
         do while (k <= nyh)
            if (k > 1) then
               k1 = ny2 - k
               dky = dny*real(k - 1)
               afdt = adt*aimag(ffct(k,1))
! update magnetic field half time step
               zt1 = cmplx(-aimag(exyt(k,3,1)),real(exyt(k,3,1)))
               zt3 = cmplx(-aimag(exyt(k,1,1)),real(exyt(k,1,1)))
               zt4 = bxyt(k,1,1) - dth*(dky*zt1)
               zt6 = bxyt(k,3,1) + dth*(dky*zt3)
! update electric field whole time step
               zt1 = cmplx(-aimag(zt6),real(zt6))
               zt3 = cmplx(-aimag(zt4),real(zt4))
               zt7 = exyt(k,1,1) + cdt*(dky*zt1) - afdt*cut(k,1,1)
               zt9 = exyt(k,3,1) - cdt*(dky*zt3) - afdt*cut(k,3,1)
! update magnetic field half time step and store electric field
               zt1 = cmplx(-aimag(zt9),real(zt9))
               zt3 = cmplx(-aimag(zt7),real(zt7))
               exyt(k,1,1) = zt7
               exyt(k,2,1) = zero
               exyt(k,3,1) = zt9
               ws = ws + dble(anorm*(zt7*conjg(zt7) + zt9*conjg(zt9)))
               zt4 = zt4 - dth*(dky*zt1)
               zt6 = zt6 + dth*(dky*zt3)
               bxyt(k,1,1) = zt4
               bxyt(k,2,1) = zero
               bxyt(k,3,1) = zt6
               wp = wp + dble(anorm*(zt4*conjg(zt4) + zt6*conjg(zt6)))
               bxyt(k1,1,1) = conjg(zt4)
               bxyt(k1,2,1) = zero
               bxyt(k1,3,1) = conjg(zt6)
               exyt(k1,1,1) = conjg(zt7)
               exyt(k1,2,1) = zero
               exyt(k1,3,1) = conjg(zt9)
               bxyt(k,1,nxh1) = zero
               bxyt(k,2,nxh1) = zero
               bxyt(k,3,nxh1) = zero
               exyt(k,1,nxh1) = zero
               exyt(k,2,nxh1) = zero
               exyt(k,3,nxh1) = zero
               bxyt(k1,1,nxh1) = zero
               bxyt(k1,2,nxh1) = zero
               bxyt(k1,3,nxh1) = zero
               exyt(k1,1,nxh1) = zero
               exyt(k1,2,nxh1) = zero
               exyt(k1,3,nxh1) = zero
            endif
            k = k + blockDim%x
         enddo
         if (threadIdx%x==1) then
            k1 = nyh + 1
            bxyt(1,1,1) = zero
            bxyt(1,2,1) = zero
            bxyt(1,3,1) = zero
            exyt(1,1,1) = zero
            exyt(1,2,1) = zero
            exyt(1,3,1) = zero
            bxyt(k1,1,1) = zero
            bxyt(k1,2,1) = zero
            bxyt(k1,3,1) = zero
            exyt(k1,1,1) = zero
            exyt(k1,2,1) = zero
            exyt(k1,3,1) = zero
            bxyt(1,1,nxh1) = zero
            bxyt(1,2,nxh1) = zero
            bxyt(1,3,nxh1) = zero
            exyt(1,1,nxh1) = zero
            exyt(1,2,nxh1) = zero
            exyt(1,3,nxh1) = zero
            bxyt(k1,1,nxh1) = zero
            bxyt(k1,2,nxh1) = zero
            bxyt(k1,3,nxh1) = zero
            exyt(k1,1,nxh1) = zero
            exyt(k1,2,nxh1) = zero
            exyt(k1,3,nxh1) = zero
         endif
      endif
      j = blockIdx%x
      if (j <= nxh1) then
! sum transverse electric field energies for each x co-ordinat
         ss(threadIdx%x) = real(ws)
! synchronize threads/
         call syncthreads()
         call lsum2(ss,blockDim%x)
! normalize transverse electric field energy for each x co-ordinate
         if (threadIdx%x==1) wf(j) = ss(1)*real(nx*ny)
! sum magnetic energies for each x co-ordinate
         ss(threadIdx%x) = real(wp)
! synchronize threads/
         call syncthreads()
         call lsum2(ss,blockDim%x)
! normalize magnetic energy for each x co-ordinate
         if (threadIdx%x==1) wm(j) = c2*ss(1)*real(nx*ny)
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpuemfield2t(fxyt,exyt,ffct,isign,nx&
     &,ny,nxvh,nyv,nxhd,nyhd)
! this subroutine either adds complex vector fields if isign > 0
! or copies complex vector fields if isign < 0
! includes additional smoothing
      implicit none
      integer, value :: isign, nx, ny, nxvh, nyv, nxhd, nyhd
      complex, dimension(nyv,3,nxvh) :: fxyt, exyt
      complex, dimension(nyhd,nxhd) :: ffct
! local data
      integer :: i, j, k, nxh, nyh, ny2, nxh1, k1
      real :: at1
      complex :: zero
      nxh = nx/2
      nyh = max(1,ny/2)
      ny2 = ny + 2
      nxh1 = nxh + 1
      zero = cmplx(0.0,0.0)
! add the fields
      if (isign > 0) then
!        do j = 1, nxh
         j = blockIdx%x
         if (j <= nxh) then
!           do k = 2, nyh
            k = threadIdx%x
            do while (k <= nyh)
               if (k > 1) then
                  k1 = ny2 - k
                  at1 = aimag(ffct(k,j))
                  do i = 1, 3
                     fxyt(k,i,j) = fxyt(k,i,j) + exyt(k,i,j)*at1
                     fxyt(k1,i,j) = fxyt(k1,i,j) + exyt(k1,i,j)*at1
                  enddo
               endif
               k = k + blockDim%x
            enddo
         endif
         if (blockIdx%x==1) then
            k1 = nyh + 1
!           do j = 1, nxh
            j = threadIdx%x
            do while (j <= nxh)
               at1 = aimag(ffct(1,j))
               do i = 1, 3
                  fxyt(1,i,j) = fxyt(1,i,j) + exyt(1,i,j)*at1
                  fxyt(k1,i,j) = fxyt(k1,i,j) + exyt(k1,i,j)*at1
               enddo
               j = j + blockDim%x
            enddo
!           do k = 2, nyh
            k = threadIdx%x
            do while (k <= nyh)
               if (k > 1) then
                  k1 = ny2 - k
                  do i = 1, 3
                     fxyt(k,i,nxh1) = zero
                     fxyt(k1,i,nxh1) = zero
                  enddo
               endif
               k = k + blockDim%x
            enddo
            if (threadIdx%x==1) then
               k1 = nyh + 1
               do i = 1, 3
                  fxyt(1,i,nxh1) = zero
                  fxyt(k1,i,nxh1) = zero
               enddo
            endif
         endif
! copy the fields
      else if (isign < 0) then
!        do j = 1, nxh
         j = blockIdx%x
         if (j <= nxh) then
!           do k = 2, nyh
            k = threadIdx%x
            do while (k <= nyh)
               if (k > 1) then
                  k1 = ny2 - k
                  at1 = aimag(ffct(k,j))
                  do i = 1, 3
                     fxyt(k,i,j) = exyt(k,i,j)*at1
                     fxyt(k1,i,j) = exyt(k1,i,j)*at1
                  enddo
               endif
               k = k + blockDim%x
            enddo
         endif
         if (blockIdx%x==1) then
            k1 = nyh + 1
!           do j = 1, nxh
            j = threadIdx%x
            do while (j <= nxh)
               at1 = aimag(ffct(1,j))
               do i = 1, 3
                  fxyt(1,i,j) = exyt(1,i,j)*at1
                  fxyt(k1,i,j) = exyt(k1,i,j)*at1
               enddo
               j = j + blockDim%x
            enddo
!           do k = 2, nyh
            k = threadIdx%x
            do while (k <= nyh)
               if (k > 1) then
                  k1 = ny2 - k
                  do i = 1, 3
                     fxyt(k,i,nxh1) = zero
                     fxyt(k1,i,nxh1) = zero
                  enddo
               endif
               k = k + blockDim%x
            enddo
            if (threadIdx%x==1) then
               k1 = nyh + 1
               do i = 1, 3
                  fxyt(1,i,nxh1) = zero
                  fxyt(k1,i,nxh1) = zero
               enddo
            endif
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpuctpose4(f,g,nx,ny,nxv,nyv)
! complex transpose using blocking algorithm with gaps
      implicit none
      integer, value :: nx, ny, nxv, nyv
      complex, dimension(nxv,nyv) :: f
      complex, dimension(nyv,nxv) :: g
! local data
      integer :: j, k, js, ks, joff, koff, mx, mxv
! The size of the shared memory array is as follows:
! complex shm2((blockDim%x+1)*blockDim%x)
      complex, shared, dimension((blockDim%x+1)*blockDim%x) :: shm2
      mx = blockDim%x
      mxv = mx + 1
      joff = mx*(blockIdx%x - 1)
      koff = mx*(blockIdx%y - 1)
      js = threadIdx%x
      ks = threadIdx%y
! copy into block
      j = js + joff
      k = ks + koff
      if ((j <= nx) .and. (k <= ny)) then
         shm2(js+mxv*(ks-1)) = f(j,k)
      endif
      call syncthreads()
! copy out from block
      j = ks + joff
      k = js + koff
      if ((j <= nx) .and. (k <= ny)) then
         g(k,j) = shm2(ks+mxv*(js-1))
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpuctpose4n(fn,gn,nx,ny,ndim,nxv,nyv&
     &)
! complex vector transpose using blocking algorithm with gaps
! ndim = vector dimension
      implicit none
      integer, value :: nx, ny, ndim, nxv, nyv
      complex, dimension(nxv,ndim,nyv) :: fn
      complex, dimension(nyv,ndim,nxv) :: gn
! local data
      integer :: i, j, k, js, ks, joff, koff, mx, mxv, nmxv, jj, kk
! The size of the shared memory array is as follows:
! complex shm2((blockDim%x+1)*ndim*blockDim%x)
      complex, shared, dimension((blockDim%x+1)*ndim*blockDim%x) ::     &
     &shmn2
      mx = blockDim%x
      mxv = mx + 1
      joff = mx*(blockIdx%x - 1)
      koff = mx*(blockIdx%y - 1)
      js = threadIdx%x
      ks = threadIdx%y
      nmxv = ndim*mxv
! copy into block
      j = js + joff
      k = ks + koff
      if ((j <= nx) .and. (k <= ny)) then
         kk = js + nmxv*(ks - 1)
         do i = 1, ndim
            shmn2(kk+mxv*(i-1)) = fn(j,i,k)
         enddo
      endif
      call syncthreads()
! copy out from block
      j = ks + joff
      k = js + koff
      if ((j <= nx) .and. (k <= ny)) then
         jj = ks + nmxv*(js - 1)
         do i = 1, ndim
            gn(k,i,j) = shmn2(jj+mxv*(i-1))
         enddo
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpufft2rcxs(f,isign,mixup,sct,indx, &
     &indy,nyi,nyp,nxhd,nyd,nxhyd,nxyhd,nsize)
! this subroutine performs the x part of a two dimensional real to
! complex fast fourier transform and its inverse, for a subset of y,
! using complex arithmetic, with data not packed
! for isign = (-1,1), input: all, output: f
! for isign = -1, approximate flop count: N*(5*log2(N) + 19/2)
! for isign = 1,  approximate flop count: N*(5*log2(N) + 15/2)
! where N = (nx/2)*ny
! indx/indy = exponent which determines length in x/y direction,
! where nx=2**indx, ny=2**indy
! if isign = -1, an inverse fourier transform in x is performed
! f(n,m) = (1/nx*ny)*sum(f(j,k)*exp(-sqrt(-1)*2pi*n*j/nx))
! if isign = 1, a forward fourier transform in x is performed
! f(j,k) = sum(f(n,m)*exp(sqrt(-1)*2pi*n*j/nx))
! mixup = array of bit reversed addresses
! sct = sine/cosine table
! nyi = initial y index used
! nyp = number of y indices used
! nxhd = first dimension of f >= nx/2+1
! nyd = second dimension of f >= ny
! nxhyd = maximum of (nx/2,ny)
! nxyhd = maximum of (nx,ny)/2
! nsize = amount of scratch complex memory used
! fourier coefficients are stored as follows:
! f(j,k) = real, imaginary part of mode j-1,k-1, where
! 1 <= j <= nx/2+1 and 1 <= k <= ny
! written by viktor k. decyk, ucla
      implicit none
      integer, value :: isign, indx, indy, nyi, nyp, nxhd, nyd
      integer, value :: nxhyd, nxyhd, nsize
      complex, dimension(nxhd,nyd) :: f
      integer, dimension(nxhyd) :: mixup
      complex, dimension(nxyhd) :: sct
! local data
      integer :: indx1, indx1y, nx, nxh, nxhh, nxh2, ny, nxy, nxhy, nyt
      integer :: nrx, i, j, k, l, j1, j2, k1, k2, ns, ns2, km, kmr, kk
      integer :: n, nn, in, nt, nh
      real :: ani, at1, at2
      complex :: t1, t2, t3
! The size of the shared memory array is as follows:
! complex s(nsize)
      complex, shared, dimension(nsize) :: s
      indx1 = indx - 1
      indx1y = max0(indx1,indy)
      nx = 2**indx
      nxh = nx/2
      nxhh = nx/4
      nxh2 = nxh + 2
      ny = 2**indy
      nxy = max0(nx,ny)
      nxhy = 2**indx1y
      nyt = nyi + nyp - 1
! calculate extent of shared memory usage:
! nn = size of shared memory in x
      nn = nxh
      in = 0
      do while (nn > nsize)
         nn = nn/2
         in = in + 1
      enddo
! nt = number of iterations in x
      nt = 2**in
      in = indx1 - in
      nh = nn/2
! inverse fourier transform
      if (isign < 0) then
! bit-reverse array elements in x
         nrx = nxhy/nxh
         k = blockIdx%x + nyi - 1
!        do k = nyi, nyt
         if (k <= nyt) then
!           do j = 1, nxh
            j = threadIdx%x
            do while (j <= nxh)
               j1 = (mixup(j) - 1)/nrx + 1
               if (j < j1) then
                  t1 = f(j1,k)
                  f(j1,k) = f(j,k)
                  f(j,k) = t1
               endif
               j = j + blockDim%x
            enddo
! synchronize threads
            call syncthreads()
         endif
! copy data to local memory
         nrx = nxy/nxh
!        do i = nyi, nyt
         i = blockIdx%x + nyi - 1
         if (i <= nyt) then
            do n = 1, nt
!              do kk = 1, nn
               kk = threadIdx%x
               do while (kk <= nn) 
                  s(kk) = f(kk+nn*(n-1),i)
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
! transform using local data in x
               do l = 1, in
                  ns = 2**(l - 1)
                  ns2 = ns + ns
                  km = nxhh/ns
                  kmr = km*nrx
!                 do kk = 1, nh
                  kk = threadIdx%x
                  do while (kk <= nh)
                     k = (kk - 1)/ns
                     j = kk - ns*k
                     k = k + 1
                     k1 = ns2*(k - 1)
                     k2 = k1 + ns
                     j1 = j + k1
                     j2 = j + k2
                     t1 = sct(1+kmr*(j-1))
                     t3 = s(j1)
                     t2 = t1*s(j2)
                     s(j2) = t3 - t2
                     s(j1) = t3 + t2
                     kk = kk + blockDim%x
                  enddo
! synchronize threads
                  call syncthreads()
               enddo
! copy data to global memory
!              do kk = 1, nn
               kk = threadIdx%x
               do while (kk <= nn)
                  f(kk+nn*(n-1),i) = s(kk)
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
            enddo
! transform using global data in x
            in = in + 1
            do l = in, indx1
               ns = 2**(l - 1)
               ns2 = ns + ns
               km = nxhh/ns
               kmr = km*nrx
!              do kk = 1, nxhh
               kk = threadIdx%x
               do while (kk <= nxhh)
                  k = (kk - 1)/ns
                  j = kk - ns*k
                  k = k + 1
                  k1 = ns2*(k - 1)
                  k2 = k1 + ns
                  j1 = j + k1
                  j2 = j + k2
                  t1 = sct(1+kmr*(j-1))
                  t3 = f(j1,i)
                  t2 = t1*f(j2,i)
                  f(j2,i) = t3 - t2
                  f(j1,i) = t3 + t2
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
            enddo
         endif
! unscramble coefficients and normalize
         kmr = nxy/nx
         ani = 0.5/(real(nx)*real(ny))
!        do k = nyi, nyt
         k = blockIdx%x + nyi - 1
         if (k <= nyt) then
!           do j = 2, nxhh
            j = threadIdx%x
            do while (j <= nxhh)
               if (j > 1) then
                  t3 = cmplx(aimag(sct(1+kmr*(j-1))),                   &
     &                 -real(sct(1+kmr*(j-1))))
                  t2 = conjg(f(nxh2-j,k))
                  t1 = f(j,k) + t2
                  t2 = (f(j,k) - t2)*t3
                  f(j,k) = ani*(t1 + t2)
                  f(nxh2-j,k) = ani*conjg(t1 - t2)
               endif
               j = j + blockDim%x
            enddo
            if (threadIdx%x==1) then
               ani = 2.0*ani
               f(nxhh+1,k) = ani*conjg(f(nxhh+1,k))
               at1 = real(f(1,k))
               at2 = aimag(f(1,k))
               f(nxh+1,k) = ani*cmplx(at1-at2,0.0)
               f(1,k) = ani*cmplx(at1+at2,0.0)
            endif
! synchronize threads
            call syncthreads()
         endif
      endif
! forward fourier transform
      if (isign > 0) then
! scramble coefficients
         kmr = nxy/nx
!        do k = nyi, nyt
         k = blockIdx%x + nyi - 1
         if (k <= nyt) then
!           do j = 2, nxhh
            j = threadIdx%x
            do while (j <= nxhh)
               if (j > 1) then
                  t3 = cmplx(aimag(sct(1+kmr*(j-1))),                   &
     &                 real(sct(1+kmr*(j-1))))
                  t2 = conjg(f(nxh2-j,k))
                  t1 = f(j,k) + t2
                  t2 = (f(j,k) - t2)*t3
                  f(j,k) = t1 + t2
                  f(nxh2-j,k) = conjg(t1 - t2)
               endif
               j = j + blockDim%x
            enddo
            if (threadIdx%x==1) then
               f(nxhh+1,k) = 2.0*conjg(f(nxhh+1,k))
               at1 = real(f(1,k))
               at2 = real(f(nxh+1,k))
               f(1,k) = cmplx(at1+at2,at1-at2)
            endif
! synchronize threads
            call syncthreads()
         endif
! bit-reverse array elements in x
         nrx = nxhy/nxh
!        do k = nyi, nyt
         k = blockIdx%x + nyi - 1
         if (k <= nyt) then
!           do j = 1, nxh
            j = threadIdx%x
            do while (j <= nxh)
               j1 = (mixup(j) - 1)/nrx + 1
               if (j < j1) then
                  t1 = f(j1,k)
                  f(j1,k) = f(j,k)
                  f(j,k) = t1
               endif
               j = j + blockDim%x
            enddo
! synchronize threads
            call syncthreads()
         endif
! copy data to local memory
         nrx = nxy/nxh
!        do i = nyi, nyt
         i = blockIdx%x + nyi - 1
         if (i <= nyt) then
            do n = 1, nt
!              do kk = 1, nn
               kk = threadIdx%x
               do while (kk <= nn)
                  s(kk) = f(kk+nn*(n-1),i)
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
! transform using local data in x
               do l = 1, in
                  ns = 2**(l - 1)
                  ns2 = ns + ns
                  km = nxhh/ns
                  kmr = km*nrx
!                 do kk = 1, nh
                  kk = threadIdx%x
                  do while (kk <= nh)
                     k = (kk - 1)/ns
                     j = kk - ns*k
                     k = k + 1
                     k1 = ns2*(k - 1)
                     k2 = k1 + ns
                     j1 = j + k1
                     j2 = j + k2
                     t1 = conjg(sct(1+kmr*(j-1)))
                     t3 = s(j1)
                     t2 = t1*s(j2)
                     s(j2) = t3 - t2
                     s(j1) = t3 + t2
                     kk = kk + blockDim%x
                  enddo
! synchronize threads
                  call syncthreads()
               enddo
! copy data to global memory
!              do kk = 1, nn
               kk = threadIdx%x
               do while (kk <= nn)
                  f(kk+nn*(n-1),i) = s(kk)
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
            enddo
! transform using global data in x
            in = in + 1
            do l = in, indx1
               ns = 2**(l - 1)
               ns2 = ns + ns
               km = nxhh/ns
               kmr = km*nrx
!              do kk = 1, nxhh
               kk = threadIdx%x
               do while (kk <= nxhh)
                  k = (kk - 1)/ns
                  j = kk - ns*k
                  k = k + 1
                  k1 = ns2*(k - 1)
                  k2 = k1 + ns
                  j1 = j + k1
                  j2 = j + k2
                  t1 = conjg(sct(1+kmr*(j-1)))
                  t3 = f(j1,i)
                  t2 = t1*f(j2,i)
                  f(j2,i) = t3 - t2
                  f(j1,i) = t3 + t2
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
            enddo
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpufft2rcys(g,isign,mixup,sct,indx, &
     &indy,nxi,nxp,nxhd,nyd,nxhyd,nxyhd,nsize)
! this subroutine performs the y part of a two dimensional real to
! complex fast fourier transform and its inverse, for a subset of x,
! using complex arithmetic, with data not packed
! for isign = (-1,1), input: all, output: g
! for isign = -1, approximate flop count: N*(5*log2(N) + 19/2)
! for isign = 1,  approximate flop count: N*(5*log2(N) + 15/2)
! where N = (nx/2)*ny
! indx/indy = exponent which determines length in x/y direction,
! where nx=2**indx, ny=2**indy
! if isign = -1, an inverse fourier transform in y is performed
! g(m,n) = (1/nx*ny)*sum(g(k,j)**exp(-sqrt(-1)*2pi*m*k/ny))
! if isign = 1, a forward fourier transform in y is performed
! g(k,j) = sum(g(m,n)*exp(sqrt(-1)*2pi*m*k/ny))
! mixup = array of bit reversed addresses
! sct = sine/cosine table
! nxi = initial x index used
! nxp = number of x indices used
! nxhd = second dimension of g >= nx/2+1
! nyd = first dimension of g >= ny
! nxhyd = maximum of (nx/2,ny)
! nxyhd = maximum of (nx,ny)/2
! nsize = amount of scratch complex memory used
! fourier coefficients are stored as follows:
! g(k,j) = real, imaginary part of mode j-1,k-1, where
! 1 <= j <= nx/2+1 and 1 <= k <= ny
! written by viktor k. decyk, ucla
      implicit none
      integer, value :: isign, indx, indy, nxi, nxp, nxhd, nyd
      integer, value :: nxhyd, nxyhd, nsize
      complex, dimension(nyd,nxhd) :: g
      integer, dimension(nxhyd) :: mixup
      complex, dimension(nxyhd) :: sct
! local data
      integer :: indx1, indx1y, nx, ny, nyh, ny2, nxy, nxhy, nxt
      integer :: nry, i, j, k, l, j1, j2, k1, k2, ns, ns2, km, kmr, kk
      integer :: n, nn, in, nt, nh
      complex :: t1, t2, t3
! The size of the shared memory array is as follows:
! complex s(nsize)
      complex, shared, dimension(nsize) :: s
      indx1 = indx - 1
      indx1y = max0(indx1,indy)
      nx = 2**indx
      ny = 2**indy
      nyh = ny/2
      ny2 = ny + 2
      nxy = max0(nx,ny)
      nxhy = 2**indx1y
      nxt = nxi + nxp - 1
! calculate extent of shared memory usage:
! nn = size of shared memory in y
      nn = ny
      in = 0
      do while (nn > nsize)
         nn = nn/2
         in = in + 1
      enddo
! nt = number of iterations in y
      nt = 2**in
      in = indy - in
      nh = nn/2
! bit-reverse array elements in y
      nry = nxhy/ny
!     do j = nxi, nxt
      j = blockIdx%x + nxi - 1
      if (j <= nxt) then
!        do k = 1, ny
         k = threadIdx%x
         do while (k <= ny)
            k1 = (mixup(k) - 1)/nry + 1
            if (k < k1) then
               t1 = g(k1,j)
               g(k1,j) = g(k,j)
               g(k,j) = t1
            endif
            k = k + blockDim%x
         enddo
! synchronize threads
         call syncthreads()
      endif
      nry = nxy/ny
! inverse fourier transform in y
      if (isign < 0) then
! copy data to local memory
!        do i = nxi, nxt
         i = blockIdx%x + nxi - 1
         if (i <= nxt) then
            do n = 1, nt
!              do kk = 1, nn
               kk = threadIdx%x
               do while (kk <= nn)
                  s(kk) = g(kk+nn*(n-1),i)
                  kk = kk + blockDim%x
                enddo
! synchronize threads
               call syncthreads()
! transform using local data in y
               do l = 1, in
                  ns = 2**(l - 1)
                  ns2 = ns + ns
                  km = nyh/ns
                  kmr = km*nry
!                 do kk = 1, nh
                  kk = threadIdx%x
                  do while (kk <= nh)
                     k = (kk - 1)/ns
                     j = kk - ns*k
                     k = k + 1
                     k1 = ns2*(k - 1)
                     k2 = k1 + ns
                     j1 = j + k1
                     j2 = j + k2
                     t1 = sct(1+kmr*(j-1))
                     t3 = s(j1)
                     t2 = t1*s(j2)
                     s(j2) = t3 - t2
                     s(j1) = t3 + t2
                     kk = kk + blockDim%x
                  enddo
! synchronize threads
                  call syncthreads()
               enddo
! copy data to global memory
!              do kk = 1, nn
               kk = threadIdx%x
               do while (kk <= nn)
                  g(kk+nn*(n-1),i) = s(kk)
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
            enddo
            in = in + 1
            do l = in, indy
               ns = 2**(l - 1)
               ns2 = ns + ns
               km = nyh/ns
               kmr = km*nry
!              do kk = 1, nyh
               kk = threadIdx%x
               do while (kk <= nyh)
                  k = (kk - 1)/ns
                  j = kk - ns*k
                  k = k + 1
                  k1 = ns2*(k - 1)
                  k2 = k1 + ns
                  j1 = j + k1
                  j2 = j + k2
                  t1 = sct(1+kmr*(j-1))
                  t3 = g(j1,i)
                  t2 = t1*g(j2,i)
                  g(j2,i) = t3 - t2
                  g(j1,i) = t3 + t2
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
            enddo
         endif
      endif
! forward fourier transform in y
      if (isign > 0) then
! copy data to local memory
!        do i = nxi, nxt
         i = blockIdx%x + nxi - 1
         if (i <= nxt) then
            do n = 1, nt
!              do kk = 1, nn
               kk = threadIdx%x
               do while (kk <= nn)
                  s(kk) = g(kk+nn*(n-1),i)
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
! transform using local data in y
               do l = 1, in
                  ns = 2**(l - 1)
                  ns2 = ns + ns
                  km = nyh/ns
                  kmr = km*nry
!                 do kk = 1, nh
                  kk = threadIdx%x
                  do while (kk <= nh)
                     k = (kk - 1)/ns
                     j = kk - ns*k
                     k = k + 1
                     k1 = ns2*(k - 1)
                     k2 = k1 + ns
                     j1 = j + k1
                     j2 = j + k2
                     t1 = conjg(sct(1+kmr*(j-1)))
                     t3 = s(j1)
                     t2 = t1*s(j2)
                     s(j2) = t3 - t2
                     s(j1) = t3 + t2
                     kk = kk + blockDim%x
                  enddo
! synchronize threads
                  call syncthreads()
               enddo
! copy data to global memory
!              do kk = 1, nn
               kk = threadIdx%x
               do while (kk <= nn)
                  g(kk+nn*(n-1),i) = s(kk)
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
            enddo
! transform using global data in y
            in = in + 1
            do l = in, indy
               ns = 2**(l - 1)
               ns2 = ns + ns
               km = nyh/ns
               kmr = km*nry
!              do kk = 1, nyh
               kk = threadIdx%x
               do while (kk <= nyh)
                  k = (kk - 1)/ns
                  j = kk - ns*k
                  k = k + 1
                  k1 = ns2*(k - 1)
                  k2 = k1 + ns
                  j1 = j + k1
                  j2 = j + k2
                  t1 = conjg(sct(1+kmr*(j-1)))
                  t3 = g(j1,i)
                  t2 = t1*g(j2,i)
                  g(j2,i) = t3 - t2
                  g(j1,i) = t3 + t2
                  kk = kk + blockDim%x
               enddo
! synchronize threads
               call syncthreads()
            enddo
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpusum1(a,sa,nx)
! 1d serial sum reductions
! nx = length of data
! sa = sum(a)
      implicit none
      integer, value :: nx
      real, dimension(8) :: a, sa
! local data
      integer :: j, js, jb, mx, joff, mxm
      real :: t
      real, shared, dimension(*) :: ss
      mx = blockDim%x
      js = threadIdx%x
      jb = blockIdx%x
      joff = mx*(jb - 1)
      j = js + joff
! copy global data to shared memory
      if (j <= nx) ss(js) = a(j)
! synchronize to make sure each thread in block has the data
      call syncthreads()
      if (js==1) then
         mxm = nx - joff
         if (mxm > mx) mxm = mx
! perform serial local sum reduction: result in t
         t = 0.0
         do j = 1, mxm
            t = t + ss(j)
         enddo
! accumulate results to global memory for each block
! for devices with compute capability 2.x
         t = atomicAdd(sa(1),t)
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      attributes(global) subroutine gpusum2(a,d,nx)
! segmented 1d sum reductions, each of length mx = blockDim%x /
! nx = length of data 
! forall (j = 1:nbx); d(j) = sum(a(1+mx*(j-1):min(nx,mx*j))); end forall
      implicit none
      integer, value :: nx
      real, dimension(8) :: a, d
! local data
      integer :: j, js, jb, mx, joff, mxm
      real, shared, dimension(*) :: ss
      mx = blockDim%x
      js = threadIdx%x
      jb = blockIdx%x
      joff = mx*(jb - 1)
      j = js + joff
! copy global data to shared memory
      if (j <= nx) ss(js) = a(j)
! synchronize to make sure each thread in block has the data
      call syncthreads()
      mxm = nx - joff
      if (mxm > mx) mxm = mx
! perform parallel local sum reduction: result in s(1)
      call lsum2(ss,mxm)
! write out result to global memory for each block
      if (js==1) d(jb) = ss(1)
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpubppush23l(ppart,fxy,bxy,kpic,qbm,dt,dtc,ek,idimp,  &
     &nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
! Push Interface for Fortran
      implicit none
      integer, intent(in) :: idimp, nppmx, nx, ny, mx, my, nxv, nyv
      integer, intent(in) :: mx1, mxy1, ipbc
      real, intent(in) :: qbm, dt, dtc
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(3,nxv,nyv) :: fxy, bxy
      integer, device, dimension(mxy1) :: kpic
      real, device, dimension(mxy1) :: ek
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = 6*(mx + 1)*(my + 1)*sizeof(f)
      n = nblock_size*sizeof(f)
      ns = max(ns,n)
      crc = cudaGetLastError()
      call gpubppush23l<<<dimGrid,dimBlock,ns>>>(ppart,fxy,bxy,kpic,qbm,&
     &dt,dtc,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpubppush23l error=', crc, ':',                   &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpubppushf23l(ppart,fxy,bxy,kpic,ncl,ihole,qbm,dt,dtc,&
     &ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ntmax,irc)
! Push Interface for Fortran
      implicit none
      integer, intent(in) :: idimp, nppmx, nx, ny, mx, my, nxv, nyv
      integer, intent(in) :: mx1, mxy1, ntmax
      real, intent(in) :: qbm, dt, dtc
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(3,nxv,nyv) :: fxy, bxy
      integer, device, dimension(mxy1) :: kpic
      integer, device, dimension(8,mxy1) :: ncl
      integer, device, dimension(2,ntmax+1,mxy1) :: ihole
      real, device, dimension(mxy1) :: ek
      integer, device, dimension(1) :: irc
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = 6*(mx + 1)*(my + 1)*sizeof(f) + nblock_size*sizeof(n)
      n = nblock_size*sizeof(f)
      ns = max(ns,n)
      ns = ns + 9*sizeof(n)
      crc = cudaGetLastError()
      call gpubppushf23l<<<dimGrid,dimBlock,ns>>>(ppart,fxy,bxy,kpic,ncl&
     &,ihole,qbm,dt,dtc,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,    &
     &ntmax,irc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpubppushf23l error=', crc, ':',                  &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpurbppush23l(ppart,fxy,bxy,kpic,qbm,dt,dtc,ci,ek,    &
     &idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
! Push Interface for Fortran
      implicit none
      integer, intent(in) :: idimp, nppmx, nx, ny, mx, my, nxv, nyv
      integer, intent(in) :: mx1, mxy1, ipbc
      real, intent(in) :: qbm, dt, dtc, ci
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(3,nxv,nyv) :: fxy, bxy
      integer, device, dimension(mxy1) :: kpic
      real, device, dimension(mxy1) :: ek
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = 6*(mx + 1)*(my + 1)*sizeof(f)
      n = nblock_size*sizeof(f)
      ns = max(ns,n)
      crc = cudaGetLastError()
      call gpurbppush23l<<<dimGrid,dimBlock,ns>>>(ppart,fxy,bxy,kpic,qbm&
     &,dt,dtc,ci,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpurbppush23l error=', crc, ':',                  &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpurbppushf23l(ppart,fxy,bxy,kpic,ncl,ihole,qbm,dt,dtc&
     &,ci,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ntmax,irc)
! Push Interface for Fortran
      implicit none
      integer, intent(in) :: idimp, nppmx, nx, ny, mx, my, nxv, nyv
      integer, intent(in) :: mx1, mxy1, ntmax
      real, intent(in) :: qbm, dt, dtc, ci
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(3,nxv,nyv) :: fxy, bxy
      integer, device, dimension(mxy1) :: kpic
      integer, device, dimension(8,mxy1) :: ncl
      integer, device, dimension(2,ntmax+1,mxy1) :: ihole
      real, device, dimension(mxy1) :: ek
      integer, device, dimension(1) :: irc
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = 6*(mx + 1)*(my + 1)*sizeof(f) + nblock_size*sizeof(n)
      n = nblock_size*sizeof(f)
      ns = max(ns,n)
      ns = ns + 9*sizeof(n)
      crc = cudaGetLastError()
      call gpurbppushf23l<<<dimGrid,dimBlock,ns>>>(ppart,fxy,bxy,kpic,  &
     &ncl,ihole,qbm,dt,dtc,ci,ek,idimp,nppmx,nx,ny,mx,my,nxv,nyv,mx1,   &
     &mxy1,ntmax,irc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpurbppushf23l error=', crc, ':',                 &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpu2ppost2l(ppart,q,kpic,qm,nppmx,idimp,mx,my,nxv,nyv,&
     &mx1,mxy1)
! Deposit Interface for Fortran
      implicit none
      integer, intent(in) :: nppmx, idimp, mx, my, nxv, nyv, mx1, mxy1
      real, intent(in) :: qm
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(nxv,nyv) :: q
      integer, device, dimension(mxy1) :: kpic
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = (mx + 1)*(my + 1)*sizeof(f)
      crc = cudaGetLastError()
      call gpu2ppost2l<<<dimGrid,dimBlock,ns>>>(ppart,q,kpic,qm,nppmx,  &
     &idimp,mx,my,nxv,nyv,mx1,mxy1)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpu2ppost2l error=',crc,':',cudaGetErrorString(crc&
     &)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpu2jppost2l(ppart,cu,kpic,qm,dt,nppmx,idimp,nx,ny,mx,&
     &my,nxv,nyv,mx1,mxy1,ipbc)
! Current Deposit Interface for Fortran
      implicit none
      integer, intent(in) :: nppmx, idimp, nx, ny, mx, my, nxv, nyv
      integer, intent(in) :: mx1, mxy1, ipbc
      real, intent(in) :: qm, dt
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(3,nxv,nyv) :: cu
      integer, device, dimension(mxy1) :: kpic
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = 3*(mx + 1)*(my + 1)*sizeof(f)
      crc = cudaGetLastError()
      call gpu2jppost2l<<<dimGrid,dimBlock,ns>>>(ppart,cu,kpic,qm,dt,   &
     &nppmx,idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpu2jppost2l error=', crc, ':',                   &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpu2jppostf2l(ppart,cu,kpic,ncl,ihole,qm,dt,nppmx,    &
     &idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ntmax,irc)
! Current Deposit Interface for Fortran
      implicit none
      integer, intent(in) :: nppmx, idimp, nx, ny, mx, my, nxv, nyv
      integer, intent(in) :: mx1, mxy1, ntmax
      real, intent(in) :: qm, dt
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(3,nxv,nyv) :: cu
      integer, device, dimension(mxy1) :: kpic
      integer, device, dimension(8,mxy1) :: ncl
      integer, device, dimension(2,ntmax+1,mxy1) :: ihole
      integer, device, dimension(1) :: irc
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = 3*(mx + 1)*(my + 1)*sizeof(f) + (nblock_size + 9)*sizeof(n)
      crc = cudaGetLastError()
      call gpu2jppostf2l<<<dimGrid,dimBlock,ns>>>(ppart,cu,kpic,ncl,    &
     &ihole,qm,dt,nppmx,idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ntmax,irc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpu2jppostf2l error=', crc, ':',                  &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpu2rjppost2l(ppart,cu,kpic,qm,dt,ci,nppmx,idimp,nx,ny&
     &,mx,my,nxv,nyv,mx1,mxy1,ipbc)
! Current Deposit Interface for Fortran
      implicit none
      integer, intent(in) :: nppmx, idimp, nx, ny, mx, my, nxv, nyv
      integer, intent(in) :: mx1, mxy1, ipbc
      real, intent(in) :: qm, dt, ci
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(3,nxv,nyv) :: cu
      integer, device, dimension(mxy1) :: kpic
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = 3*(mx + 1)*(my + 1)*sizeof(f)
      crc = cudaGetLastError()
      call gpu2rjppost2l<<<dimGrid,dimBlock,ns>>>(ppart,cu,kpic,qm,dt,ci&
     &,nppmx,idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ipbc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpu2rjppost2l error=', crc, ':',                  &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpu2rjppostf2l(ppart,cu,kpic,ncl,ihole,qm,dt,ci,nppmx,&
     &idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ntmax,irc)
! Current Deposit Interface for Fortran
      implicit none
      integer, intent(in) :: nppmx, idimp, nx, ny, mx, my, nxv, nyv
      integer, intent(in) :: mx1, mxy1, ntmax
      real, intent(in) :: qm, dt, ci
      real, device, dimension(nppmx,idimp,mxy1) :: ppart
      real, device, dimension(3,nxv,nyv) :: cu
      integer, device, dimension(mxy1) :: kpic
      integer, device, dimension(8,mxy1) :: ncl
      integer, device, dimension(2,ntmax+1,mxy1) :: ihole
      integer, device, dimension(1) :: irc
! local data
      integer :: n, m, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      n = mxy1
      m = (n - 1)/maxgsx + 1
      n = min(n,maxgsx)
      dimGrid = dim3(n,m,1)
      ns = 3*(mx + 1)*(my + 1)*sizeof(f) + (nblock_size + 9)*sizeof(n)
      crc = cudaGetLastError()
      call gpu2rjppostf2l<<<dimGrid,dimBlock,ns>>>(ppart,cu,kpic,ncl,   &
     &ihole,qm,dt,ci,nppmx,idimp,nx,ny,mx,my,nxv,nyv,mx1,mxy1,ntmax,irc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpu2rjppostf2l error=', crc, ':',                 &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpucaguard2l(qc,q,nx,ny,nxe,nye,nxvh,nyv)
! Guard Cell Interface for Fortran
      implicit none
      integer, intent(in) :: nx, ny, nxe, nye, nxvh, nyv
      complex, device, dimension(nxvh,nyv) :: qc
      real, device, dimension(nxe,nye) :: q
! local data
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      dimGrid = dim3(ny,1,1)
      crc = cudaGetLastError()
      call gpucaguard2l<<<dimGrid,dimBlock>>>(qc,q,nx,ny,nxe,nye,nxvh,  &
     &nyv)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpucaguard2l error=',crc,':',                     &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpucacguard2l(cuc,cu,nx,ny,nxe,nye,nxvh,nyv)
! Guard Cell Interface for Fortran
      implicit none
      integer, intent(in) :: nx, ny, nxe, nye, nxvh, nyv
      complex, device, dimension(nxvh,3,nyv) :: cuc
      real, device, dimension(3,nxe,nye) :: cu
! local data
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      dimGrid = dim3(ny,1,1)
      crc = cudaGetLastError()
      call gpucacguard2l<<<dimGrid,dimBlock>>>(cuc,cu,nx,ny,nxe,nye,nxvh&
     &,nyv)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpucacguard2l error=',crc,':',                    &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpucbguard2l(bxyc,bxy,nx,ny,nxe,nye,nxvh,nyv)
! Guard Cell Interface for Fortran
      implicit none
      integer, intent(in) :: nx, ny, nxe, nye, nxvh, nyv
      complex, device, dimension(nxvh,3,nyv) :: bxyc
      real, device, dimension(3,nxe,nye) :: bxy
! local data
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      dimGrid = dim3(ny,1,1)
      crc = cudaGetLastError()
      call gpucbguard2l<<<dimGrid,dimBlock>>>(bxyc,bxy,nx,ny,nxe,nye,   &
     &nxvh,nyv)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpucbguard2l error=',crc,':',                     &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpuppord2l(ppart,ppbuff,kpic,ncl,ihole,idimp,nppmx,nx,&
     &ny,mx,my,mx1,my1,npbmx,ntmax,irc)
! Sort Interface for Fortran
      implicit none
      integer, intent(in) :: idimp, nppmx, nx, ny, mx, my, mx1, my1
      integer, intent(in) :: npbmx, ntmax
      real, device, dimension(nppmx,idimp,mx1*my1) :: ppart
      real, device, dimension(npbmx,idimp,mx1*my1) :: ppbuff
      integer, device, dimension(mx1*my1) :: kpic
      integer, device, dimension(8,mx1*my1) :: ncl
      integer, device, dimension(2,ntmax+1,mx1*my1) :: ihole
      integer, device, dimension(1) :: irc
! local data
      integer :: mxy1, n, m, ns
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      mxy1 = mx1*my1
      m = (mxy1 - 1)/maxgsx + 1
      n = min(mxy1,maxgsx)
      dimGrid = dim3(n,m,1)
! find which particles are leaving tile
      ns = (nblock_size+9)*sizeof(n)
      crc = cudaGetLastError()
      call gpuppfnd2l<<<dimGrid,dimBlock,ns>>>(ppart,kpic,ncl,ihole,    &
     &idimp,nppmx,nx,ny,mx,my,mx1,my1,ntmax,irc)
!     crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpuppfnd2l error=',crc,':',cudaGetErrorString(crc)
         stop
      endif
! buffer particles that are leaving tile and sum ncl
      ns = 9*sizeof(n)
      crc = cudaGetLastError()
      call gpuppmov2l<<<dimGrid,dimBlock,ns>>>(ppart,ppbuff,ncl,ihole,  &
     &idimp,nppmx,mx1,my1,npbmx,ntmax,irc)
!     crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpuppmov2l error=',crc,':',cudaGetErrorString(crc)
         stop
      endif
! copy incoming particles from ppbuff into ppart, update kpic
      ns = (nblock_size+18)*sizeof(n)
      crc = cudaGetLastError()
      call gpuppord2l<<<dimGrid,dimBlock,ns>>>(ppart,ppbuff,kpic,ncl,   &
     &ihole,idimp,nppmx,mx1,my1,npbmx,ntmax,irc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpuppord2l error=',crc,':',cudaGetErrorString(crc)
         stop
      endif 
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpuppordf2l(ppart,ppbuff,kpic,ncl,ihole,idimp,nppmx,  &
     &mx1,my1,npbmx,ntmax,irc)
! Sort Interface for Fortran
      implicit none
      integer, intent(in) :: idimp, nppmx, mx1, my1, npbmx, ntmax
      real, device, dimension(nppmx,idimp,mx1*my1) :: ppart
      real, device, dimension(npbmx,idimp,mx1*my1) :: ppbuff
      integer, device, dimension(mx1*my1) :: kpic
      integer, device, dimension(8,mx1*my1) :: ncl
      integer, device, dimension(2,ntmax+1,mx1*my1) :: ihole
      integer, device, dimension(1) :: irc
! local data
      integer :: mxy1, n, m, ns
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      mxy1 = mx1*my1
      m = (mxy1 - 1)/maxgsx + 1
      n = min(mxy1,maxgsx)
      dimGrid = dim3(n,m,1)
! buffer particles that are leaving tile and sum ncl
      ns = 9*sizeof(n)
      crc = cudaGetLastError()
      call gpuppmov2l<<<dimGrid,dimBlock,ns>>>(ppart,ppbuff,ncl,ihole,  &
     &idimp,nppmx,mx1,my1,npbmx,ntmax,irc)
!     crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpuppmov2l error=',crc,':',cudaGetErrorString(crc)
         stop
      endif
! copy incoming particles from ppbuff into ppart, update kpic
      ns = (nblock_size+18)*sizeof(n)
      crc = cudaGetLastError()
      call gpuppord2l<<<dimGrid,dimBlock,ns>>>(ppart,ppbuff,kpic,ncl,   &
     &ihole,idimp,nppmx,mx1,my1,npbmx,ntmax,irc)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpuppord2l error=',crc,':',cudaGetErrorString(crc)
         stop
      endif 
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpupois23t(qt,fxyt,ffct,we,nx,ny,nxvh,nyv,nxhd,nyhd)
! Poisson Solver Interface for Fortran
      implicit none
      integer, intent(in) :: nx, ny, nxvh, nyv, nxhd, nyhd
      complex, device, dimension(nyv,nxvh) :: qt
      complex, device, dimension(nyv,3,nxvh) :: fxyt
      complex, device, dimension(nyhd,nxhd) :: ffct
      real, device, dimension(nxvh) :: we
! local data
      integer :: nxh1, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      nxh1 = nx/2 + 1
      dimGrid = dim3(nxh1,1,1)
      ns = (nblock_size)*sizeof(f)
      crc = cudaGetLastError()
      call gpupois23t<<<dimGrid,dimBlock,ns>>>(qt,fxyt,ffct,we,nx,ny,   &
     &nxvh,nyv,nxhd,nyhd)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpupois23t error=',crc,':',cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpucuperp2t(cut,nx,ny,nxvh,nyv)
! Poisson Solver Interface for Fortran
      implicit none
      integer, intent(in) :: nx, ny, nxvh, nyv
      complex, device, dimension(nyv,3,nxvh) :: cut
! local data
      integer :: nxh1
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      nxh1 = nx/2 + 1
      dimGrid = dim3(nxh1,1,1)
      crc = cudaGetLastError()
      call gpucuperp2t<<<dimGrid,dimBlock>>>(cut,nx,ny,nxvh,nyv)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpucuperp2t error=',crc,':',cudaGetErrorString(crc&
     &)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpuibpois23t(cut,bxyt,ffct,ci,wm,nx,ny,nxvh,nyv,nxhd, &
     &nyhd)
! Poisson Solver Interface for Fortran
      implicit none
      integer, intent(in) :: nx, ny, nxvh, nyv, nxhd, nyhd
      real, intent(in) :: ci
      complex, device, dimension(nyv,3,nxvh) :: cut, bxyt
      complex, device, dimension(nyhd,nxhd) :: ffct
      real, device, dimension(nxvh) :: wm
! local data
      integer :: nxh1, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      nxh1 = nx/2 + 1
      dimGrid = dim3(nxh1,1,1)
      ns = (nblock_size)*sizeof(f)
      crc = cudaGetLastError()
      call gpuibpois23t<<<dimGrid,dimBlock,ns>>>(cut,bxyt,ffct,ci,wm,nx,&
     &ny,nxvh,nyv,nxhd,nyhd)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpuibpois23t error=', crc, ':',                   &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpumaxwel2t(exyt,bxyt,cut,ffct,ci,dt,wf,wm,nx,ny,nxvh,&
     &nyv,nxhd,nyhd)
! Maxwell Solver Interface for Fortran
      implicit none
      integer, intent(in) :: nx, ny, nxvh, nyv, nxhd, nyhd
      real, intent(in) :: ci, dt
      complex, device, dimension(nyv,3,nxvh) :: exyt, bxyt, cut
      complex, device, dimension(nyhd,nxhd) :: ffct
      real, device, dimension(nxvh) :: wf, wm
! local data
      integer :: nxh1, ns
      real :: f
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      nxh1 = nx/2 + 1
      dimGrid = dim3(nxh1,1,1)
      ns = (nblock_size)*sizeof(f)
      crc = cudaGetLastError()
      call gpumaxwel2t<<<dimGrid,dimBlock,ns>>>(exyt,bxyt,cut,ffct,ci,dt&
     &,wf,wm,nx,ny,nxvh,nyv,nxhd,nyhd)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpumaxwel2t error=', crc, ':',                    &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpuemfield2t(fxyt,exyt,ffct,isign,nx,ny,nxvh,nyv,nxhd,&
     &nyhd)
! Maxwell Solver Interface for Fortran
      implicit none
      integer, intent(in) :: isign, nx, ny, nxvh, nyv, nxhd, nyhd
      complex, device, dimension(nyv,3,nxvh) :: fxyt, exyt
      complex, device, dimension(nyhd,nxhd) :: ffct
! local data
      integer :: nxh1
      type (dim3) :: dimBlock, dimGrid
      dimBlock = dim3(nblock_size,1,1)
      nxh1 = nx/2 + 1
      dimGrid = dim3(nxh1,1,1)
      crc = cudaGetLastError()
      call gpuemfield2t<<<dimGrid,dimBlock>>>(fxyt,exyt,ffct,isign,nx,ny&
     &,nxvh,nyv,nxhd,nyhd)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpuemfield2t error=', crc, ':',                   &
     &cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpuwfft2rcs(f,g,isign,mixup,sct,indx,indy,nxhd,nyd,   &
     &nxhyd,nxyhd)
! wrapper function for real to complex fft, without packed data
! if isign = -1, f = input, g = output
! if isign = 1, g = input, f = output
! nxhd must be >= nx/2 + 1
      implicit none
      integer, intent(in) :: isign, indx, indy, nxhd, nyd, nxhyd, nxyhd
      complex, device, dimension(nxhd,nyd) :: f
      complex, device, dimension(nyd,nxhd) :: g
      integer, device, dimension(nxhyd) :: mixup
      complex, device, dimension(nxyhd) :: sct
! local data
      integer :: nxh, nxh1, ny, nsize, ns
      integer :: nxi = 1, nyi = 1, mx = 16
      complex :: z
      type (dim3) :: dimBlock, dimBlockt
      type (dim3) :: dimGridx, dimGridy, dimGridtx, dimGridty
      dimBlock = dim3(nblock_size,1,1)
      dimBlockt = dim3(mx,mx,1)
! calculate range of indices
      nxh = 2**(indx - 1)
      nxh1 = nxh + 1
      ny = 2**indy
      dimGridx = dim3(nxh1,1,1)
      dimGridy = dim3(ny,1,1)
      dimGridtx = dim3((nxh1-1)/mx+1,(ny-1)/mx+1,1)
      dimGridty = dim3((ny-1)/mx+1,(nxh1-1)/mx+1,1)
! inverse fourier transform
      if (isign < 0) then
! perform x fft
         nsize = min(nxh,1024)
         ns = nsize*sizeof(z)
         crc = cudaGetLastError()
         call gpufft2rcxs<<<dimGridy,dimBlock,ns>>>(f,isign,mixup,sct,  &
     &indx,indy,nyi,ny,nxhd,nyd,nxhyd,nxyhd,nsize)
!        crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpufft2rcxs error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
! transpose f to g
         ns = (mx+1)*mx*sizeof(z)
         crc = cudaGetLastError()
         call gpuctpose4<<<dimGridtx,dimBlockt,ns>>>(f,g,nxh1,ny,nxhd,  &
     &nyd)
!        crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpuctpose4 error=',crc,':',                    &
     &cudaGetErrorString(crc)
            stop
         endif
! perform y fft
         nsize = min(ny,1024)
         ns = nsize*sizeof(z)
         crc = cudaGetLastError()
         call gpufft2rcys<<<dimGridx,dimBlock,ns>>>(g,isign,mixup,sct,  &
     &indx,indy,nxi,nxh1,nxhd,nyd,nxhyd,nxyhd,nsize)
         crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpufft2rcys error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
! forward fourier transform
      else if (isign > 0) then
! perform y fft
         nsize = min(ny,1024)
         ns = nsize*sizeof(z)
         crc = cudaGetLastError()
         call gpufft2rcys<<<dimGridx,dimBlock,ns>>>(g,isign,mixup,sct,  &
     &indx,indy,nxi,nxh1,nxhd,nyd,nxhyd,nxyhd,nsize)
!        crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpufft2rcys error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
! transpose g to f
         ns = (mx+1)*mx*sizeof(z)
         crc = cudaGetLastError()
         call gpuctpose4<<<dimGridty,dimBlockt,ns>>>(g,f,ny,nxh1,nyd,   &
     &nxhd)
!        crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpuctpose4 error=',crc,':',                    &
     &cudaGetErrorString(crc)
            stop
         endif
! perform x fft
         nsize = min(nxh,1024)
         ns = nsize*sizeof(z)
         crc = cudaGetLastError()
         call gpufft2rcxs<<<dimGridy,dimBlock,ns>>>(f,isign,mixup,sct,  &
     &indx,indy,nyi,ny,nxhd,nyd,nxhyd,nxyhd,nsize)
         crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpufft2rcxs error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpuwfft2rcsn(fn,gn,isign,mixup,sct,indx,indy,ndim,nxhd&
     &,nyd,nxhyd,nxyhd)
! wrapper function for multiple real to complex ffts,
! without packed data 
! if isign = -1, fn = input, gn = output
! if isign = 1, gn = input, fn = output
! ndim = vector dimension
! nxhd must be >= nx/2 + 1
      implicit none
      integer, intent(in) :: isign, indx, indy, ndim, nxhd, nyd
      integer, intent(in) :: nxhyd, nxyhd
      complex, device, dimension(nxhd,ndim,nyd) :: fn
      complex, device, dimension(nyd,ndim,nxhd) :: gn
      integer, device, dimension(nxhyd) :: mixup
      complex, device, dimension(nxyhd) :: sct
! local data
      integer :: nxh, nxh1, ny, nxp, nyp, nnxd, nnyd, nsize, ns
      integer :: nxi = 1, nyi = 1, mx = 16
      complex :: z
      type (dim3) :: dimBlock, dimBlockt
      type (dim3) :: dimGridx, dimGridy, dimGridtx, dimGridty
      dimBlock = dim3(nblock_size,1,1)
      dimBlockt = dim3(mx,mx,1)
! calculate range of indices
      nxh = 2**(indx - 1)
      nxh1 = nxh + 1
      ny = 2**indy
      nxp = ndim*nxh1
      nyp = ndim*ny
      nnxd = ndim*nxhd
      nnyd = ndim*nyd
      dimGridx = dim3(nxp,1,1)
      dimGridy = dim3(nyp,1,1)
      dimGridtx = dim3((nxh1-1)/mx+1,(ny-1)/mx+1,1)
      dimGridty = dim3((ny-1)/mx+1,(nxh1-1)/mx+1,1)
! inverse fourier transform 
      if (isign < 0) then
! perform x fft
         nsize = min(nxh,1024)
         ns = nsize*sizeof(z)
         crc = cudaGetLastError()
         call gpufft2rcxs<<<dimGridy,dimBlock,ns>>>(fn,isign,mixup,sct, &
     &indx,indy,nyi,nyp,nxhd,nnyd,nxhyd,nxyhd,nsize)
!        crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpufft2rcxs error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
! transpose f to g
         ns = ndim*(mx+1)*mx*sizeof(z)
         crc = cudaGetLastError()
         call gpuctpose4n<<<dimGridtx,dimBlockt,ns>>>(fn,gn,nxh1,ny,ndim&
     &,nxhd,nyd)
!        crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpuctpose4n error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
! perform y fft
         nsize = min(ny,1024)
         ns = nsize*sizeof(z)
         crc = cudaGetLastError()
         call gpufft2rcys<<<dimGridx,dimBlock,ns>>>(gn,isign,mixup,sct, &
     &indx,indy,nxi,nxp,nnxd,nyd,nxhyd,nxyhd,nsize)
         crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpufft2rcys error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
! forward fourier transform
      else if (isign > 0) then
! perform y fft
         nsize = min(ny,1024)
         ns = nsize*sizeof(z)
         crc = cudaGetLastError()
         call gpufft2rcys<<<dimGridx,dimBlock,ns>>>(gn,isign,mixup,sct, &
     &indx,indy,nxi,nxp,nnxd,nyd,nxhyd,nxyhd,nsize)
!        crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpufft2rcys error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
! transpose g to f
         ns = ndim*(mx+1)*mx*sizeof(z)
         crc = cudaGetLastError()
         call gpuctpose4n<<<dimGridty,dimBlockt,ns>>>(gn,fn,ny,nxh1,ndim&
     &,nyd,nxhd)
!        crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpuctpose4n error=',crc,':',                    &
     &cudaGetErrorString(crc)
            stop
         endif
! perform x fft
         nsize = min(nxh,1024)
         ns = nsize*sizeof(z)
         crc = cudaGetLastError()
         call gpufft2rcxs<<<dimGridy,dimBlock,ns>>>(fn,isign,mixup,sct, &
     &indx,indy,nyi,nyp,nxhd,nnyd,nxhyd,nxyhd,nsize)
         crc = cudaThreadSynchronize()
         crc = cudaGetLastError()
         if (crc /= 0) then
            write (*,*) 'gpufft2rcxs error=',crc,':',                   &
     &cudaGetErrorString(crc)
            stop
         endif
      endif
      end subroutine
!
!-----------------------------------------------------------------------
      subroutine fgpusum2(a,sa,nx)
! segmented 1d parallel sum reduction of input array a, of length nx
! first reduce individual blocks in parallel, writing result to scr
! then reduce scr serially, result is written to sa
      implicit none
      integer, intent(in) :: nx
      real, device, dimension(*) :: a, sa
! local data
      integer :: nbx, nbs, ns
      integer, save :: len = 0
!     real, device, dimension(:), allocatable :: scr
      real :: f
      type (dim3) :: dimBlock, dimGrid, dimGrid1
      nbx = (nx - 1)/nblock_size + 1
      dimBlock = dim3(nblock_size,1,1)
      dimGrid = dim3(nbx,1,1)
      nbs = (nbx - 1)/nblock_size + 1
      dimGrid1 = dim3(nbs,1,1)
! create scratch array
      if (len < nbx) then
         if (len > 0) deallocate(scr)
         allocate(scr(nbx))
         len = nbx
      endif
! reduce individual blocks in parallel
      ns = nblock_size*sizeof(f)
      crc = cudaGetLastError()
      call gpusum2<<<dimGrid,dimBlock,ns>>>(a,scr,nx)
!     crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpusum2 error=',crc,':',cudaGetErrorString(crc)
         stop
      endif
! 1d serial reduction
      crc = cudaGetLastError()
      call gpusum1<<<dimGrid1,dimBlock,ns>>>(scr,sa,nbx)
      crc = cudaThreadSynchronize()
      crc = cudaGetLastError()
      if (crc /= 0) then
         write (*,*) 'gpusum1 error=',crc,':',cudaGetErrorString(crc)
         stop
      endif
      end subroutine
!
      end module
